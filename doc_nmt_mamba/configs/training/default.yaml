# Default training configuration for 1x H100 80GB HBM3
# Optimized for maximum throughput on single GPU (no DDP overhead)

# =============================================================================
# Batch Size - MAXIMIZE FOR SINGLE H100 80GB
# =============================================================================
# 200M model uses ~8-12GB VRAM. With 80GB available, maximize batch size
# to saturate H100 compute throughput. Single GPU = no DDP comm overhead.
batch_size: 256                   # Aggressive for single H100 (adjust per seq_len)
gradient_accumulation_steps: 1    # No accumulation needed with large batch

# =============================================================================
# Training Loop
# =============================================================================
max_steps: 100000

# =============================================================================
# Optimization
# =============================================================================
learning_rate: 3e-4
weight_decay: 0.01
betas: [0.9, 0.95]
eps: 1e-8

# Adaptive Gradient Clipping (AGC) - parameter-free alternative to fixed clip norm
# AGC clips based on ||weight|| / ||grad|| ratio (Brock et al., 2021)
use_agc: true
agc_clip_factor: 0.01
max_grad_norm: 1.0   # Fallback if use_agc: false

# Fused optimizer - CRITICAL FOR H100
# Runs entire optimizer step on GPU without CPU round-trips (~5-10% speedup)
use_fused_optimizer: true

# =============================================================================
# Learning Rate Schedule - ADAPTIVE WARMUP
# =============================================================================
scheduler_type: cosine
warmup_ratio: 0.05     # Warmup as fraction of max_steps (5% default)
min_warmup_steps: 100  # Floor for short training runs
min_lr: 1e-6

# =============================================================================
# Label Smoothing - ADAPTIVE
# =============================================================================
# null = auto-scale based on vocab_size: 0.1 * log2(vocab) / log2(32768)
label_smoothing: null

# =============================================================================
# Precision - H100 NATIVE BF16
# =============================================================================
# NEVER use fp16 on H100 (overflow risk). BF16 is native and optimal.
mixed_precision: bf16
use_bf16: true

# =============================================================================
# torch.compile - FREE PERFORMANCE
# =============================================================================
# H100 + Inductor + cudagraphs = ~20-30% speedup
use_compile: true
compile_mode: max-autotune  # Aggressive optimization (longer startup, faster runtime)

# =============================================================================
# H100 Hardware Optimizations
# =============================================================================
tf32_matmul: true           # ~3x speedup on FP32 math
cudnn_benchmark: true       # Autotune convolutions

# =============================================================================
# Memory Optimization
# =============================================================================
gradient_checkpointing: true  # Enable for 8K sequences

# =============================================================================
# Distributed Training (Single H100 - DDP disabled by default)
# =============================================================================
distributed_strategy: none     # Single GPU - no DDP overhead
static_graph: true             # Enable for torch.compile compatibility

# FSDP settings (if using FSDP for larger models)
fsdp_sharding: full_shard
fsdp_cpu_offload: false        # Not needed with 80GB VRAM

# =============================================================================
# Logging and Checkpointing
# =============================================================================
log_steps: 100
eval_steps: 1000
save_steps: 5000
save_total_limit: 3
output_dir: outputs

# =============================================================================
# Regularization
# =============================================================================
dropout: 0.1

# =============================================================================
# Resume
# =============================================================================
resume_from: null
