# FAST training configuration - Maximum speed on H100 80GB
# Optimizations:
# - torch.compile with max-autotune
# - Reduced sequence length (2048) for faster iteration
# - gradient_checkpointing OFF (H100 has enough VRAM)
# - Larger batch size

# Training loop
max_steps: 50000  # Reduced for faster completion
batch_size: 128   # Increased (no gradient checkpointing = more VRAM)
gradient_accumulation_steps: 1
max_grad_norm: 1.0

# Optimization
learning_rate: 2e-4  # Slightly higher for faster convergence
weight_decay: 0.01
betas: [0.9, 0.95]

# Scheduling
scheduler_type: cosine
warmup_steps: 2000  # Reduced warmup
min_lr: 1e-6

# Memory optimization - SPEED FOCUSED
gradient_checkpointing: false  # OFF for speed (H100 80GB has enough VRAM)
use_bf16: true

# torch.compile - NOW WORKING
use_compile: true
compile_mode: max-autotune

# H100 optimizations
tf32_matmul: true
cudnn_benchmark: true

# Logging and checkpointing
log_steps: 50  # More frequent logging
eval_steps: 2000
save_steps: 5000
save_total_limit: 2
output_dir: outputs

# Regularization
label_smoothing: 0.1

# Reproducibility
seed: 42
deterministic: false

# Resume
resume_from: null
