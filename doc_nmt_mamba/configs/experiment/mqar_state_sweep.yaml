# @package _global_
# MQAR State Capacity Sweep
# Paper Reference: Figure 1
#
# Tests Mamba-2 state capacity limits using MQAR synthetic task.
# d_state=64 forces the state to "forget" earlier pairs.
# Demonstrates why sparse cross-attention is needed for long-range deps.
#
# Curriculum: num_pairs [16, 32, 64, 128, 256, 512]
# Expected: Performance cliff at num_pairs > d_state

defaults:
  - /experiment/_base
  - override /model: hybrid_small  # Small model for synthetic task
  - override /data: mqar
  - override /training: default

experiment:
  name: mqar_state_sweep
  paper_ref: "Figure 1 - State Capacity"
  description: "MQAR state capacity sweep with curriculum learning"

project:
  name: align_mamba_icml
  seed: 42

# Override model for MQAR (decoder-only, smaller)
model:
  d_state: 64  # Match MQAR config
  encoder_layers: 0  # Decoder-only for MQAR
  decoder_layers: 8

# Shorter training for synthetic task
training:
  max_steps: 20000
  eval_steps: 500
  save_steps: 2000
  warmup_steps: 1000
  learning_rate: 1e-4

logging:
  wandb:
    enabled: true
    tags: ["mqar", "figure1", "state_capacity", "synthetic"]
