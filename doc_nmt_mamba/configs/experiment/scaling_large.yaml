# @package _global_
# Scaling Experiment: Large Model (400M params)
# Paper Reference: Figure 3
#
# Tests upper scaling limit of Align-Mamba architecture.
# 24 encoder + 24 decoder layers, d_model=1024
# NOTE: May need reduced seq_len for memory

defaults:
  - /experiment/_base
  - override /model: hybrid_large
  - override /data: iwslt14_de_en
  - override /training: default

experiment:
  name: scaling_large
  paper_ref: "Figure 3 - 400M"
  description: "Scaling experiment: 400M parameter model"

project:
  name: align_mamba_icml
  seed: 42

# Adjust batch size for larger model
training:
  per_device_train_batch_size: 64  # Reduced from 128
  per_device_eval_batch_size: 128  # Reduced from 256

logging:
  wandb:
    enabled: true
    tags: ["scaling", "figure3", "large", "400M"]
