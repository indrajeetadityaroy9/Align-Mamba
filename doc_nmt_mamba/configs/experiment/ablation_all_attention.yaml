# @package _global_
# Ablation: Pure Attention (All Attention Layers)
# Paper Reference: Table 2
#
# Standard Transformer baseline with 100% attention layers.
# Uses attention_ratio=1.0 to convert all Mamba blocks to attention.
# Control experiment to show O(L^2) cost of pure attention.

defaults:
  - /experiment/_base
  - override /model: baseline_transformer
  - override /data: iwslt14_de_en
  - override /training: default

experiment:
  name: ablation_all_attention
  paper_ref: "Table 2 - Pure Attention"
  description: "Ablation: 100% attention layers (Transformer baseline)"

project:
  name: align_mamba_icml
  seed: 42

logging:
  wandb:
    enabled: true
    tags: ["ablation", "table2", "pure_attention", "transformer"]
