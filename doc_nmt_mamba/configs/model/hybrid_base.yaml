# Base model configuration (77M params)
# Main experiments

name: base

# Architecture
encoder_layers: 12
decoder_layers: 12
d_model: 512
d_state: 64
d_conv: 4
expand: 2

# Attention configuration (1:7 ratio)
attention_ratio: 0.125
n_heads: 8
head_dim: 64
cross_attn_every: 4

# Regularization
dropout: 0.1

# Positional encoding
max_seq_len: 8192
rope_theta: 10000.0

# Vocabulary (overridden by tokenizer at runtime)
# Using 32K BPE tokenizer (32768 vocab) for proper parameter allocation
vocab_size: 32768
pad_token_id: 0
bos_token_id: 1
eos_token_id: 2
