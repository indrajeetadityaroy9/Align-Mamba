# Transformer Baseline configuration (77M params)
# CONTROL EXPERIMENT: Pure Transformer for comparison with Hybrid Mamba
#
# This config matches the base Mamba model in parameters but uses
# attention_ratio=1.0 (all attention layers, no Mamba)

name: base_transformer

# Architecture - same as base model
encoder_layers: 12
decoder_layers: 12
d_model: 512
d_state: 64  # Not used (no Mamba blocks)
d_conv: 4    # Not used (no Mamba blocks)
expand: 2    # Not used (no Mamba blocks)

# CRITICAL: attention_ratio=1.0 makes ALL layers attention (pure Transformer)
attention_ratio: 1.0  # 100% attention = Transformer baseline
n_heads: 8
head_dim: 64
cross_attn_every: 4  # Cross-attention every 4 layers

# Regularization
dropout: 0.1

# Positional encoding
max_seq_len: 8192
rope_theta: 10000.0

# Vocabulary (overridden by tokenizer at runtime)
vocab_size: 32768
pad_token_id: 0
bos_token_id: 1
eos_token_id: 2
