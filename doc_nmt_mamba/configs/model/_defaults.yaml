# _defaults.yaml - shared model configuration
# All model configs inherit from this file

# Adaptive parameters (null = auto-computed)
# d_state: computed from max_seq_len and vocab_size using RMT formula
# attention_ratio: computed as 2/encoder_layers (guarantees 2 attention layers)
# hybrid_positions: computed as [0, N//3, 2N//3] for N decoder layers
d_state: null
attention_ratio: null
hybrid_positions: null

# Mamba block defaults
d_conv: 4
expand: 2

# Attention defaults
head_dim: 64

# Regularization
dropout: 0.1

# Positional encoding
max_seq_len: 8192

# Vocabulary (overridden by tokenizer at runtime)
vocab_size: 32768
pad_token_id: 0
bos_token_id: 1
eos_token_id: 2

# Initialization
# Mimetic init zeros A_log so SSM acts like attention at init (Trockman et al., 2024)
mimetic_init: false
