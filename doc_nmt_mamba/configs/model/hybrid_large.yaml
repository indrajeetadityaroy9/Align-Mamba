# Large model configuration (400M params)
# Final scaling test
# NOTE: May need reduced seq_len (4K) to fit batch_size=64

name: large

# Architecture
encoder_layers: 24
decoder_layers: 24
d_model: 1024
d_state: 128
d_conv: 4
expand: 2

# Attention configuration (1:7 ratio = 3 attention layers in 24)
attention_ratio: 0.125
n_heads: 16
head_dim: 64
cross_attn_every: 4

# Regularization
dropout: 0.1

# Positional encoding
max_seq_len: 4096  # Reduced for memory
rope_theta: 10000.0

# Vocabulary (overridden by tokenizer at runtime)
# Using 32K BPE tokenizer (32768 vocab) for proper parameter allocation
vocab_size: 32768
pad_token_id: 0
bos_token_id: 1
eos_token_id: 2
