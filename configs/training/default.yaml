# Training configuration for H100
#
# Adaptive parameters computed at runtime:
# - warmup_steps: From gradient norm stability
# - weight_decay: Per-parameter, scaled by magnitude
# - agc_clip_factor: Per-parameter, from initialization scale

batch_size: 256
gradient_accumulation_steps: 1
max_steps: 100000

learning_rate: 3e-4

# Adaptive Gradient Clipping (AGC)
use_agc: true

# Learning Rate Schedule
scheduler_type: cosine

# Label Smoothing (null = auto-scale)
label_smoothing: null

# Precision
use_bf16: true

# torch.compile
use_compile: true
compile_mode: max-autotune

# Memory
gradient_checkpointing: true

# Distributed
distributed_strategy: none
static_graph: true

# Logging
log_steps: 100
eval_steps: 1000
save_steps: 5000
output_dir: outputs
