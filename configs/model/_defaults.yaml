# Model defaults - inherited by all model configs
# Token IDs and Mamba parameters are in constants.py

# Adaptive parameters (null = auto-computed)
d_state: null
attention_ratio: null
hybrid_positions: null

# Attention
head_dim: 64

# Regularization
dropout: 0.1

# Positional encoding
max_seq_len: 8192

# Vocabulary (overridden by tokenizer at runtime)
vocab_size: 32768

# Initialization
mimetic_init: false
