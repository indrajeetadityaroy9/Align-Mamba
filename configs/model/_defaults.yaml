# Model defaults - SOTA architecture by default
#
# Architecture integrates:
# - Polarized Mamba (arXiv:2501.00658): A=0/A=1 channels
# - Zamba-style shared GSA (arXiv:2411.15242)
# - Capacity-aware cross-attention placement (arXiv:2506.11891)

# Core architecture
d_model: 256
d_state: 64
n_heads: 8
head_dim: 64

# Regularization
dropout: 0.1

# Positional encoding
max_seq_len: 8192

# Vocabulary (overridden by tokenizer at runtime)
vocab_size: 8192
