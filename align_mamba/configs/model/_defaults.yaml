# Model defaults - SOTA architecture by default
#
# Architecture integrates:
# - Polarized Mamba (arXiv:2501.00658): A=0/A=1 channels
# - Zamba-style shared GSA (arXiv:2411.15242)
# - Capacity-aware cross-attention placement (arXiv:2506.11891)

# Core architecture
d_model: 256
d_state: 64
n_heads: 8
head_dim: 64

# Regularization
dropout: 0.1

# Positional encoding
max_seq_len: 8192

# Vocabulary
vocab_size: 8192

# =============================================================================
# SOTA Features Configuration
# =============================================================================
# Block types:
#   polarized       - A=0/A=1 channels (default, arXiv:2501.00658)
#   state_expanded  - HGRN2 d→d² capacity (arXiv:2404.07904)
#   memmamba        - Memory pool augmentation (arXiv:2510.03279)
#
# Attention types:
#   softmax         - FlashAttention-2 (default)
#   based           - Taylor linear + sliding window (arXiv:2402.18668)

sota:
  block_type: polarized
  attention_type: softmax

  # State expansion (HGRN2) - used when block_type=state_expanded
  state_expansion_head_dim: 128

  # BASED attention - used when attention_type=based
  based_feature_dim: 16
  based_window_size: 64

  # MemMamba - used when block_type=memmamba
  memmamba_pool_size: 50
  memmamba_summary_dim: 64
  memmamba_tau1: 0.5
  memmamba_tau2: 0.3
  memmamba_cross_layer_freq: 4
