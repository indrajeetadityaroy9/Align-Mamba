RAM-Net

## RAM-NET: EXPRESSIVE LINEAR ATTENTION WITH SELECTIVELY ADDRESSABLE MEMORY


**Kaicheng Xiao, Haotian Li, Liran Dong, Guoliang Xing**
The Chinese University of Hong Kong
_{_ xk023, lh023, dl123, glxing _}_ @ie.cuhk.edu.hk


ABSTRACT


While linear attention architectures offer efficient inference, compressing unbounded history into a fixed-size memory inherently limits expressivity and causes
information loss. To address this limitation, we introduce Random Access Memory Network (RAM-Net), a novel architecture designed to bridge the gap between the representational capacity of full attention and the memory efficiency
of linear models. The core of RAM-Net maps inputs to high-dimensional sparse
vectors serving as explicit addresses, allowing the model to selectively access a
massive memory state. This design enables exponential state size scaling without additional parameters, which significantly mitigates signal interference and
enhances retrieval fidelity. Moreover, the inherent sparsity ensures exceptional
computational efficiency, as state updates are confined to minimal entries. Extensive experiments demonstrate that RAM-Net consistently surpasses state-of-theart baselines in fine-grained long-range retrieval tasks and achieves competitive
performance in standard language modeling and zero-shot commonsense reasoning benchmarks, validating its superior capability to capture complex dependencies with significantly reduced computational overhead.


1 INTRODUCTION


Transformer has emerged as the dominant architecture for sequence modeling due to their effectiveness in capturing long-range dependencies (Vaswani et al., 2017). However, despite their widespread
success, they fundamentally incur a linear memory growth during inference. The standard full attention mechanism necessitates caching the complete history of key-value pairs to ensure retrieval
precision, creating a substantial memory bottleneck for long sequences (Dao et al., 2022). Moreover,
standard transformers lack an explicit mechanism to selectively discard obsolete information (Rae
et al., 2019). Instead, they simply concatenate new tokens to the existing historical context, treating
every input as a permanent entry in the memory. Consequently, this passive accumulation of data
leads to significant computational inefficiencies and attention dilution when processing extensive
contexts.


To mitigate the quadratic computational cost and linear memory growth of full attention, substantial
research has been dedicated to linearizing the attention mechanism. Existing approaches range from
kernel-based approximations (Katharopoulos et al., 2020; Choromanski et al., 2020; Wang et al.,
2020) to recurrent architectures and state space models (Gu et al., 2021; Poli et al., 2023; Gu & Dao,
2024; Peng et al., 2023; Ma et al., 2022; Qin et al., 2024). Despite their diversity, these methods
fundamentally rely on compressing unbounded historical context into a fixed-size recurrent state.
While this formulation successfully achieves constant memory and linear time complexity during
inference, it introduces a significant expressivity bottleneck. Specifically, compressing the entire
history into a single fixed-size state forces distinct contextual features to compete for limited storage
(Sun et al., 2024). Unlike full attention, which retains direct access to specific past key-value pairs,
linear architectures rely on this superposed state for retrieval. Consequently, this compact representation often compromises the ability to model fine-grained long-range interactions, particularly in
tasks where high-resolution recall is essential (Arora et al., 2024b).


To bridge the gap between representational capacity and memory efficiency, we introduce RAMNet. This architecture is designed to reconcile the high-fidelity retrieval capabilities of full attention


1


RAM-Net


Figure 1: Comparison of memory mechanisms. (a) Full Attention: Retains the entire history for retrieval, resulting in linear memory growth. (b) Linear Attention: Compresses history into a fixed-size
state. The reliance on kernel-based similarity leads to limited capacity and inevitable interference.
(c) RAM-Net: Decouples memory capacity from feature dimension via an Address Decoder, which
maps dense vectors **k** _t_ and **v** _t_ into high-dimensional sparse addresses **w** _t_ and **r** _t_ . This enables massive state capacity and high-fidelity retrieval with constant memory state size.


with the constant memory overhead of linear models. Departing from the traditional paradigm of
compressing history into a fixed-size latent bottleneck, the core of RAM-Net utilizes a differentiable address decoder. This mechanism projects dense low-dimensional input vectors into highdimensional sparse vectors that serve as explicit addresses, enabling the model to selectively access
a massive global memory state. This design allows for exponential state size scaling without any
increase in learnable parameters. By distributing historical context across this expanded memory
space, RAM-Net effectively isolates diverse semantic features, significantly mitigating the signal
interference and information loss inevitably caused by compact state compression. Moreover, despite the massive memory capacity, RAM-Net maintains exceptional computational efficiency. The
inherent sparsity ensures that state updates and retrieval operations are strictly confined to a minimal
set of active entries. This mechanism mimics the precision of Random Access Memory, enabling
the model to capture fine-grained, long-range dependencies with high precision while keeping computational costs low.


2 BACKGROUND


To provide a unified view of sequence modeling, we revisit the attention mechanism from the perspective of state maintenance. At each step _t_, the model maintains a state **S** _t_ by updating and
retrieving historical information from it.

Given an input row vector **x** _t ∈_ R [1] _[×][d][m]_, we first obtain the query **q** _t,_ **k** _t ∈_ R [1] _[×][d][k]_ and value **v** _t ∈_
R [1] _[×][d][v]_ via learnable projections **W** _q,_ **W** _k ∈_ R _[d][m][×][d][k]_ and **W** _v ∈_ R _[d][m][×][d][v]_ . In this framework,
we interpret the attention process as two distinct operations: the key and value serve to update the
memory state via a _write_ operator _W_, while the query performs information retrieval via a _read_
operator _R_ :


**S** _t_ = _W_ ( **S** _t−_ 1 _,_ **k** _t,_ **v** _t_ ) _,_ **o** _t_ = _R_ ( **S** _t,_ **q** _t_ ) _._ (1)


The output is subsequently projected by **W** _o_ . Different attention architectures fundamentally differ
in how they structure the memory state **S** _t_ and define these specific update ( _W_ ) and retrieval ( _R_ )
mechanisms (Arora et al., 2024b).


2


RAM-Net


2.1 FULL ATTENTION


Full Attention (Vaswani et al., 2017) explicitly maintains the entire history as its memory state. As
illustrated in Fig. 1 (a), the state at step _t_ is defined as a tuple **S** _t_ = ( **K** _t,_ **V** _t_ ), consisting of the
accumulated key matrix **K** _t ∈_ R _[t][×][d][k]_ and value matrix **V** _t ∈_ R _[t][×][d][v]_ .


Specifically, the update operator _W_ appends the current inputs to the history cache, while the read
operator _R_ computes Softmax-normalized scores **r** _t ∈_ R [1] _[×][t]_ to retrieve context via the query-key
matching:
**K** _t_ = Concat( **K** _t−_ 1 _,_ **k** _t_ ) _,_
**V** _t_ = Concat( **V** _t−_ 1 _,_ **v** _t_ ) _,_



**r** _t_ = Softmax( **q** _t_ **K** _[⊤]_ _t_ [)] _[,]_

**o** _t_ = **r** _t_ **V** _t._



(2)



However, since the size of state **S** _t_ grows linearly with the sequence length _t_, this approach incurs
significant memory and computational costs during inference.


2.2 LINEAR ATTENTION


Linear attention restricts the state to a fixed-size **S** _t ∈_ R _[d][k][×][d][v]_, as shown in Fig. 1 (b). To compress
unbounded history into these finite state, linear attention introduces a kernel function _ϕ_ ( _·_ ). It uses
the mapped dense vectors **w** _t_ = _ϕ_ ( **k** _t_ ) _∈_ R [1] _[×][d][k]_ and **r** _t_ = _ϕ_ ( **q** _t_ ) _∈_ R [1] _[×][d][k]_ to explicitly control
the writing and reading of the state. The state update _W_ becomes a recurrent accumulation, and
retrieval _R_ becomes a linear projection:


**S** _t_ = _g_ ( **x** _t_ ) **S** _t−_ 1 + **w** _t_ _[⊤]_ **[v]** _[t][,]_ **o** _t ∝_ **r** _t_ **S** _t._ (3)


Here, _g_ ( **x** _t_ ) acts as a decay gate to alleviate capacity exhaustion via selective forgetting, a mechanism widely used in modern variants.


However, the memory capacity is tightly coupled with the feature dimension _dk_, which defines the
number of memory rows. Scaling up capacity to reduce information overlap necessitates increasing
_dk_, which causes the memory and computational cost of the state update to grow synchronously.
This coupling makes it inefficient to expand the state for complex contexts, leading to the superposition of memories (Sun et al., 2024) and inaccurate retrieval.


3 RAM-NET


To scale the memory capacity without incurring additional parameter or computational overheads,
we introduce **RAM-Net**, as illustrated in Fig. 1 (c). The core design is an efficient mapping mechanism called _address decoding_ that transforms the dense, low-dimensional vectors **k** _t,_ **q** _t ∈_ R [1] _[×][d][k]_
into high-dimensional sparse write and read vectors **w** _t,_ **r** _t ∈_ R [1] _[×][M]_ (where _M ≫_ _dk_ ). Specifically,
we enforce Top- _K_ sparsity on **w** _t_ and **r** _t_, resulting in _K_ -hot vectors that act as explicit addresses.
Correspondingly, we define the memory state **S** _t ∈_ R _[M]_ _[×][d][v]_ to comprise _M_ discrete memory slots.
In this framework, the non-zero indices of **w** _t_ and **r** _t_ act as pointers, activating only _K_ specific slots
for each read or write operation—a sparse access pattern analogous to Random Access Memory
(RAM).


This architecture yields two critical advantages. First, decoupling the memory capacity _M_ from
the feature dimension _dk_ enables massive state size scaling without inflating model parameters.
The resulting large capacity of **S** _t_ significantly reduces signal interference, ensuring high-fidelity
retrieval. Second, since only _K_ of the _M_ memory slots are active at each write or read operation,
the model achieves a highly efficient computational cost of _O_ ( _K · dv_ ).


3.1 ADDRESS DECODER


To derive high-dimensional sparse addresses, we propose an efficient address decoding method
_AK,U_ ( _·_ ), which is composed of a _Product Softmax_ expansion _ρU_ ( _·_ ) that expands the input into
a concentrated high-dimensional distribution, and a Top- _K_ truncation _TK_ ( _·_ ) that enforces sparsity
by discarding minor values.


3


RAM-Net


Figure 2: Overview of the RAM-Net architecture. The Address Decoder transforms **k** _t_ and **q** _t_
vectors into high-dimensional sparse addresses via Product Softmax, Top- _K_ truncation, and Cyclic
Address Positional Embedding (CAPE). For visual clarity, we illustrate a simplified configuration
with _U_ = 3 partitions and sub-dimension _dp_ = 2. This results in a total memory capacity of _M_ = 8
slots with selection sparsity _K_ = 1.


_U_ **-order product softmax** _ρU_ ( _·_ ). As shown in Fig. 2, we partition the vector **k** _t_ (and similarly **q** _t_ )
into _U_ independent sub-vectors **k** _t_ = [ **k** [(1)] _t_ _[, . . .,]_ **[ k]** _t_ [(] _[U]_ [)] ], where each component has a dimension of
_dp_ = _dk/U_ . We then synthesize the full address vector over _M_ = ( _dp_ ) _[U]_ slots via the Kronecker
product ( _⊗_ ) with temperature scaling parameter _τ_, defined as the _U_ -order product softmax (where
_U_ denotes the construction order)::




**k** [(] _t_ _[u]_ [)]
_τ_







_ρU_ ( **k** _t_ ) =



_U_


Softmax

_u_ =1



_._ (4)



Notably, this enables scaling the memory capacity of state **S** _t_ to the high-dimensional space _M_
without introducing any additional model parameters. Moreover, compared to directly applying
Softmax over the massive dimension _M_, this product formulation ensures more effective gradient
propagation and optimization stability, leading to superior performance (see detailed analysis in Sec.
5.3).


**Top-** _K_ **truncation** _TK_ ( _·_ ). Subsequently, the truncation operator _TK_ enforces explicit sparsity by
retaining only the _K_ largest values and zeroing out the rest. The complete formulation of the address
decoding is given by:
_AK,U_ ( **k** _t_ ) = _TK_ ( _ρU_ ( **k** _t_ )) _._ (5)


This sparse addressing significantly reduces the computational complexity of subsequent state updates and retrieval operations, as only _K_ active slots require processing per step. At the same time,
top- _K_ can be combined with the structure of product softmax to achieve fast address decoding (see
detailed analysis in Sec. 4).


3.2 CYCLIC ADDRESS POSITIONAL EMBEDDING


Since the vectors **q** _t_ and **k** _t_ are derived solely from the input vector, the address decoding inherently
lacks sequence order information. To bridge this gap, we draw inspiration from Rotary Positional
Embeddings (RoPE) (Su et al., 2024) and propose _Cyclic Address Positional Embedding (CAPE)_,
which incorporates relative positional information via a cyclic shift operator _Pt_ ( _·_ ).


Specifically, _Pt_ performs a time-dependent circular shift on the address vector. For any vector
**a** _∈_ R _[M]_, the operator acts as a cyclic permutation defined by [ _Pt_ ( **a** )] _i_ = **a** ( _i_ + _t_ ) mod _M_ _._ By applying
this transformation, the interaction between a write operation at step _t_ and a read operation at step
_t_ _[′]_ becomes dependent on the relative distance _t −_ _t_ _[′]_ . This effectively converts absolute addressing
into relative addressing, allowing the model to capture temporal structures analogous to a temporal
convolution (especially when **k** _t_ and **q** _t_ are constant). In summary, the final write and read vectors
**w** _t_ and **r** _t_ are formally defined as:


**w** _t_ = _Pt_ ( _AK,U_ ( **k** _t_ )) _,_
(6)
**r** _t_ = _Pt_ ( _AK,U_ ( **q** _t_ )) _._


4


RAM-Net


3.3 MEMORY WRITE AND READ


Standard linear gating restricts memory updates to a rigid Exponential Moving Average (EMA),
where the retention weight is determined by the complement of the write weight 1 _−_ **w** _t_ _[⊤]_ [. This]
forces the model to forget historical information in exact proportion to the new input intensity,
thereby preventing independent control over information preservation.


To address this, we introduce Power Decay Moving Average (PDMA), which generalizes the aggregation mechanism by decoupling the forgetting rate from the writing intensity. The update rules are
defined as:
**S** _t_ = Diag(1 _−_ **w** _t_ ) _[γ]_ _·_ **S** _t−_ 1 + **w** _t_ _[⊤]_ **[v]** _[t][,]_ **S** 0 = **0** _,_



**z** _t_ = Diag(1 _−_ **w** _t_ ) _[γ]_ _·_ **z** _t−_ 1 + **w** _t_ _[⊤][,]_ **z** 0 = **1** _[⊤]_ _/M,_

**o** _t_ = **r** _t_ �Diag( **z** _t_ + _ϵ_ ) _[−]_ [1] _·_ **S** _t_ - _,_



(7)



where **1** _∈_ R [1] _[×][M]_ is a row vector of ones. Here, the hyperparameter _γ ≥_ 0 controls the non-linear
decay term (1 _−_ **w** _t_ _[⊤]_ [)] _[γ]_ [, while] **[ z]** _[t]_ [tracks the accumulated weight mass for dynamic normalization.]
This formulation creates a continuous spectrum of memory behaviors: as _γ →_ 0, the decay term
approaches unity, transforming the update into a Weighted Cumulative Mean that preserves longterm dependencies without attenuation. Conversely, setting _γ >_ 1 accelerates forgetting beyond the
standard linear decay ( _γ_ = 1), enabling the model to aggressively suppress historical noise in favor
of the most recent context.


4 IMPLEMENTATION


**Architecture configuration** . We employ a hybrid strategy that alternates between relative and absolute addressing modes. Shallow layers primarily utilize positional embedding to capture local
temporal dynamics. In contrast, deeper layers primarily employ absolute addressing by omitting the
cyclic shift operator _Pt_ in equ. 6.


**Training convergence acceleration** . We introduce a dynamic scalar re-parameterization. For the
projection matrices **W** _q_ and **W** _k_, we decompose each weight _wij_ into _e_ _[α]_ _· wij_ _[′]_ [, where] _[ α]_ [ is a learn-]
able per-head parameter. This learnable scalar is equivalent to a dynamic temperature of softmax,
ensuring effective gradient flow and facilitating faster convergence.

**Gradient stability** . For the non-linear decay term (1 _−_ **w** _t_ _[⊤]_ [)] _[γ]_ [ in PDMA, we address the critical]
gradient issues near the boundary **w** _t →_ 1, where the derivative becomes infinity when _γ <_ 1,
and vanishes to zero when _γ ≥_ 1. To resolve this, we employ a proxy gradient technique during
backpropagation, substituting the original derivative with that of a smoothed function ( _ϵ_ +(1 _−ϵ_ )(1 _−_
**w** _t_ )) _[γ]_ . This separation of computation paths ensures robust gradient flow without compromising the
exactness of the forward calculation.


**Computational efficiency** . During address decoding, directly applying Top- _K_ selection on an address vector of size _M_ = _d_ _[U]_ _p_ [entails prohibitive computational costs. To circumvent this, we avoid]
explicit vector construction by leveraging the combinatorial structure of the addresses. Specifically,
we employ a log-domain beam search strategy that iteratively merges and sorted candidates. This
approach ensures numerical stability and significantly reduces retrieval complexity from _O_ ( _d_ _[U]_ _p_ [)][ to]
_O_ ( _U · K_ [2] + _U · dp_ log _dp_ ).


To optimize execution, we develop specialized kernels for both training and inference. During
parallel training, we implement a segment-based approach where sequences are sorted to aggregate
sparse read/write operations into contiguous time-slots, enabling efficient batched computation. For
autoregressive inference, we leverage sparsity to directly access memory slots via decoded indices,
retrieving values without full-state iteration.


5 EXPERIMENTS


5.1 SYNTHETIC BENCHMARKS


We evaluate the retrieval capability of our model using the multi-query associative recall (MQAR)
task from the Zoology framework (Arora et al., 2024a). This task requires the model to memorize


5


RAM-Net


Figure 3: MQAR accuracy vs. total state size. Figure 4: Ablation study of product softmax
order _U_ .


a sequence of key-value pairs embedded within a long context and subsequently retrieve the correct value associated with a specific query key. Our main baselines include full attention (Vaswani
et al., 2017), linear attention (Katharopoulos et al., 2020), sliding window attention (Beltagy et al.,
2020), GLA (Yang et al., 2024), DeltaNet (Schlag et al., 2021a), Gated DeltaNet (Yang et al., 2024),
RWKV-7 (Peng et al., 2025), Mamba2 (Dao & Gu, 2024), and H3 (Fu et al., 2022). We test these
models across various configurations to cover a spectrum of memory capacities. All reported results
are obtained from our own implementations.


We present the comparative results in Fig. 3, which plots the MQAR accuracy against
the state size, representing the total memory overhead required during inference. Following the default Zoology configuration, we report the uniform average accuracy across seven
settings denoted as ( _N_ pairs _, L_ seq), representing the number of pairs and sequence length:
(4 _,_ 64) _,_ (8 _,_ 64) _,_ (16 _,_ 64) _,_ (32 _,_ 128) _,_ (64 _,_ 256) _,_ (128 _,_ 512), and (256 _,_ 1024). As illustrated, RAM-Net
consistently achieves superior retrieval accuracy across various state sizes. This effectively validates
that our memory mechanism demonstrates superior information storage efficiency compared to other
architectures.


5.2 LANGUAGE MODELING


**Baselines and Experimental Setup.** We compare our method against baselines including Transformer++ (Touvron et al., 2023), GLA (Yang et al., 2024), HGRN2 (Qin et al., 2024), Gated
DeltaNet (Yang et al., 2024), and Mamba2 (Dao & Gu, 2024). To ensure a fair comparison, all
models are pre-trained under a unified configuration using the Flame framework (Zhang & Yang,
2025), utilizing implementations from Flash Linear Attention (Yang & Zhang, 2024). We fix the
hidden size (model width) to 1024, and scale to 340M. Training is conducted on the FineWeb-Edu
dataset (Lozhkov et al., 2024) with a budget of 10B tokens. We employ the Llama 2 tokenizer (32k
vocabulary) and a context window of 4,096 tokens. We optimize the model using AdamW with a
peak learning rate of 1 _._ 0 _×_ 10 _[−]_ [3], a weight decay of 0 _._ 1, and gradient clipping of 1 _._ 0. The learning
rate follows a cosine decay schedule with a 500M token warm-up. The global batch size is set to
0.5M tokens, distributed across 8 NVIDIA H200 GPUs.


**Evaluation.** We evaluate performance using the Language Model Evaluation Harness (Gao et al.,
2024) on standard benchmarks: WikiText-103 (Merity et al., 2016), MMLU(Hendrycks et al., 2021),
ARC-Challenge/Easy (Clark et al., 2018), OpenbookQA (Mihaylov et al., 2018), SciQ (Welbl et al.,
2017), COPA (Roemmele et al., 2011), PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019),
and WinoGrande (Sakaguchi et al., 2019). The comparative results are summarized in Table 1.
RAM-Net achieves comparable performance with other SOTA methods.


6


RAM-Net


Table 1: Zero-shot performance comparison on language modeling and common-sense reasoning.









|Model|Wiki. MMLU ARC-e ARC-c OBQA SciQ COPA PIQA Hella. Wino.<br>ppl ↓ acc ↑ acc ↑ acc ↑ accn ↑ acc ↑ acc ↑ acc ↑ acc ↑ acc ↑|Params.2 Active State3<br>size ↓ per token ↓|
|---|---|---|
|Transformer++<br>GLA<br>HGRN2<br>Gated DeltaNet1<br>Mamba21<br>RAM-Net (Top-8)|35.96<br>23.0<br>55.7<br>24.0<br>**34.2**<br>82.9<br>67.0<br>65.0<br>32.5<br>50.6<br>34.37<br>23.0<br>56.1<br>21.3<br>31.2<br>79.1<br>66.0<br>64.1<br>31.5<br>50.8<br>30.58<br>23.2<br>**59.1**<br>**24.6**<br>33.0<br>80.7<br>**73.0**<br>**67.2**<br>32.9<br>51.1<br>**28.79**<br>23.0<br>59.0<br>23.5<br>32.2<br>**83.1**<br>66.0<br>66.7<br>**33.2**<br>51.7<br>29.96<br>**24.1**<br>56.8<br>24.4<br>32.2<br>81.4<br>71.0<br>66.8<br>33.1<br>**51.9**<br>32.33<br>23.3<br>57.1<br>**24.6**<br>**34.2**<br>79.4<br>68.0<br>65.3<br>31.7<br>50.9|341.1M<br>50.3M<br>341.7M<br>3.1M<br>341.1M<br>3.1M<br>347.0M<br>8.5M<br>349.6M<br>12.9M<br>**340.8M**<br>**0.4M**|


1 Include an additional cross-token short convolution layer with kernel size _k_ = 4; other models use token-wise projections only.
2 The number of trainable parameters, while the word embedding parameters are excluded to ensure consistency with tied word embeddings.
3 Number of activated states update/read per token, directly reflecting compute and memory-bandwidth demand (Transformer++ is calculated
at sequence length _L_ = 1024).


5.3 ABLATION STUDY


We conduct an ablation study to validate the design choices of the Address Decoder. Specifically,
we decouple the impact of product softmax order _U_ (the Product Softmax order of **k** and **q** ) from the
total memory capacity _M_, analyzing how each factor contributes to the model’s retrieval capability.


**Product Softmax Order** _U_ **.** First, we analyze the impact of the Product Softmax order _U_ . We treat
the setting _U_ = 1 as a baseline, where the network applies softmax directly across the entire massive
dimension of memory slots without decomposition. In contrast, configurations with _U >_ 1 employ
our proposed product softmax formulation to derive addresses from smaller sub-vectors.


As illustrated in Fig. 4, we observe a significant improvement in MQAR accuracy as _U_ increases.
We attribute this performance gain to the superior gradient dynamics facilitated by the product softmax structure. For the non-decomposed baseline ( _U_ = 1), the combination of global Softmax and
Top- _K_ truncation results in extreme gradient sparsity on **r** and **w**, where at most _K/M_ = _K/_ ( _dp_ ) _[U]_
elements receive non-zero gradients. This sparsity induces high variance in gradient estimation,
leading to optimization instability. In contrast, the Kronecker product structure acts as an efficient
gradient distributor. During backpropagation, it distributes effective gradients from **r** and **w** to all
_U_ sub-vectors, ensuring that each sub-vector receives gradient feedback on at least 1 _/dp_ of its elements. This mechanism effectively reduces gradient variance, ensuring stable convergence even
with high-dimensional memory states.


**Memory Capacity** _M_ **.** Next, we examine the scalability of memory capacity _M_ . The results
demonstrate a clear positive correlation between the number of memory slots and retrieval performance. As shown in Fig. 4, increasing _M_ consistently leads to higher MQAR accuracy. A larger
memory capacity enables the model to map distinct semantic features to non-overlapping slots more
effectively. This reduction in superposition preserves high-fidelity information and ensures higher
precision in long-range retrieval.


6 RELATED WORK


**Linear Sequence Modeling** To mitigate the quadratic complexity of standard Transformers, linear sequence models compress the unbounded historical context into a fixed-size recurrent state
(Katharopoulos et al., 2020; Sun et al., 2023). Early approaches, such as Linear Transformers
(Katharopoulos et al., 2020) and RetNet (Sun et al., 2023), typically employ kernel-based feature
maps or time-invariant decay mechanisms to aggregate context. While efficient, these methods rely
on static dynamics that fail to adaptively filter information based on input content.


To enhance expressivity, recent architectures have introduced data-dependent dynamics. Models
like Mamba2 (Dao & Gu, 2024), RWKV7 (Peng et al., 2023), and HGRN2 (Qin et al., 2024) utilize input-driven gating mechanisms to actively modulate information retention. Furthermore, approaches such as DeltaNet (Schlag et al., 2021b), Gated DeltaNet (Yang et al., 2024), and Kimi Linear (Team et al., 2025) adopt the Delta Rule, formulating the state update as a dynamic retrieve-andrewrite process. This mechanism can be generalized as **S** _t_ = _W_ ( **S** _t−_ 1 _,_ **k** _t, β_ ( **v** _t −R_ ( **S** _t−_ 1 _,_ **k** _t_ ))),
where _R_ represents the retrieval of existing knowledge from the state **S** _t−_ 1 using the key **k** _t_, and
the update operator _W_ incorporates the error-driven correction weighted by _β_ . This formulation


7


RAM-Net


interprets the recurrent update as an online optimization step, allowing for more precise memory
editing (Schlag et al., 2021b; Sun et al., 2024).


Despite these advancements in update rules, purely linear models share a fundamental bottleneck:
the fixed state capacity. Compressing a long sequence into a compact hidden state inevitably results
in memory collisions and information loss, limiting performance on fine-grained retrieval tasks over
long horizons. In contrast, RAM-Net projects historical context into a expanded high-dimensional
memory space. By leveraging sparse read-write mechanisms, our approach isolates distinct semantic
features into separate slots, thereby minimizing memory superposition and ensuring high-fidelity
retrieval.


**Vector Quantization and Explicit Memory** To bypass the limitations of compressed recurrent
states, several architectures employ explicit memory slots as storage units. Approaches such as
Transformer-VQ (Lingle, 2023) and PQCache (Zhang et al., 2025) utilize Vector Quantization (VQ)
to map inputs to discrete memory entries. While efficient, these methods rely on codebook-based
addressing involving non-differentiable selection operators (e.g., argmax), which necessitates approximation techniques like the Straight-Through Estimator (STE) (Bengio et al., 2013) for training.


In contrast to discrete quantization, Neural Turing Machines (NTM) (Graves et al., 2014) employ
differentiable memory mechanisms. However, they rely on simulating the movement of a read-write
head to access memory, resulting in an indirect slot selection mechanism. Conversely, RAM-Net
distinguishes itself by implementing a fully differentiable address decoder that projects inputs into
sparse high-dimensional address vectors, enabling direct slot selection. This design avoids the nondifferentiable operations present in VQ methods while eliminating the sequential head-shifting logic
of NTMs.


7 DISCUSSION


The explicit addressing design of RAM-Net provides inherent properties beneficial for both model
interpretability and system efficiency. By explicitly tracking memory updates and retrievals via
vectors **w** _t_ and **r** _t_, the model offers granular insights into token interactions and the functional roles
of individual memory slots. Moreover, the sparse access pattern is particularly advantageous for
hardware-constrained environments. It allows for hierarchical memory management, where the full
state resides in cheaper storage (e.g., CPU RAM or SSD), and only frequently accessed hot slots are
dynamically cached in VRAM via policies like Least Recently Used (LRU), significantly reducing
the GPU memory overhead.


8 CONCLUSION


In this work, we propose RAM-Net to reconcile the trade-off between the high fidelity of full attention and the memory efficiency of linear models. RAM-Net introduces a differentiable address
decoding mechanism that maps inputs to explicit high-dimensional memory slots. This paradigm allows state capacity to scale independently of model parameters. Empirical validation across retrieval
and language modeling tasks demonstrates that RAM-Net offers a robust solution for processing
complex sequences while maintaining the efficiency benefits of sparse computation.


REFERENCES


Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri
Rudra, and Christopher R´e. Zoology: Measuring and improving recall in efficient language models. In _Proceedings of 12th International Conference on Learning Representations (ICLR)_ . ICLR,
2024a.


Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley,
James Zou, Atri Rudra, and Christopher R´e. Simple linear attention language models balance
the recall-throughput tradeoff. _arXiv preprint arXiv:2402.18668_, 2024b.


Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
_arXiv preprint arXiv:2004.05150_, 2020.


8


RAM-Net


Yoshua Bengio, Nicholas L´eonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. _arXiv preprint arXiv:1308.3432_, 2013.


Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In _Proceedings of the AAAI conference on artificial intelligence_,
volume 34, pp. 7432–7439, 2020.


Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention
with performers. _arXiv preprint arXiv:2009.14794_, 2020.


Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and
Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge,
[2018. URL https://arxiv.org/abs/1803.05457.](https://arxiv.org/abs/1803.05457)


Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through
structured state space duality. _arXiv preprint arXiv:2405.21060_, 2024.


Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R´e. Flashattention: Fast and memoryefficient exact attention with io-awareness. _Advances in neural information processing systems_,
35:16344–16359, 2022.


Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R´e.
Hungry hungry hippos: Towards language modeling with state space models. _arXiv preprint_
_arXiv:2212.14052_, 2022.


Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang
Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model
[evaluation harness, 07 2024. URL https://zenodo.org/records/12608602.](https://zenodo.org/records/12608602)


Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014. [URL https:](https://arxiv.org/abs/1410.5401)
[//arxiv.org/abs/1410.5401.](https://arxiv.org/abs/1410.5401)


Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In _First_
_conference on language modeling_, 2024.


Albert Gu, Karan Goel, and Christopher R´e. Efficiently modeling long sequences with structured
state spaces. _arXiv preprint arXiv:2111.00396_, 2021.


Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. [URL https:](https://arxiv.org/abs/2009.03300)
[//arxiv.org/abs/2009.03300.](https://arxiv.org/abs/2009.03300)


Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc¸ois Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. In _International conference on ma-_
_chine learning_, pp. 5156–5165. PMLR, 2020.


Lucas D Lingle. Transformer-vq: Linear-time transformers via vector quantization. _arXiv preprint_
_arXiv:2309.16354_, 2023.


Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017. URL
[https://arxiv.org/abs/1608.03983.](https://arxiv.org/abs/1608.03983)


[Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL https:](https://arxiv.org/abs/1711.05101)
[//arxiv.org/abs/1711.05101.](https://arxiv.org/abs/1711.05101)


Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest
[collection of educational content, 2024. URL https://huggingface.co/datasets/](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)
[HuggingFaceFW/fineweb-edu.](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)


Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan
May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. _arXiv preprint_
_arXiv:2209.10655_, 2022.


9


RAM-Net


Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
[models, 2016. URL https://arxiv.org/abs/1609.07843.](https://arxiv.org/abs/1609.07843)


Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. _arXiv preprint arXiv:1809.02789_,
2018.


Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman,
Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv: Reinventing rnns for
the transformer era. _arXiv preprint arXiv:2305.13048_, 2023.


Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Xingjian Du, Haowen Hou, Jiaju Lin,
Jiaxing Liu, Janna Lu, William Merrill, et al. Rwkv-7” goose” with expressive dynamic state
evolution. _arXiv preprint arXiv:2503.14456_, 2025.


Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua
Bengio, Stefano Ermon, and Christopher R´e. Hyena hierarchy: Towards larger convolutional
language models. In _International Conference on Machine Learning_, pp. 28043–28078. PMLR,
2023.


Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong.
Hgrn2: Gated linear rnns with state expansion, 2024. [URL https://arxiv.org/abs/](https://arxiv.org/abs/2404.07904)
[2404.07904.](https://arxiv.org/abs/2404.07904)


Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive
transformers for long-range sequence modelling. _arXiv preprint arXiv:1911.05507_, 2019.


Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In _2011 AAAI Spring Sympo-_
_sium Series_ [, 2011. URL https://people.ict.usc.edu/˜gordon/publications/](https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF)
[AAAI-SPRING11A.PDF.](https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF)


Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver[sarial winograd schema challenge at scale, 2019. URL https://arxiv.org/abs/1907.](https://arxiv.org/abs/1907.10641)
[10641.](https://arxiv.org/abs/1907.10641)


Imanol Schlag, Kazuki Irie, and J¨urgen Schmidhuber. Linear transformers are secretly fast weight
programmers. In Marina Meila and Tong Zhang (eds.), _Proceedings of the 38th International_
_Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pp.
[9355–9366. PMLR, 18–24 Jul 2021a. URL https://proceedings.mlr.press/v139/](https://proceedings.mlr.press/v139/schlag21a.html)
[schlag21a.html.](https://proceedings.mlr.press/v139/schlag21a.html)


Imanol Schlag, Kazuki Irie, and J¨urgen Schmidhuber. Linear transformers are secretly fast weight
programmers. In _International conference on machine learning_, pp. 9355–9366. PMLR, 2021b.


Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.


Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei
Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive
hidden states. _arXiv preprint arXiv:2407.04620_, 2024.


Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and
Furu Wei. Retentive network: A successor to transformer for large language models. _arXiv_
_preprint arXiv:2307.08621_, 2023.


Kimi Team, Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin
Men, Songlin Yang, Zhiyuan Li, Wentao Li, Enzhe Lu, Weizhou Liu, Yanru Chen, Weixin Xu,
Longhui Yu, Yejie Wang, Yu Fan, Longguang Zhong, Enming Yuan, Dehao Zhang, Yizhi Zhang,
T. Y. Liu, Haiming Wang, Shengjun Fang, Weiran He, Shaowei Liu, Yiwei Li, Jianlin Su, Jiezhong
Qiu, Bo Pang, Junjie Yan, Zhejun Jiang, Weixiao Huang, Bohong Yin, Jiacheng You, Chu Wei,
Zhengtao Wang, Chao Hong, Yutian Chen, Guanduo Chen, Yucheng Wang, Huabin Zheng, Feng
Wang, Yibo Liu, Mengnan Dong, Zheng Zhang, Siyuan Pan, Wenhao Wu, Yuhao Wu, Longyu


10


RAM-Net


Guan, Jiawen Tao, Guohong Fu, Xinran Xu, Yuzhi Wang, Guokun Lai, Yuxin Wu, Xinyu Zhou,
Zhilin Yang, and Yulun Du. Kimi linear: An expressive, efficient attention architecture, 2025.
[URL https://arxiv.org/abs/2510.26692.](https://arxiv.org/abs/2510.26692)


Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee
Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
[language models, 2023. URL https://arxiv.org/abs/2302.13971.](https://arxiv.org/abs/2302.13971)


Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural informa-_
_tion processing systems_, 30, 2017.


Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention
with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.


Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions,
[2017. URL https://arxiv.org/abs/1707.06209.](https://arxiv.org/abs/1707.06209)


Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations
of linear attention mechanism, January 2024. [URL https://github.com/fla-org/](https://github.com/fla-org/flash-linear-attention)
[flash-linear-attention.](https://github.com/fla-org/flash-linear-attention)


Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with
delta rule. _arXiv preprint arXiv:2412.06464_, 2024.


Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma[chine really finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830.](https://arxiv.org/abs/1905.07830)


Hailin Zhang, Xiaodong Ji, Yilin Chen, Fangcheng Fu, Xupeng Miao, Xiaonan Nie, Weipeng Chen,
and Bin Cui. Pqcache: Product quantization-based kvcache for long context llm inference. _Pro-_
_ceedings of the ACM on Management of Data_, 3(3):1–30, 2025.


Yu Zhang and Songlin Yang. Flame: Flash language modeling made easy, January 2025. URL
[https://github.com/fla-org/flame.](https://github.com/fla-org/flame)


11


RAM-Net


A LANGUAGE MODELING EXPERIMENT


Table 2 details the experimental setup. For RAM-Net, we consistently use a Top-8 setting for both
training and evaluation. Regarding positional encoding, we apply CAPE to all heads in the initial 4
layers and half of the heads in the subsequent 4 layers, while the remaining heads operate without
positional embeddings. The complete training configuration is summarized in Table 3.







|Model|Width Layers Heads Active State Total State Other Configure<br>hidden size number of stack attn. heads per token per token feature size|
|---|---|
|Transformer++<br>GLA<br>HGRN2<br>GDN<br>Mamba2<br>RAM-Net (Top-8)|1024<br>24<br>16<br>50.3M<br>50.3M<br>_dk_ =_ dv_ = 64<br>1024<br>24<br>4<br>3.1M<br>3.1M<br>_dk_ = 128_, dv_ = 256<br>1024<br>24<br>8<br>3.1M<br>3.1M<br>_df_ =_ di_ = 128<br>1024<br>21<br>3<br>8.5M<br>8.5M<br>_dh_ = 256_, dv_ = 512_, kconv_ = 4<br>1024<br>48<br>32<br>12.9M<br>12.9M<br>_dh_ = 64_, dssm_ = 128_, kconv_ = 4<br>1024<br>27<br>16<br>0.4M<br>28.8M<br>_dv_ = 64_, U_ = 5_, dp_ = 4|


Table 2: **Comparison of architectural configurations and state overheads.** _Active State_ denotes
the effective state size utilized during per-token computation, whereas _Total State_ represents the full
state size. Here, Transformer++ is evaluated at sequence length 1024.


**Category** **Parameter** **Value**


Dataset FineWeb-Edu (Lozhkov et al., 2024)
**Data**
Training budget 10B tokens


Tokenizer Llama 2 (Touvron et al., 2023)
**Tokenizer** Vocabulary size 32k
Context window 4,096


Optimizer AdamW (Loshchilov & Hutter, 2019)
**Optimization** Weight decay 0.1
Gradient clipping 1.0



**LR Schedule**



Schedule method Cosine decay (Loshchilov & Hutter, 2017)
Linear Warm-up 500M tokens
Peak LR 1 _._ 0 _×_ 10 _[−]_ [3]

Min LR 1 _._ 0 _×_ 10 _[−]_ [4]



Batch per device 65.5K tokens
**Batch** Global batch size 0.5M tokens
Device number 8


Table 3: Training configuration summary.


B MEMORY ACCESS PATTERN


To investigate the memory interaction dynamics of RAM-Net, we visualize the memory access
traces in Fig. 5. This figure depicts the slot activation patterns of four representative heads selected
from different layers, where read and write events exhibit distinct diagonal shifts attributable to positional encoding offsets. Specifically, panels (a) and (b) correspond to heads utilizing CAPE. In contrast, panels (c) and (d) operate without CAPE, displaying position-agnostic patterns driven purely
by semantic content. Collectively, these traces reveal a hybrid mechanism of fixed and dynamic
accesses. The observed spatiotemporal locality in these patterns highlights significant potential for
future system-level optimizations, such as caching strategies.


12


RAM-Net


Figure 5: Visualization of memory access traces: read (green) and write (red) events across memory
slots over time (tokens).


13


