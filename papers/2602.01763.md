## **A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**

**Xiaowei Ye** [* 1] **Xiaoyu He** [* 2] **Chao Liao** [2] **Chen Wu** [2] **Pinyan Lu** [2 3]



**Abstract**


Transformers serve as the foundation of most
modern large language models. To mitigate the
quadratic complexity of standard full attention,
various efficient attention mechanisms, such as
linear and hybrid attention, have been developed.
A fundamental gap remains: their expressive
power relative to full attention lacks a rigorous
theoretical characterization. In this work, we theoretically characterize the performance differences
among these attention mechanisms. Our theory
applies to all linear attention variants that can
be formulated as a recurrence, including Mamba,
DeltaNet, etc. Specifically, we establish an expressiveness hierarchy: for the sequential function composition-a multi-step reasoning task that
must occur within a model’s forward pass, an
( _L_ + 1)-layer full attention network is sufficient,
whereas any hybrid network interleaving _L −_ 1
layers of full attention with a substantially larger
number (2 [3] _[L]_ [2] ) of linear attention layers cannot
solve it. This result demonstrates a clear separation in expressive power between the two types
of attention. Our work provides the first provable
separation between hybrid attention and standard
full attention, offering a theoretical perspective for
understanding the fundamental capabilities and
limitations of different attention mechanisms.


**1. Introduction**


The Transformer architecture (Vaswani et al., 2017) serves
as the backbone of modern large language models (LLMs),
exemplified by numerous recent models (OpenAI, 2023;
DeepSeek-AI, 2024; MiniMax, 2025b; Team et al., 2025;
Yang et al., 2025a). Its core component is the _self-attention_


*Equal contribution ; work completed during internship
at Huawei 1 ´Ecole Polytechnique, Palaiseau, France 2Taylor
Lab, Huawei [3] Key Laboratory of Interdisciplinary Research
of Computation and Economics, Shanghai University of Finance and Economics, Shanghai, China. Correspondence
to: Xiaoyu He _<_ hexiaoyu12@huawei.com _>_, Pinyan Lu
_<_ lu.pinyan@mail.shufe.edu.cn _>_ .


_Preprint. February 3, 2026._



_unit_ : in the standard full attention mechanism, it models interactions among input elements as inner-products between
low-dimensional embeddings and calculates the output via
a softmax weighted sum.


Despite the success of standard full attention, its quadratic
computation and linear-memory complexity remain significant bottlenecks. Hence, various new attention mechanisms
have been proposed, including linear attention (Katharopoulos et al., 2020; Kasai et al., 2021; Schlag et al., 2021;
Peng et al., 2021; Yang et al., 2024) used in models such
as Mamba (Gu & Dao, 2024), Minimax-M1 (MiniMax,
2025a), RWKV (Peng et al., 2025) and Gated DeltaNet
(Yang et al., 2025b); linear-full hybrid attention used in
models such as Hunyuan-TurboS (Team, 2025b), Qwen3Next (QwenTeam, 2025), and Kimi Linear (Team, 2025a));
as well as log-linear attention (Guo et al., 2025) and sparse
attention mechanisms (Guo et al., 2019; Qiu et al., 2020;
Zaheer et al., 2020; Beltagy et al., 2020; Guo et al., 2022;
Yuan et al., 2025; Lu et al., 2025). A natural question arises:
_Can these attention mechanisms perform better than full_
_attention?_ This paper aims to answer this question through
a theoretical comparison of these attention mechanisms with
full attention.


We first analyze the Transformer with hybrid attention mechanisms on the sequential function composition task introduced by (Chen et al., 2025). Our results show that even
when the number of linear attention layers grows exponentially relative to the number of full attention layers, the
performance gain remains marginal.


Moreover, we examine sparse attention mechanisms. We
establish a hardness result for the 2-Sum task under a singlelayer sparse attention setup, providing the first provable
separation between sparse attention and full attention.


**1.1. Our result**


Consider a Transformer with _H_ attention heads, head dimension _d_, precision _p_, and prompt length _n_ . We establish
the following lower bound for a class of hybrid attention
mechanisms.


**Theorem 1.1.** _For any L, an_ ( _L −_ 1 _,_ 2 [3] _[L]_ [2] _, · · ·,_ 2 [3] _[L]_ [2] ) _-_
_hybrid Transformer cannot solve L-sequential function com-_
_position whenever Hdp ≤_ _n_ [2] _[−]_ [4] _[L][−]_ [2] _._



1


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**



Our analysis covers a broad class of linear attention mechanisms that admit a recurrent formulation—including Mamba
(Gu & Dao, 2024), Minimax-M1 (MiniMax, 2025a), RWKV
(Peng et al., 2025) and Gated DeltaNet (Yang et al.,
2025b)—establishing a general framework for comparison.


The formal definitions of an ( _L, a_ 1 _, · · ·, aL_ )-hybrid transformer and _L_ -sequential function composition are given in
Definitions 2.3 and 2.5. Together with the findings in (Chen
et al., 2025), our result implies that incorporating linear attention layers does not yield substantial performance gains.
A detailed comparison is presented in Table 1.


_Table 1._ Complexity of _L_ -FuncComp for different architecture.

|Model/Task|L-SeqCom|
|---|---|
|Full -_ L_ layers|Ω(poly_ n_) (Chen et al., 2025)|
|Full -_ L_ + 1 layers|_O_(poly log_ n_) (Chen et al., 2025)|
|~~Hybrid -~~<br>(_L −_1_,_ 23_L_2)<br>layers|Ω(poly_ n_) (Theorem 1.1)|



We give some remarks on the _L_ -sequential function composition task, which we discuss in this paper. This task
formally captures the essence of multi-step reasoning (e.g.,
multi-hop retrieval) that must occur within a model’s forward pass, where the solution requires composing functions
sequentially, with the output of one step defining the input
context for the next. Our theoretical instantiation employs
carefully constrained “retrieval scopes” at each step to enable rigorous analysis. This task is of a theoretical nature,
and the implications of our results in the real world demand
verification of practice.


We also analyze sparse attention mechanisms.

**Theorem 1.2.** _Any single-layer_ ( _B, k_ ) _-sparse attention solv-_
_ing the_ 2 _-Sum must satisfy Hdp_ = Ω( _B_ log _n_ ) _, while a_
_single-layer full attention can solve it with H_ = 1 _, d_ = 3 _,_
_and p_ = log _n._


This result demonstrates a strict efficiency gap for large
_B_ . The ( _B, k_ )-sparse attention mechanism and the 2-Sum
problem are formally defined in Definitions 2.4 and 2.6,
respectively. Our proofs adopt a methodology based on
communication complexity. We provide an overview of our
results in Table 2.


_Table 2._ Complexity of full and sparse attention for 2-Sum.

|Model|Complexity|
|---|---|
|Full|_O_(log_ n_) (Sanford et al., 2023)|
|Sparse (Block)|Ω(_B_ log_ n_)|



**1.2. Organization and overview**


Section 2 provides the mathematical background, including
formal definitions of attention mechanisms and the studied



tasks: sequential function composition and 2-Sum. Section
3 introduces communication protocol for hybrid attention
mecanisms to solve sequential function composition. In
Section 4, we adapt methods from (Chen et al., 2025) to
derive a lower bound for a hybrid architecture, thus proving
Theorem 1.1. Finally, Section 5 establishes Theorem 1.2,
offering the theoretical limitation on sparse attention.


**1.3. Related work**


**Transformer-RNN comparison.** There have been several
results on the theoretical comparison between Transformers
and RNNs (Elman, 1990). (Jelassi et al., 2024) demonstrated a representation gap between RNNs and Transformers in repeating a long sequence. (Sanford et al., 2023)
proved a linear-constant separation of RNNs and Transformers on the sparse averaging task, and later a separation on
_k_ -hop induction head tasks (Sanford et al., 2024a).


(Bhattamishra et al., 2024) establishes computational separations between Transformers and RNNs on tasks such
as Index Lookup, String Equality, Nearest Neighbor, and
Associative Recall. Additionally, they prove a linear lower
bound for one-layer Transformers on bounded Dyck languages (Hahn, 2020), forming a separation as constant-size
RNNs solve this task (Bhattamishra et al., 2020; Hewitt
et al., 2020). In practice, however, one-layer Transformers
are rarely used. Multi-layer Transformers are shown to succeed on bounded Dyck languages (Wen et al., 2023; Yao
et al., 2021), indicating that Transformers match RNNs on
language recognition while excelling at retrieval.


Further work by (Wen et al., 2025) extends the comparison
to one-layer RNNs augmented with chain-of-thought (CoT).
Although CoT improves RNN performance, it fails to close
the representational gap with Transformers: under mild
complexity assumptions, an RNN with CoT is strictly more
powerful than a plain RNN but still exponentially weaker
than a CoT-enhanced Transformer on algorithmic problems.
(Wen et al., 2025) also suggest that RAG-based improvements can achieve Turing completeness, offering ways to
narrow the gap. One proposed approach uses a hybrid RNNTransformer mechanism, which has been implemented in
models like Hunyuan-TurboS (Team, 2025b), Qwen3-Next
(QwenTeam, 2025), and Kimi Linear (Team, 2025a).


We make some contributions to the comparative expressiveness of recurrent and transformer-based models. In
particular, we establish a new separation result for hybrid
architectures on the sequential function composition task.


**Theoretical study of Transformer.** Theoretical research on
Transformers has progressed along two main fronts: expressive power and inherent limitations. The line on expressive
power began with (Perez et al.´, 2019)’s investigation of their
ability to emulate Turing machines, followed by studies on



2


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**



adversarial robustness (Hsieh et al., 2019), universal approximation (Yun et al., 2020; Wei et al., 2022a), and Turing
completeness (P´erez et al., 2021; Wei et al., 2022a).


Concerning limitations, (Hahn, 2020) pioneered this direction by proving that hard-attention Transformers cannot
recognize parity or Dyck languages. Subsequent work established further limitations—either via communication complexity for one-layer model (Peng et al., 2024; Sanford et al.,
2023; 2024a;b) or under circuit complexity conjectures (Sanford et al., 2024a; Peng et al., 2024; Merrill & Sabharwal,
2023). A significant advance was made by (Chen et al.,
2025), who proved the first unconditional lower bound for
multi-layer Transformers (on sequential function composition) by introducing the concept of indistinguishable decomposition. In this paper, we adapt their techniques to derive a
lower bound for hybrid architectures.


**Chain of Thought (CoT).** Chain-of-Thought (CoT) (Wei
et al., 2022b) enhances the reasoning by inducing step-bystep reasoning traces, thereby providing Transformers with
an augmented computational workspace. This augmentation
confers significant expressive benefits: constant-size Transformers with CoT are known to simulate any polynomialtime algorithm (Perez et al.´, 2021; Merrill & Sabharwal,
2024; Feng et al., 2023; Li et al., 2024; Li & Wang, 2025).
Given that Transformers (without CoT) are believed to be
limited to the complexity class TC [0] (Merrill & Sabharwal,
2023), these results suggest (assuming standard conjectures
like P _̸⊂_ TC [0] ) that CoT strictly improves the computational power of Transformers. Notably, (Chen et al., 2025)
recently proved the first unconditional separation for CoT,
introducing a key proof technique for such results. They
prove that a single-layer small Transformer with CoT can
solve the _L_ -sequential function composition task, while an
_L_ -layer small Transformer cannot.


Complementary to these expressiveness results, another line
of work establishes lower bounds on the number of CoT
steps necessary for specific tasks. Lower bounds have been
shown for single-layer Transformers on iterated function
composition (Peng et al., 2024). Other works connect the required step count to structural complexity measures like the
Ehrenfeucht-Haussler rank (Barcelo et al., 2025) or establish bounds for various problems (e.g., parity, multiplication,
median, graph reachability) under restricted attention patterns (Amiri et al., 2025).


**Sparse attention mechanisms.** The quadratic complexity
of full attention creates a significant computation bottleneck
for long-context processing. Sparse attention mechanisms,
leveraging inherent sparsity in attention matrices (Ge et al.,
2024; Jiang et al., 2023), have emerged as a key approach to
improve efficiency. Such mechanisms have been applied in
many models such as MoBA (Lu et al., 2025), NSA (Yuan
et al., 2025), DSA (DeepSeek-AI, 2025), etc.



Despite their widespread adoption for computational efficiency, we establish a fundamental limitation of (singlelayer) sparse attention: on tasks requiring uniform attention
across all input tokens, any sparse mechanism is provably
less powerful than full attention. We provide a formal lower
bound for compression- and selection-based sparse strategies, which constitute two of the three canonical categories
outlined by (Yuan et al., 2025) (the third, sliding window, is
functionally analogous to an RNN layer).


**2. Preliminaries**


We begin by formalizing the key components of our study.
This section first introduces the attention mechanisms we
analyze: full attention, linear attention, log-linear attention,
and hybrid architectures. We then formally define the primary tasks: function evaluation, permutation composition,
and sequential function composition.


**Notation.** For integers _n ≥_ 0 and _n_ 2 _≥_ _n_ 1, we use the
notation [ _n_ ] = _{_ 1 _,_ 2 _, . . ., n}_ and [ _n_ 1 : _n_ 2] = _{n_ 1 _, n_ 1 +
1 _, . . ., n_ 2 _}_, with the convention [0] = ∅.


**2.1. Transformer with full attention**


We consider the decoder-only Transformer architecture. Following the notations of (Chen et al., 2025), let _L_ be the number of attention layers, _H_ be the number of attention heads
per layer, _p_ be the precision, _d_ be the model dimension, and
_n_ be the (input) prompt length. It is typically assumed that
_Hdp ≥_ _O_ (log( _n_ )).


An _L_ -layer decoder-only Transformer consists of alternating
attention layers and MLP layers:

_f_ tran = _f_ mlp [(] _[L]_ [)] _[◦]_ _[f]_ attn [ (] _[L]_ [)] _[◦· · · ◦]_ _[f]_ mlp [ (1)] _[◦]_ _[f]_ attn [ (1)]

Given an input sequence _x_ [(0)] = ( _x_ [(0)] 1 _[, . . ., x]_ _n_ [(0)][)] _[ ∈]_ [(][R] _[dH]_ [)] _[n]_ [,]
the Transformer inductively computes the output of the _ℓ_ -th
attention layer _y_ [(] _[ℓ]_ [)] = ( _y_ 1 [(] _[ℓ]_ [)] _[, . . ., y]_ _n_ [(] _[ℓ]_ [)][)][ and the output of the]
_ℓ_ -th MLP layer _x_ [(] _[ℓ]_ [)] = ( _x_ [(] 1 _[ℓ]_ [)] _[, . . ., x]_ _n_ [(] _[ℓ]_ [)][)][. For layer] _[ ℓ]_ _[∈]_ [[] _[L]_ []][.]

_•_ **Attention layer** _f_ attn [(] _[ℓ]_ [)] [: For each attention head] _[ h][ ∈]_ [[] _[H]_ []]
and position _i ∈_ [ _n_ ], the output is computed as


   _yi_ [(] _[ℓ,h]_ [)] = _αi,j_ [(] _[ℓ,h]_ [)] _V_ [(] _[ℓ,h]_ [)] _x_ [(] _j_ _[ℓ][−]_ [1)] _∈_ R _[d]_ (1)

_j≤i_


where _{αi,j_ [(] _[ℓ,h]_ [)] _}j≤i_ is the attention score from the _h_ -th head,
given by the softmax operation

exp(( _x_ [(] _i_ _[ℓ][−]_ [1)] ) _[⊤]_ ( _Q_ [(] _[ℓ,h]_ [)] ) _[⊤]_ _K_ [(] _[ℓ,h]_ [)] _x_ [(] _j_ _[ℓ][−]_ [1)] )
_αi,j_ [(] _[ℓ,h]_ [)] = _._

~~�~~ exp(( _x_ [(] _i_ _[ℓ][−]_ [1)] ) _[⊤]_ ( _Q_ [(] _[ℓ,h]_ [)] ) _[⊤]_ _K_ [(] _[ℓ,h]_ [)] _x_ [(] _j_ _[ℓ][−]_ [1)] )

_j≤i_


Here, _Q_ [(] _[ℓ,h]_ [)] _, K_ [(] _[ℓ,h]_ [)] _, V_ [(] _[ℓ,h]_ [)] _∈_ R _[d][×][dH]_ denote the query,
key, and value matrices of the _h_ -th head, with each entry
represented using _p_ -bit precision.



3


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**



Finally, the output of the _ℓ_ -th attention layer is the concatenation of each head,


_yi_ [(] _[ℓ]_ [)] = ( _yi_ [(] _[ℓ,]_ [1)] _, . . ., yi_ [(] _[ℓ,H]_ [)] ) _∈_ R _[dH]_ _∀i ∈_ [ _n_ ]


_•_ **MLP layer** _f_ mlp [(] _[ℓ]_ [)] [: The output of the] _[ ℓ]_ [-th layer (and also]
the input to the ( _ℓ_ + 1)-th layer) is an arbitrary function
_g_ [(] _[ℓ]_ [)] : R [2] _[dH]_ _→_ R _[dH]_ applied position-wise:


_x_ [(] _i_ _[ℓ]_ [)] = _g_ [(] _[ℓ]_ [)] ( _x_ [(] _i_ _[ℓ][−]_ [1)] _, yi_ [(] _[ℓ]_ [)][)] _[ ∈]_ [R] _[dH]_ _[.]_


Note that here we modify the definition of the MLP layer
to adapt to various types of transformer architectures and
residual connections (Zhu et al., 2025; Xie et al., 2025).


**2.2. Linear attention, RNN, and hybrid attention**


For linear attention, the attention probabilities become


_φ_ ( _Q_ [(] _[ℓ,h]_ [)] _x_ [(] _i_ _[ℓ][−]_ [1)] ) _[⊤]_ _φ_ ( _K_ [(] _[ℓ,h]_ [)] _x_ [(] _j_ _[ℓ][−]_ [1)] )
_αi,j_ [(] _[ℓ,h]_ [)] =

~~�~~ _j≤i_ _[φ]_ [(] _[Q]_ [(] _[ℓ,h]_ [)] _[x]_ _i_ [(] _[ℓ][−]_ [1)] ) _[⊤]_ _φ_ ( _K_ [(] _[ℓ,h]_ [)] _x_ [(] _j_ _[ℓ][−]_ [1)] )


where _φ_ : R _[d]_ _→_ R _[d]_ is an arbitrary function. This formulation of linear attention leads to a linear runtime complexity.
The key is to suppose that we maintain cumulative states


_Si_ [(] _[ℓ,h]_ [)] = _Si_ [(] _−_ _[ℓ,h]_ 1 [)] [+] _[ V]_ [ (] _[ℓ,h]_ [)] _[x]_ _i_ [(] _[ℓ][−]_ [1)] _⊗_ _φ_ ( _K_ [(] _[ℓ,h]_ [)] _x_ [(] _i_ _[ℓ][−]_ [1)] ) _,_

_Zi_ [(] _[ℓ,h]_ [)] = _Zi_ [(] _−_ _[ℓ,h]_ 1 [)] [+] _[ φ]_ [(] _[K]_ [(] _[ℓ,h]_ [)] _[x]_ _i_ [(] _[ℓ][−]_ [1)] )


with _S_ 0 [(] _[ℓ,h]_ [)] = 0 and _Z_ 0 [(] _[ℓ,h]_ [)] = 0, we have

_yi_ [(] _[ℓ,h]_ [)] = _[φ]_ [(] _[Q]_ [(] _[ℓ,h]_ [)] _[x]_ _i_ [(] _[ℓ][−]_ [1)] ) _[⊤]_ _Si_ [(] _[ℓ,h]_ [)] _._
_φ_ ( _Q_ [(] _[ℓ,h]_ [)] _x_ [(] _i_ _[ℓ][−]_ [1)] ) _[⊤]_ _Zi_ [(] _[ℓ,h]_ [)]


Indeed, linear attention can be viewed as an RNN.


**Definition 2.1** (Recurrent neural network (RNN)) **.** An RNN
layer takes as input the sequence _x_ = ( _x_ 1 _, · · ·, xn_ ) _∈_
(R _[d]_ ) _[n]_ and produces an output sequence _y_ = ( _y_ 1 _, . . ., yn_ ) _∈_
(R _[d]_ ) _[n]_ computed as follows. One chooses h0 _∈_ R _[m]_,
then computes inductively h _i_ = _g_ ( _i_ )( _xi,_ h _i−_ 1) and _yi_ =
_f_ ( _i_ )( _xi,_ h _i_ ) for _i_ = 1 _, . . ., n_, and _g_ ( _i_ ) : R _[d]_ [+] _[m]_ _→_ R _[d]_ and
_f_ ( _i_ ) : R _[d]_ [+] _[m]_ _→_ R _[d]_ are arbitrary functions.


These h _i_ are called hidden states of the RNN layer. Two
key factors characterize the capacity of an RNN: the hidden
dimension _m_, as defined in the Definition 2.1, and the precision _p_, meaning that _hi_ are represented by _p_ -bit numbers.


**Lemma 2.2** (Linear attention as RNN) **.** _A one-layer and_
_one-head linear attention of dimension d and precision p_
_can be viewed as an RNN layer of hidden dimension d_ [2] + _d_
_and precision p. More generally, linear attention of head_
_number H, layer number L, dimension d, and precision p_
_can be viewed as a multi-head and multi-layer RNN of layer_
_number L, hidden dimension H_ ( _d_ [2] + _d_ ) _, and precision p._



and the final output is computed as a weighted combination


_yi_ = _λyi_ _[compress]_ + (1 _−_ _λ_ ) _yi_ _[select]_ _._


**2.4. Tasks**


We formally define the tasks analyzed in this paper: sequential function composition (Chen et al., 2025) and 2-Sum.


**Definition 2.5** ( _L_ -sequential function composition) **.** Given
an integer _L ≥_ 2, an _L_ -sequential function composition
task, denoted _L_ -FuncComp( _w, z_ 0 _, z_ 1 _, . . ., zL_ ) is defined by
positive integers _m, n_ 1 _, · · ·, nL−_ 1, a sequence of functions
_z_ 0 _, z_ 1 _. . ., zL_ and a query _w_ = ( _w_ 1 _, . . ., wL−_ 1) _∈_ [ _n_ 1] _×_

_· · · ×_ [ _nL−_ 1]. Here, _z_ 0 _∈_ [ _m_ ] is the initial input, and _zℓ_ _∈_
_Aℓ_ := _{_ [ _Nℓ−_ 1] _→_ [ _Nℓ−_ 1] _} ≃_ [ _Nℓ_ _[N]_ _−_ _[ℓ]_ 1 _[−]_ [1] []][ for] _[ ℓ]_ _[∈]_ [[] _[L]_ []][ with]


_Nℓ_ = _m ·_     - _nℓ_ _∀ℓ_ _∈_ [0 : _L −_ 1] _._ (2)


_ℓ_ _[′]_ _∈_ [ _ℓ_ ]


Compute _i_ 0 = _z_ 0 _∈_ [ _m_ ] _, i_ 1 = _z_ 1( _i_ 0) _∈_ [ _N_ 0] and inductively for _ℓ_ = 1 _,_ 2 _, . . ., L −_ 1: _iℓ_ +1 = _zℓ_ +1( _wℓ, iℓ_ ) _∈_ [ _Nℓ_ ].
The final output is _L_ -FuncComp( _w, z_ 0 _, z_ 1 _, . . ., zL_ ) = _iL_ .



The hybrid attention architecture is defined as follows.


**Definition 2.3** (Hybrid Transformer architecture) **.** An
( _L, a_ 1 _, · · ·, aL_ )-hybrid Transformer is an ( _L_ + _a_ 1 + _· · ·_ +
_aL_ )-layer Transformer consisting of _L_ full attention layers, each followed by _a_ 1 _, · · ·, aL_ layers of linear attention,
respectively. i.e.,


( _Tlinear_ [(] _[L,a][L]_ [)] _[◦· · · ◦]_ _[T]_ _linear_ [ (] _[L,]_ [1)] _[◦]_ _[T]_ [ (] _softmax_ _[L]_ [)] [)]

_◦· · · ◦_ ( _Tlinear_ [(1] _[,a]_ [1][)] _[◦· · · ◦]_ _[T]_ _linear_ [ (1] _[,]_ [1)] _[◦]_ _[T]_ [ (1)] _softmax_ [)] _[.]_


**2.3. Sparse attention**


We consider the sparse attention mechanism with block
compression and selection strategies.


**Definition 2.4.** A single-layer one-head ( _B, k_ )-sparse attention is defined as follows: the input tokens are divided into blocks of _B_ tokens, and then applies a compression map _f_ : (R _[d]_ ) _[B]_ _→_ R _[d]_ to get compressed tokens
_x_ [1: _B_ ] _, · · ·, x_ [ _tB_ +1 _,_ ( _t_ +1) _B_ ] _∈_ R _[d]_, where _t_ = _⌊_ _B_ _[i]_ _[⌋]_ [.]

A block selection function _g_ : R _[d]_ _×_ R _[d]_ _→_ R is then applied to assign a score to each block relative to the current
token. The _k_ blocks with the highest scores are selected.
Let _s_ 1 _, · · ·, sk_ be the indices of these selected blocks, corresponding to token ranges [ _s_ 1 _B_ : ( _s_ 1 + 1) _B_ ] _, · · ·,_ [ _skB_ :
( _sk_ + 1) _B_ ]. We then calculate the compressed output and
the selected output


_yi_ _[compress]_ =        - _αi,jV x_ [ _jB_ +1:( _j_ +1) _B_ ]

( _j_ +1) _B≤i_



_yi_ _[select]_ = 
_k_ _[′]_ _∈_ [ _k_ ]




 - _αi,jV x_ [ _jB_ +1:( _j_ +1) _B_ ]

_j∈_ [ _sk′_ _B_ +1:( _sk′_ +1) _B_ ]



4


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**



To solve the _L_ -sequential function composition task, we
assume the Transformer receives the input prompt in the
following format: first, the _L_ functions _zL, zL−_ 1 _, . . ., z_ 0 are
listed, followed by the query _w_ . For simplicity, we assume
each entry of a function _zℓ_ (for _ℓ_ _∈_ [0 : _L −_ 1]) is encoded
by a single token (thus requiring _Nℓ−_ 1 tokens for _zℓ_ ), and
the query _w_ is also encoded in a single token.


**Definition 2.6** (The 2-Sum task) **.** Given input sequence
( _xi_ ) _i∈_ [ _n_ +1] _∈_ [ _M_ ] _[n]_ [+1], with _M_ = _O_ ( _n_ ), the goal is to
output the sequence ( _yi_ ) _i∈_ [ _n_ ] with







_yi_ =




1 _,_ _∃j < i, xi_ + _xj ≡_ 0 mod _M_
_._
0 _,_ otherwise



**3. Hybrid communication model**


To prove Theorem 1.1, we introduce an ( _L, a_ 1 _, · · ·, aL_ )hybrid communication model for _L_ -sequential function composition, which extends the framework of (Chen et al., 2025).
The model comprises _L_ + 2 players, each holding one of
the following: the _L_ functions, the initial input, or the query.
Communication is organized into _L_ epochs. Within each
epoch, a single round simulates a full attention layer, followed by several rounds that implement the subsequent
linear attention layers.









We remark that the players are forgetful, the player _j_ does
not remember anything sent from previous players.


**4. Hybrid Communication Lower Bound**


We demonstrate a limitation of hybrid Transformer architectures combining full attention with efficient linear attention
layers on solving deep sequential composition tasks. Our
main theorem shows that even abundant linear attention cannot substitute for the expressive power of full attention on
certain hierarchical tasks.


We prove Theorem 1.1 via a communication complexity
argument. The core of our proof is an inductive construction
of indistinguishable input sets that become impossible for
the model to distinguish, despite requiring different outputs.


We analyze the hybrid Transformer’s computation through
the lens of information flow between layers. Each full attention layer enables parallel aggregation of information from
all previous positions, while linear attention layers only
permit sequential, recurrent propagation. The latter cannot
create the rich interactions needed for deep composition.


The _L_ -sequential function composition task is specifically



5


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**



designed to control the contribution of the linear attention
layers to the transcript space and to ensure technical requirements. Concrete choices of parameters of the task are given
in Appendix B.


The proof technique provides a general framework for analyzing hierarchical computation in structured transformers. It demonstrates that linear attention—despite its efficiency—lacks the expressive power required for deep compositional reasoning, even when augmented with limited
full attention.


The rest of this section provides a sketch of our proof; the
detailed proof is given in Appendix B.


**4.1. Parameters and strateggy**


To prove Theorem 1.1, we construct parameters of the task
such that the input length satisfies _n ≤_ ( _Hdp_ ) [4] _[·]_ [16] _[L]_, yet the
task cannot be solved by an ( _L −_ 1 _,_ 2 [3] _[L]_ [2] _, · · ·,_ 2 [3] _[L]_ [2] )-hybrid
Transformer. We assume that _Hdp ≥_ 2.


Indeed, we prove a stronger result: even if we allow pretreatment by a single-layer Transformer, the _L_ -sequential
function composition task cannot be solved by a small
( _L −_ 1 _,_ 2 [3] _[L]_ [2] _, · · ·,_ 2 [3] _[L]_ [2] )-hybrid Transformer. Equivalently,
we prove that a small ( _L,_ 0 _,_ 2 [3] _[L]_ [2] _, · · ·,_ 2 [3] _[L]_ [2] )-hybrid Transformer cannot solve _L_ -sequential function composition.


**Notation.** For notational convenience, we use _z−_ 1 and _w_ interchangeably to denote player _−_ 1’s input. In the following,
we elaborate on several key definitions that will be crucial
to our proof.


  - (Soft transcript Π [(] _j,i_ _[ℓ]_ [)][) For any] _[ i][ ∈]_ [[] _[−]_ [1 :] _[ L][ −]_ [1]][,]

_j ∈_ [ _i_ + 1 : _L_ ], _ℓ_ _∈_ [ _L_ ], recall that Π [(] _j,i_ _[ℓ]_ [)] [de-]
notes the soft transcript sent from player _j_ to player
_i_ at the _ℓ_ -th epoch of communication. Its value is
determined by the inputs of players [ _i_ : _L_ ] (i.e.,
_zL, . . ., zi_ ) and is independent of the the inputs of players [ _−_ 1 : _i −_ 1] (i.e., _zi−_ 1 _, . . ., z−_ 1). For any fixed
inputs � _zL ∈_ [ _NL−_ 1] _[N][L][−]_ [1] _, . . .,_   - _zi ∈_ [ _Ni−_ 1] _[N][i][−]_ [1], let
Π [(] _j,i_ _[ℓ]_ [)][(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z][i]_ [)][ denote the soft transcript when player]
_t_ receives input _zt_ = � _zt_ ( _t ∈_ [ _i_ : _L_ ]).

  - (Linear transcript Σ [(] _i_ +1 _[ℓ]_ [)] _[,m]_ [) For any] _[ i][ ∈]_ [[] _[−]_ [1] _[, L][ −]_ [1]][,] _[ l][ ∈]_

[ _L_ ], and _m ∈_ [0 _, aℓ_ _−_ 1], recall Σ [(] _i_ +1 _[ℓ]_ [)] _[,m]_ is the linear transcript sent from player _i_ +1 to player _i_ in the ( _m_ +1)-th
linear round of the _ℓ_ -th epoch of communication. Its
value is determined by the inputs of players [ _i_ + 1 : _L_ ]
(i.e., _zL, . . ., zi_ +1) and is independent of the the inputs
of players [ _−_ 1 : _i_ ] (i.e., _zi, . . ., z−_ 1). For any fixed
inputs � _zL ∈_ [ _NL−_ 1] _[N][L][−]_ [1] _, . . .,_   - _zi ∈_ [ _Ni−_ 1] _[N][i][−]_ [1], let
Σ [(] _i_ +1 _[ℓ]_ [)] _[,m]_ [(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z][i]_ [)][ denote the transcript when player] _[ t]_
receives input _zt_ = � _zt_ ( _t ∈_ [ _i_ : _L_ ]).




  - (The partial composition value) For any _ℓ_ _∈_ [0 : _L_ ],
the value of _iℓ_ is determined by _w, z_ 0 _, . . ., zℓ_ . We
write _iℓ_ ( _w,_   -   - _z_ 0 _, . . .,_   - _zℓ_ ) to denote its value when _w_ =
_w, z_   - 0 = � _z_ 0 _, . . ., zℓ_ = � _zℓ_ .


**Indistinguishable decomposition.** Our key concept for
the proof is _indistinguishable decomposition_ introduced in
(Chen et al., 2025). A indistinguishable decomposition
is formed by two sets _R≥ℓ_ and _Z<ℓ_, where _R≥ℓ_ is a set
of input assignments to players [ _ℓ_ : _L_ ] and _Z<ℓ_ is a set
of input assignments to players [ _−_ 1 : _ℓ_ _−_ 1]). The key
property is that for any fixed input _z<ℓ_ _∈_ _Z<ℓ_ for the first
_ℓ_ players, all assignments to _R≥ℓ_ are indistinguishable to
players [ _−_ 1 : _ℓ_ _−_ 1] on inputs _z<ℓ_ after _ℓ_ epochs, because
they produce identical communication transcripts. Formally,
we adapt the definition in (Chen et al., 2025) as follows.
**Definition 4.1** (Indistinguishable decomposition) **.** Let _ℓ_ _∈_

[2 : _L_ ], an indistinguishable decomposition is a pair of sets
_R≥ℓ_ _⊆_ _AL_ _×AL−_ 1 _×· · ·×Aℓ_ and _Z<ℓ_ = _Z−_ 1 _×· · ·×Zℓ−_ 1
with _Z−_ 1 = _A−_ 1 _, Z_ 0 _⊆_ _A_ 0 _, · · ·, Zℓ−_ 1 _⊆_ _Aℓ−_ 1, such that
for every � _z<ℓ_ _∈_ _Z<ℓ_, and for every � _α≥ℓ,_ _β_ [�] _≥ℓ_ _∈_ _R≥ℓ_, it
satisfies:

Π [(] _j,i_ _[ℓ][′]_ [)][(] _[z]_ [�] _[<ℓ][,]_ [ �] _[α][≥][ℓ]_ [) = Π] _j,i_ [(] _[ℓ][′]_ [)][(] _[z]_ [�] _[<ℓ][,]_ [ �] _[β][≥][ℓ]_ [)]

for every _j ∈_ [ _ℓ_ : _L_ ], _i ∈_ [ _−_ 1 : _ℓ_ _−_ 1], and _ℓ_ _[′]_ _∈_ [ _ℓ_ ], and

Σ [(] _i_ +1 _[ℓ][′]_ [)] _[,m]_ ( _z_   - _<ℓ,_   - _α≥ℓ_ ) = Σ [(] _i_ +1 _[ℓ][′]_ [)] _[,m]_ ( _z_   - _<ℓ,_ _β_ [�] _≥ℓ_ )

for every _i ∈_ [ _−_ 1 : _ℓ_ _−_ 1], _ℓ_ _[′]_ _∈_ [ _ℓ_ ], and _m ∈_ [0 _, aℓ′ −_ 1].


The utility of an indistinguishable decomposition becomes
clear when _ℓ_ = _L_ . In this case, for every input assignment
from _Z<L_ to players [ _−_ 1 : _L −_ 1], player _−_ 1 (the final
output player) observes identical communication transcripts
after _L_ epochs (i.e., at the end of the protocol) regardless
of which input _⃗zL ∈_ _R≥L_ is assigned to player _L_ . Consequently, for every � _z<L ∈_ _Z<L_, the output of the protocol _L_ -FuncComp( _z_ - _<L,_ - _zL_ ) must be the same for every
_z_ - _L ∈_ _R≥L_ . We will carefully define the set _R≥ℓ_ and _Z<ℓ_
so that satisfying this requirement leads to a contradiction,
thereby establishing the desired lower bound.


For a subset _Z<ℓ_, we define _Iℓ−_ 1( _Z<ℓ_ ) to be the set of all
possible values for the intermediate composition after the
( _ℓ_ _−_ 1)-th epoch when the inputs to players [ _−_ 1 : _ℓ_ _−_ 1] are
restricted to _Z<ℓ_ :


_{iℓ−_ 1( _z_  - _−_ 1 _,_  - _z_ 0 _, . . .,_  - _zℓ−_ 1) : � _z−_ 1 _,_  - _z_ 0 _, . . .,_  - _zℓ−_ 1) _∈_ _Z<ℓ}._


The following lemma, as in (Chen et al., 2025), shows that
the desired lower bound follows from a good enough indistinguishable configuration for _ℓ_ = _L_ .

**Lemma 4.2** (Lemma B.5) **.** _An L-epoch hybrid communi-_
_cation protocol does not solve L-_ FuncComp _if there is an_
_indistinguishable decomposition R≥L and Z<L, such that_
_both |R≥L| and |IL−_ 1( _Z<L_ ) _| are large._



6


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**



Since all _zL ∈_ _R≥L_ are indistinguishable to player _−_ 1 (the
output player), the protocol must output the same answer
for every _zL ∈_ _R≥L_ . However, because _IL−_ 1( _Z<L_ ) is
large, distinct _zL ∈_ _R≥L_ would require different outputs
for some input in _Z<L_ . This contradicts the correctness of
the protocol, thereby completing the proof.


The proof of Theorem 1.1 is then completed by constructing
the required decomposition via an inductive argument.


**Lemma 4.3** (Lemma B.7) **.** _For any ℓ_ _∈_ [2 : _L_ ] _, we can_
_construct by induction an indistinguishable decomposition_
( _R≥ℓ, Z<ℓ_ ) _, where R≥ℓ_ _⊆_ _AL × AL−_ 1 _× · · · × Aℓ_ _and_
_Z<ℓ_ = _Z−_ 1 _× Z_ 0 _× · · · × Zℓ−_ 1 _, with Z−_ 1 = [ _n_ 1 _· · · nL−_ 1] _,_
_Z_ 0 _⊆_ _A_ 0 _and Zi ⊆_ _Ai, together with the soft transcript_
_from players_ [ _ℓ_ : _L_ ] _to_ [ _−_ 1 : _ℓ_ _−_ 1] _for the first ℓ_ _epochs,_
_when the players_ [ _−_ 1 : _ℓ_ _−_ 1] _take input from Z<ℓ_ _and the_
_linear transcript from player ℓ_ _to ℓ_ _−_ 1 _for the first ℓ_ _epochs,_
_when the players_ [ _−_ 1 : _ℓ_ _−_ 1] _take input from Z<ℓ, such_
_that the soft transcripts and linear transcripts are consistent,_
_and that |R≥ℓ| and |Iℓ−_ 1( _Z<ℓ_ ) _| are large._


**4.2. The Initial step**


We first consider the base case _ℓ_ = 2.


**Step 1: Choosing** _Z_ 0 _, Z_ 1 **.** We set _Z_ 0 = [ _x_ 0]. The next step
is to select the set _Z_ 1 _⊆_ _A_ 1. Consider all possible first epoch
messages from the player 1 to the player _−_ 1, The total number of of distinct such message patterns is 2 [2] _[Hdp][·|][Z][−]_ [1] _[|]_ =
2 [2] _[Hdp]_ [(] _[n]_ [1] _[···][n][L][−]_ [1][)] . We select by the pigeonhole principle
a message pattern Ψ [�] [(1)] 1 _,−_ 1 _[∈{]_ [0] _[,]_ [ 1] _[}]_ [2] _[Hdp][·]_ [(] _[n]_ [1] _[···][n][L][−]_ [1][)][ such]
that the consistency set _S_ of input � _z_ 1 _∈_ _A_ 1 satisfies
_|S| ≥|A_ 1 _| ·_ 2 _[−]_ [2] _[Hdp][·]_ [(] _[n]_ [1] _[···][n][L][−]_ [1][)] . We then retract a suitable
subset _Z_ 1 _⊆_ _S_ by the following lemma.

**Lemma 4.4** (Lemma B.8) **.** _There exists a subset Z_ 1 _⊆_ _S_
_such that I_ 1( _Z<_ 2) _is large._


The next step is to fix the transcripts from players _j ∈_ [2 :
_L −_ 1] to players _i_ = _−_ 1 _,_ 0 _,_ 1 at the first two epochs.


**Step 2.1: Fixing the transcripts to player** _−_ 1 **.** We
begin with the first epoch. Our goal is to fix soft transcripts sent to player _−_ 1 in the first epoch for every � _z_ 1 _∈_
_Z_ 1 _,_ - _z_ 0 _∈_ _Z_ 0 _,_ - _z−_ 1 _∈_ _Z−_ 1. The first epoch message from
player _j_ to player _−_ 1 depends only on � _z−_ 1 and player
_j_ ’s input, and is independent of � _z_ 1 _,_ - _z_ 0. Therefore, the total number of possible such transcript patterns is at most
2 [2] _[Hdp][·|][Z][−]_ [1] _[|]_ = 2 [2] _[Hdp][·]_ [(] _[n]_ [1] _[···][n][L][−]_ [1][)] . By the pigeonhole principle, we can choose a fixed pattern with the consistency set of
maximal size _|C_ 1 _| ≥|AL| · · · |A_ 2 _| ·_ 2 _[−]_ [2] _[Hdp][·]_ [(] _[n]_ [1] _[···][n][L][−]_ [1][)] _[·][L]_ _._


For the second epoch, the goal is to fix soft transcripts sent
to player _−_ 1 in the second epoch for every � _z_ 1 _∈_ _Z_ 1 _,_ - _z_ 0 _∈_
_Z_ 0 _,_ - _z−_ 1 _∈_ _Z−_ 1. Crucially, the transcript from player _j_
depends only on the information state _X−_ [(1)] 1 [and] _[ X]_ _j_ [(1)], which
are themselves independent of the choice of _z_ 1 _∈_ _Z_ 1. This



independence holds because the only component of _X−_ [(1)] 1
and _Xj_ [(1)] that depends on _z_ 1 is the first epoch message from
player 1 to _−_ 1, which is fixed. We choose transcripts with
the consistency set _C_ 2 of maximal size.


**Step 2.2: Fixing the transcripts to player** 0 **and player** 1 **.**
Similarly, We fix the value of soft transcripts sent to player
0 in the first two epochs so that the consistency set _C_ 3 has
maximal size.


We follow the same strategy to fix the value of soft transcripts and the linear transcripts sent to player 1 in the first
two epochs so that the consistency set _C_ 4 attains its maximal size. Take _R≥_ 2 to be _C_ 4, this concludes the proof for
the base case _ℓ_ = 2.


**4.3. Inductive step**


Suppose we are done up to _ℓ_ _∈_ [2 : _L −_ 1]. We continue our
construction for _ℓ_ + 1.


The key insight is that _Zℓ_ is indistinguishable to players

[ _−_ 1 : _ℓ_ _−_ 1] after _ℓ_ epochs, when these players take input
from _Z<ℓ_ . Hence, the ( _ℓ_ +1)-th epoch transcripts to players

[ _−_ 1 : _ℓ_ _−_ 1] are independent of the choice of _zℓ_ _∈_ _Zℓ_ .


**Step 1: Choosing the set** _Zℓ_ **.** Recall that the size of
_R≥ℓ_ is large. We would like to select a rectangular subset
from _R≥ℓ_ . As in (Chen et al., 2025), we have the following
lemma.


**Lemma 4.5** (Lemma B.9) **.** _There exists a subset S_ [(] _[ℓ]_ [)] =
_S_ 1 [(] _[ℓ]_ [)] _× S_ 2 [(] _[ℓ]_ [)] _[, where][ S]_ 1 [(] _[ℓ]_ [)] _⊆_ _AL × · · · × Aℓ_ +1 _, S_ 2 [(] _[ℓ]_ [)] _⊆_ _Aℓ,_
_such that S_ 1 [(] _[ℓ]_ [)] _and Iℓ−_ 1( _S_ 2 [(] _[ℓ]_ [)][)] _[ are large.]_


We take _Zℓ_ = _S_ 2 [(] _[ℓ]_ [)] and _S≥ℓ_ +1 = _S_ 1 [(] _[ℓ]_ [)][. Next, we are going]
to fix the transcripts. Recall that we need to fix all transcripts
from players [ _ℓ_ + 1 : _L_ ] to players [ _−_ 1 : _ℓ_ ] in the first
_ℓ_ + 1 epochs, when players [ _−_ 1 : _ℓ_ ] receive input from
_Z≤ℓ_ = _Z−_ 1 _× Z_ 0 _× · · · × Zℓ_ . We proceed in a few steps.


**Step 2.1: Fixing the transcripts to players** [ _−_ 1 : _ℓ_ _−_ 1]
**in the first** _ℓ_ **epochs.** We simply the transcripts given
by the induction hypothesis. Lemma B.10 guaranties the
consistency.


**Step 2.2: Fixing the transcripts to players** [ _−_ 1 : _ℓ_ _−_ 1]
**at the** ( _ℓ_ + 1) **-th epoch.** Our key insight is that _Zℓ_ is
indistinguishable to players [ _−_ 1 : _ℓ_ _−_ 1] when they take
input from _Z≤ℓ−_ 1. Hence, their transcripts are independent
of the choice of _zℓ_ _∈_ _Zℓ_ . We consider the soft transcripts
from players [ _ℓ_ +1 _, L_ ] to players [ _−_ 1 : _ℓ−_ 1] at the ( _ℓ_ +1)-th
epoch




    -    Φ [(] _[ℓ]_ [+1)] = Φ [(] _j,i_ _[ℓ]_ [+1)]



_j∈_ [ _ℓ_ +1: _L_ ] _,i∈_ [ _−_ 1: _ℓ−_ 1]



7


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**



where


     -      Φ [(] _j,i_ _[ℓ]_ [+1)] = Φ [(] _j,i_ _[ℓ]_ [+1)] ( _z_  - _ℓ−_ 1 _, . . .,_  - _zi_ ) _z_  - _ℓ−_ 1 _∈Zℓ...,z_  - _i∈Zi_

and Φ [(] _j,i_ _[ℓ]_ [+1)] ( _z_    - _ℓ−_ 1 _, . . .,_    - _zi_ ) _∈_ domain(Π [(] _j,i_ _[ℓ]_ [+1)] )


Comparing Λ [(] _j,i_ _[ℓ]_ [+1] _[,ℓ]_ [+1)] with Φ [(] _j,i_ _[ℓ]_ [+1)], we note that Φ [(] _j,i_ _[ℓ]_ [+1)] is
independent of � _zℓ_ _∈_ _Zℓ_ . For any Φ [(] _[ℓ]_ [+1)], define _S_ (Φ [(] _[ℓ]_ [+1)] )
to be the set of all ( _z_ - _L, . . .,_ - _zℓ_ +1) _∈_ _S≥ℓ_ +1 that are consistent with the transcripts Φ [(] _[ℓ]_ [+1)] .


**Lemma 4.6** (Lemma B.11) **.** _We have_

   - _S_ (Φ [(] _[ℓ]_ [+1)] ) = _S≥ℓ_ +1 _._


Φ [(] _[ℓ]_ [+1)]


Now as in (Chen et al., 2025), we take the transcripts with
maximal consistency set _T≥ℓ_ +1 = _S_ (Φ [�] [(] _[ℓ]_ [+1)] ) _⊆_ _S≥ℓ_ +1.


**Step 2.3: Fixing the transcript to player** _ℓ_ **.** This follows a
greedy selection strategy. First, consider the soft transcripts
sent to player _ℓ_ in the first _ℓ_ + 1 epochs




  -   Ψ = Ψ [(] _j,ℓ_ _[ℓ][′]_ [)][(] _[z]_ [�] _[ℓ]_ [)]

_j∈_ [ _ℓ_ +1: _L_ ] _,ℓ_ _[′]_ _∈_ [ _ℓ_ +1] _,z_        - _ℓ∈Zℓ_



where Ψ [(] _j,ℓ_ _[ℓ][′]_ [)][(] _[z]_ [�] _[ℓ]_ [)] _[ ∈]_ [domain][(Π][(] _j,ℓ_ _[ℓ][′]_ [)][)]


Define _T_ (Ψ) to be its consistency set. We can upper-bound
the number of distinct Ψ and use the pigeonhole principle
to lower-bound the size of the maximal consistency set
(Lemma B.13).


Next, we fix linear transcripts to player _ℓ_ in the first _ℓ_ +
1 epochs to maintain the maximal consistency set. We
can then take _R≥ℓ_ +1 to be this consistency set, and this
completes the induction step.


**5. Lower bound for sparse attention**


We now prove Theorem 1.2. For convenience, we assume
that _B_ divides _n_ . The proof proceeds by relating a ( _B, k_ )sparse attention to the following communication model.











In this model, each block of _B_ tokens corresponds to one
of the first _B_ _[n]_ [players, the last token corresponds to the final]



player, and the compressed representation (of size _Hdp_ bits)
corresponds to the message sent to the final player.


One can prove that, for the last player to output the correct
answer, each of the first players must communicate the set
of its _B_ tokens via a compressed message of _Hdp_ bits. This
implies the desired result; details are given in Appendix A.6.


**6. Conclusion**


In this work, we establish a unified hierarchy of expressive
power for efficient attention mechanisms within a communication complexity framework. By analyzing hybrid architectures, we prove an unconditional lower bound: even an
abundance of linear attention layers cannot substitute for
the compositional power of a single full attention layer in
solving deep sequential function composition tasks. These
tasks formally model the requirement for multi-step sequential computation within a single forward pass, capturing
problems like multi-hop retrieval where each step’s output
defines the input context for the next. Our theoretical framework introduces carefully constrained “retrieval scopes” at
each internal computational step to enable rigorous analysis.
This result provides a theoretical justification for the empirical observation that simply interleaving linear and full
attention yields limited gains on deep reasoning problems.


Furthermore, we identify a fundamental limitation of singlelayer sparse attention mechanisms based on block compression and selection. For tasks like 2-Sum that require uniform
pair-wise comparisons, any such sparse mechanism is provably weaker than full attention unless its effective capacity
scales with the block size.


Collectively, our results offer a principled theoretical framework for understanding efficient attention variants. They
suggest that future architectures for long-context processing
must either embrace the expressive power of full attention
through smarter approximations, or explicitly design around
these limitations—for instance, by employing task-sufficient
sparsity patterns or incorporating mechanisms to bypass the
communication bottlenecks we have identified. The communication models and proof techniques introduced here
may serve as a foundation for further theoretical analysis of
structured Transformers.


**References**


Amiri, A., Huang, X., Rofin, M., and Hahn, M. Lower
bounds for chain-of-thought reasoning in hard-attention
transformers. In _Forty-second International Conference_
_on Machine Learning_, 2025.


Barcelo, P., Kozachinskiy, A., and Steifer, T. Ehrenfeuchthaussler rank and chain of thought. In _Forty-second_
_International Conference on Machine Learning_ . arxiv:



8


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**



2501.12997, 2025.


Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The
long-document transformer. _arXiv:2004.05150_, 2020.


Bhattamishra, S., Ahuja, K., and Goyal, N. On the practical ability of recurrent neural networks to recognize
hierarchical languages. In _Proceedings of the 28th Inter-_
_national Conference on Computational Linguistics_, pp.
1481–1494. International Committee on Computational
Linguistics, December 2020.


Bhattamishra, S., Hahn, M., Blunsom, P., and Kanade, V.
Separations in the representational capabilities of transformers and recurrent architectures. In _The Thirty-eighth_
_Annual Conference on Neural Information Processing_
_Systems_, 2024.


Chen, L., Peng, B., and Wu, H. Theoretical limitations
of multi-layer transformer. In _the 66th IEEE Annual_
_Symposium on Foundations of Computer Science (FOCS)_,
2025.


DeepSeek-AI. Deepseek-v3 technical report. _ArXiv_,
2412.19437, 2024.


DeepSeek-AI. Deepseek-v3.2-exp: Boosting long-context
efficiency with deepseek sparse attention, 2025.


Elman, J. L. Finding structure in time. _Cognitive Science_,
14(2):179–211, 1990.


Feng, G., Zhang, B., Gu, Y., Ye, H., He, D., and Wang, L.
Towards revealing the mystery behind chain of thought:
A theoretical perspective. In _Thirty-seventh Conference_
_on Neural Information Processing Systems_, 2023.


Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J.
Model tells you what to discard: Adaptive KV cache compression for LLMs. In _The Twelfth International Confer-_
_ence on Learning Representations_ [, 2024. URL https:](https://openreview.net/forum?id=uNrFpDPMyo)
[//openreview.net/forum?id=uNrFpDPMyo.](https://openreview.net/forum?id=uNrFpDPMyo)


Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. In _First Conference on_
_Language Modeling_, 2024.


Guo, H., Yang, S., Goel, T., Xing, E. P., Tri, D., and Kim, Y.
Log-linear attention. _ArXiv_, 2506.04761, 2025.


Guo, M., Ainslie, J., Uthus, D., Ontanon, S., Ni, J., Sung,
Y.-H., and Yang, Y. LongT5: Efficient text-to-text transformer for long sequences. In _Findings of the Association_
_for Computational Linguistics: NAACL 2022_, pp. 724–
736, Seattle, United States, July 2022. Association for
Computational Linguistics.



Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., and Zhang,
Z. Star-transformer. In _Proceedings of the 2019 Confer-_
_ence of the North American Chapter of the Association_
_for Computational Linguistics: Human Language Tech-_
_nologies, Volume 1 (Long and Short Papers)_, pp. 1315–
1325, Minneapolis, Minnesota, June 2019. Association
for Computational Linguistics.


Hahn, M. Theoretical limitations of self-attention in neural
sequence models. _Transactions of the Association for_
_Computational Linguistics_, 8:156–171, 2020.


Hewitt, J., Hahn, M., Ganguli, S., Liang, P., and Manning,
C. D. RNNs can generate bounded hierarchical languages
with optimal memory. In _Proceedings of the 2020 Con-_
_ference on Empirical Methods in Natural Language Pro-_
_cessing (EMNLP)_, pp. 1978–2010, November 2020.


Hsieh, Y.-L., Cheng, M., Juan, D.-C., Wei, W., Hsu, W.-L.,
and Hsieh, C.-J. On the robustness of self-attentive models. In _Proceedings of the 57th Annual Meeting of the As-_
_sociation for Computational Linguistics_, pp. 1520–1529,
Florence, Italy, July 2019. Association for Computational
Linguistics.


Jelassi, S., Brandfonbrener, D., Kakade, S. M., and Malach,
E. Repeat after me: Transformers are better than state
space models at copying. In _Forty-first International_
_Conference on Machine Learning_, 2024.


Jiang, H., Wu, Q., Lin, C.-Y., Yang, Y., and Qiu, L. LLMLingua: Compressing prompts for accelerated inference of
large language models. In _Proceedings of the 2023 Con-_
_ference on Empirical Methods in Natural Language Pro-_
_cessing_, pp. 13358–13376, Singapore, December 2023.
Association for Computational Linguistics.


Kasai, J., Peng, H., Zhang, Y., Yogatama, D., Ilharco, G.,
Pappas, N., Mao, Y., Chen, W., and Smith, N. A. Finetuning pretrained transformers into RNNs. In _Proceedings_
_of the 2021 Conference on Empirical Methods in Natu-_
_ral Language Processing_, pp. 10630–10643, Online and
Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.


Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
Transformers are RNNs: Fast autoregressive transformers
with linear attention. In _Proceedings of the 37th Interna-_
_tional Conference on Machine Learning_, volume 119 of
_Proceedings of Machine Learning Research_, pp. 5156–
5165. PMLR, 13–18 Jul 2020.


Li, Q. and Wang, Y. Constant bit-size transformers are turing
complete. In _The Thirty-ninth Annual Conference on Neu-_
_ral Information Processing Systems_ [, 2025. URL https:](https://openreview.net/forum?id=RBWnyDEBKf)
[//openreview.net/forum?id=RBWnyDEBKf.](https://openreview.net/forum?id=RBWnyDEBKf)



9


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**



Li, Z., Liu, H., Zhou, D., and Ma, T. Chain of thought empowers transformers to solve inherently serial problems.
In _The Twelfth International Conference on Learning_
_Representations_, 2024.


Lu, E., Jiang, Z., Liu, J., Du, Y., Jiang, T., Hong, C., Liu, S.,
He, W., Yuan, E., Wang, Y., HUang, Z., Yuan, H., Xu, S.,
Xu, X., Lai, G., Chen, Y., Zheng, H., Yan, J., Su, J., Wu,
Y., Zhang, Y., Yang, Z., Zhou, X., Zhang, M., and Qiu, J.
Moba: Mixture of block attention for long-context llms.
_arXiv preprint arXiv:2502.13189_, 2025.


Merrill, W. and Sabharwal, A. The parallelism tradeoff:
Limitations of log-precision transformers. _Transactions_
_of the Association for Computational Linguistics_, 11:531–
545, 2023.


Merrill, W. and Sabharwal, A. The expressive power of
transformers with chain of thought. In _The twelfth Inter-_
_national Conference on Learning Representations_, 2024.


MiniMax. Minimax-m1: Scaling test-time compute efficiently with lightning attention. _ArXiv_, 2506.13585,
2025a.


MiniMax. Minimax-m2. _Github_,
https://github.com/MiniMax-AI/MiniMax-M2, 2025b.


OpenAI. Gpt-4 technical report. _ArXiv_, 2303.08774, 2023.


Peng, B., Narayanan, S., and Papadimitriou, C. On limitations of the transformer architecture. In _First Conference_
_on Language Modeling_, 2024.


Peng, B., Zhang, R., Goldstein, D., Alcaide, E., Du, X.,
Hou, H., Lin, J., Liu, J., Lu, J., Merrill, W., et al. Rwkv-7”
goose” with expressive dynamic state evolution. _arXiv_
_preprint arXiv:2503.14456_, 2025.


Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N.,
and Kong, L. Random feature attention. In _International_
_Conference on Learning Representations_, 2021.


Perez, J., Marinkovi´ c, J., and Barcel´ o, P. On the turing´
completeness of modern neural network architectures. In
_International Conference on Learning Representations_,
2019.


Perez, J., Barcel´ o, P. P., and Marinkovic, J. Attention is´
turing-complete. _Journal of Machine Learning Research_,
22(75):1–35, 2021.


Qiu, J., Ma, H., Levy, O., Yih, W.-t., Wang, S., and Tang, J.
Blockwise self-attention for long document understanding. In _Findings of the Association for Computational_
_Linguistics: EMNLP 2020_, pp. 2555–2565. Association
for Computational Linguistics, November 2020.



QwenTeam. Qwen3-next: Towards ultimate training & inference efficiency. Technical report, https://qwen3-next.com/

 - Technical Documentation, 2025.


Sanford, C., Hsu, D., and Telgarsky, M. Representational
strengths and limitations of transformers. In _Thirty-_
_seventh Conference on Neural Information Processing_
_Systems_, 2023.


Sanford, C., Hsu, D., and Telgarsky, M. Transformers, parallel computation, and logarithmic depth. In _International_
_Conference on Machine Learning_, 2024a.


Sanford, C., Hsu, D., and Telgarsky, M. One-layer transformers fail to solve the induction heads task. _Arxiv_,
2408.14332, 2024b.


Schlag, I., Irie, K., and Schmidhuber, J. Linear transformers
are secretly fast weight programmers. In _Proceedings_
_of the 38th International Conference on Machine Learn-_
_ing_, volume 139 of _Proceedings of Machine Learning_
_Research_, pp. 9355–9366. PMLR, 18–24 Jul 2021.


Stirling, J. _Methodus differentialis: sive tractatus de sum-_
_matione et interpolatione serierum infinitarum. Auctore_
_Jacobo Stirling, R.S.S._ Impensis Ric. Manby ad Insignia
Principis in vico vulgo dicto Ludgate-Hill, 1753.


Team, K. Kimi linear: An expressive, efficient attention
architecture. _Arxiv_, 2510.26692, 2025a.


Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen,
N., Chen, R., Chen, Y., Chen, Y., Chen, Y., et al.
Kimi k2: Open agentic intelligence. _arXiv preprint_
_arXiv:2507.20534_, 2025.


Team, T. H. Hunyuan-turbos: Advancing large language
models through mamba-transformer synergy and adaptive
chain-of-thought. _Arxiv_, 2505.15431, 2025b.


Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In _Advances in Neural Information_
_Processing Systems_, volume 30. Curran Associates, Inc.,
2017.


Wei, C., Chen, Y., and Ma, T. Statistically meaningful
approximation: a case study on approximating turing
machines with transformers. In _Advances in Neural Infor-_
_mation Processing Systems_, volume 35, pp. 12071–12083,
2022a.


Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter,
Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of
thought prompting elicits reasoning in large language
models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho,
K. (eds.), _Advances in Neural Information Processing_
_Systems_, 2022b.



10


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**



Wen, K., Li, Y., Liu, B., and Risteski, A. Transformers
are uninterpretable with myopic methods: a case study
with bounded dyck grammars. In _Proceedings of the_
_37th International Conference on Neural Information_
_Processing Systems_, NIPS ’23, Red Hook, NY, USA,
2023. Curran Associates Inc.


Wen, K., Dang, X., and Lyu, K. RNNs are not transformers
(yet): The key bottleneck on in-context retrieval. In
_The Thirteenth International Conference on Learning_
_Representations_, 2025.


Xie, Z., Wei, Y., Cao, H., Zhao, C., Deng, C., Li, J.,
Dai, D., Gao, H., Chang, J., Zhao, L., et al. mhc:
Manifold-constrained hyper-connections. _arXiv preprint_
_arXiv:2512.24880_, 2025.


Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B.,
Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical
report. _arXiv preprint arXiv:2505.09388_, 2025a.


Yang, S., Wang, B., Zhang, Y., Shen, Y., and Kim, Y. Parallelizing linear transformers with the delta rule over sequence length. In _The Thirty-eighth Annual Conference_
_on Neural Information Processing Systems_, 2024.


Yang, S., Kautz, J., and Hatamizadeh, A. Gated delta networks: Improving mamba2 with delta rule. In _The Thir-_
_teenth International Conference on Learning Representa-_
_tions_, 2025b.


Yao, S., Peng, B., Papadimitriou, C., and Narasimhan, K.
Self-attention networks can process bounded hierarchical
languages. In _Proceedings of the 59th Annual Meeting_
_of the Association for Computational Linguistics and the_
_11th International Joint Conference on Natural Language_
_Processing (Volume 1: Long Papers)_, pp. 3770–3785.
Association for Computational Linguistics, August 2021.


Yuan, J., Gao, H., Dai, D., Luo, J., Zhao, L., Zhang, Z.,
Xie, Z., Wei, Y., Wang, L., Xiao, Z., Wang, Y., Ruan,
C., Zhang, M., Liang, W., and Zeng, W. Native sparse
attention: Hardware-aligned and natively trainable sparse
attention. In _Proceedings of the 63rd Annual Meeting of_
_the Association for Computational Linguistics (Volume 1:_
_Long Papers)_, pp. 23078–23097, Vienna, Austria, 2025.
Association for Computational Linguistics.


Yun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S., and
Kumar, S. Are transformers universal approximators
of sequence-to-sequence functions? In _International_
_Conference on Learning Representations_, 2020.


Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q.,
Yang, L., et al. Big bird: Transformers for longer sequences. _Advances in Neural Information Processing_
_Systems_, 33, 2020.



Zhu, D., Huang, H., Huang, Z., Zeng, Y., Mao, Y., Wu,
B., Min, Q., and Zhou, X. Hyper-connections. In _The_
_Thirteenth International Conference on Learning Repre-_
_sentations_, 2025.



11


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**


**A. Lower bounds for linear attention, log-linear attention and sparse attention**


In this section, we present communication models for different attention mechanisms on various tasks. We abstract the
process of solving such tasks via linear attention and log-linear attention as certain communication protocols, and prove
lower bounds in Theorems A.3, A.6, A.5, A.8 and A.9 via communication complexity arguments.


**A.1. Definitions**


A.1.1. LOG-LINEAR ATTENTION


In the log-linear attention mechanism proposed in (Guo et al., 2025), the output of the _ℓ_ -th attention layer becomes



_yi_ [(] _[ℓ,h]_ [)] =



_R−_ 1

- _λ_ [(] _i_ _[r,ℓ,h]_ [)] _Si_ [(] _[r,ℓ,h]_ [)] _Qx_ [(] _i_ _[ℓ][−]_ [1] _[,h]_ [)] (3)

_r_ =0



where _λ_ [(] _i_ _[r,ℓ,h]_ [)] are weights that depend only on _x_ [(] _i_ _[ℓ][−]_ [1] _[,h]_ [)], _R_ = _⌈_ log2 _i_ + 1 _⌉_ + 1 and _Si_ [(] _[r,ℓ,h]_ [)] are hidden states. In (Guo et al.,
2025), these states are calculated via the recursion



_,_



_Si_ [(] _[r,ℓ,h]_ [)] =








_V x_ [(] _i_ _[ℓ][−]_ [1] _[,h]_ [)] ( _Kx_ [(] _i_ _[ℓ][−]_ [1] _[,h]_ [)] ) _[⊤]_ if _r_ =0
0 if 0 _<r≤_ lssb( _i_ )

- _rr−_ _[′]_ =01 _[S]_ _i_ [(] _−_ _[r][′]_ 1 _[,ℓ,h]_ [)] if _r_ = lssb( _i_ )+1
_Si_ [(] _−_ _[r,ℓ,h]_ 1 [)] if _r>_ lssb( _i_ )+1







where lssb( _t_ ) = max _{ℓ_ _∈_ N _|_ 2 _[ℓ]_ divides _t}_ . Under such settings, a single-layer one-head log-linear attention of dimension
_d_ can be viewed as an RNN of hidden dimension 3 _d_ [2], with hidden states comprising the 3 values of _Si_ [(] _[r]_ [)] that are potentially
non-zero, that is, h _i_ = ( _Si_ [(0)] _, Si_ [(lssb(] _[i]_ [))] _, Si_ [(lssb(] _[i]_ [)+1)] ).


We consider a more general setting


                             _Si_ [(] _[r,ℓ,h]_ [)] = _f_ [(] _[r,ℓ,h]_ [)][ �] ( _Si_ [(] _−_ _[r][′]_ 1 _[,ℓ,h]_ [)] ) _r′∈_ [0 _,r−_ 1] _, x_ [(] _i_ _[ℓ][−]_ [1] _[,h]_ [)] (4)


where _f_ [(] _[r,ℓ,h]_ [)] are certain pre-trained functions. This enables computation with logarithmic time and memory, as it only
requires maintaining a set of hidden states whose size is logarithmic in the sequence length.


A.1.2. FUNCTION EVALUATION AND PERMUTATION COMPOSITION


**Definition A.1** (Function evaluation) **.** In a function evaluation task Eva( _f, x_ ), the input consist of a function _f_ : [ _n_ ] _→_ [ _n_ ]
represented by _n_ tokens encoding _f_ (1) _, · · ·, f_ ( _n_ ), and an element _x ∈_ [ _n_ ] described by a single token, and the goal is to
output Eva( _f, x_ ) = _f_ ( _x_ ) _∈_ [ _n_ ].


The permutation composition task can be viewed as _n_ parallel instances of the function evaluation task.

**Definition A.2** (Permutation composition) **.** In a permutation composition task PerCom( _σ, τ_ ), the input consist of two
bijections _σ, τ_ : [ _n_ ] _→_ [ _n_ ] (each occupies _n_ tokens to describe _σ_ (1) _, · · ·, σ_ ( _n_ ) and then _τ_ (1) _, · · ·, τ_ ( _n_ )), and the goal is to
output their composition in the form of the sequence _σ_ ( _τ_ (1)) _, · · ·, σ_ ( _τ_ ( _n_ )).


**A.2. Lower bound for linear attention on** Eva


In this subsection, we establish communication models of linear attention and log-linear attention for different tasks.

**Theorem A.3.** _An L-layer linear attention cannot solve_ Eva _whenever LHd_ ( _d_ + 1) _p < n_ log _n, while a single-layer full_
_attention solves the task with Hdp_ = _O_ (poly log _n_ ) _. The same holds for linear attention with CoT._


Theorem A.3 can be implied by the following result since linear attention can be viewed as an RNN (Lemma 2.2).

**Theorem A.4.** _An L-layer RNN with H heads, hidden dimension m and precision p cannot solve_ Eva _whenever LHmp <_
_n_ log _n. The same holds for RNNs with CoT._


To prove this theorem, we first present the communication model for an RNN without CoT to solve Eva.


12


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**


The _L_ communication rounds correspond to the _L_ layers of the RNN. In each round _ℓ_, the message _Mℓ_ that Alice sends to
Bob corresponds to the hidden state h [(] _n_ _[ℓ,h]_ [)] at position _n_ of the _ℓ_ -th layer and _h_ -th head. Bob can then use this information to
update the hidden state h [(] _n_ _[ℓ,h]_ +1 [)] [and then compute] _[ y]_ _n_ [(] _[ℓ,h]_ +1 [)][, the output of the] _[ ℓ]_ [-layer at position] _[ n]_ [ + 1][.]


In the variant of this model describing the computation of function evaluation via RNN with CoT, we introduce a third
participant, Charles, to model the computation involving the additional tokens generated during Chain-of-Thought (CoT)
steps. In this extended model, Bob must send both the hidden state and the output _yn_ [(] _[ℓ,h]_ +1 [)] [to Charles. This output] _[ y]_ _n_ [(] _[ℓ,h]_ +1 [)]
serves as the initial value for the first token that Charles processes. Consequently, the size of the message (in bits) becomes
_Hmp_ + _Hdp_ .


The key insight is that Charles’s knowledge is entirely derived from the information sent by Bob. Therefore, if Charles can
compute the final output, then Bob, who possesses at least the same information, must also be able to compute it.


The key to the proof is that the total number of communicated bits is _LHmp_, which should be at least log( _n_ _[n]_ ) = _n_ log _n_
for Bob to distinguish all the functions.


We also establish communication models for PerCom and 2-Sum, detailed proofs are given in Appendix A.


_Proof of Theorem A.4._ We first consider the case without CoT. Over _L_ rounds, Alice sends a total of _LHmp_ bits to Bob,
while Bob sends no messages to Alice. These messages allow Bob to distinguish at most 2 _[LHmp]_ different functions.


If 2 _[LHmp]_ _< n_ _[n]_, or equivalently, if _LHmp < n_ log _n_, then there exist two distinct functions _f_ 1 _, f_ 2 : [ _n_ ] _→_ [ _n_ ] ( _f_ 1 _̸_ = _f_ 2)
that are indistinguishable to Bob; that is, the sequence of messages from Alice is identical for _f_ 1 and _f_ 2. Since _f_ 1 _̸_ = _f_ 2,
there exists some element _x ∈_ [ _n_ ] such that _f_ 1( _x_ ) _̸_ = _f_ 2( _x_ ). If Bob’s input is _x_, it becomes impossible to output the correct
answer, as Bob cannot determine whether to output _f_ 1( _x_ ) or _f_ 2( _x_ ).


In the CoT case, all of Charles’ information is derived from Bob. Therefore, if Charles can determine the answer _f_ ( _x_ ), then
Bob must also be able to compute it. This contradicts the lower bound established for the case without CoT.


**A.3. Lower bound for log-linear attention on** Eva


In this subsection, we establish the communication model for log-linear attention to solve Eva, yielding a lower bound.
**Theorem A.5.** _An L-layer log-linear attention cannot solve_ Eva _whenever LHd_ [2] _p <_ Θ( _n_ ) _, while a single-layer full_
_attention solves the task with Hdp_ = _O_ (poly log _n_ ) _. The same holds for log-linear attention with CoT._


Recall from (3) and (4) that to compute the output _yn_ _[ℓ,h]_ +1 [, Bob requires the hidden states] _[ S]_ _n_ [(] _[r,ℓ,h]_ +1 [)] for all _r ∈_ [0 _, ⌈_ log( _n_ +1)+1 _⌉_ ].
These states, in turn, depend on the previous hidden states _Sn_ [(] _[r,ℓ,h]_ [)], for _r ∈_ [0 _, ⌈_ log( _n_ ) + 1 _⌉_ ]. Therefore, Alice only needs
to send Bob these _O_ (log _n_ ) hidden states. This requires _O_ ( _d_ [2] _p_ log _n_ ) bits per head, resulting in a total message size of
_O_ ( _Hd_ [2] _p_ log _n_ ) bits per layer.





13


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**





In the model for log-linear attention with CoT, we introduce a new player, Charles, to deal with the tokens of CoT
steps. In this model, Bob sends Charles the output _yn_ [(] _[ℓ,h]_ +1 [)] [along with the relevant hidden states, forming a message of]
_Hd_ [2] _p_ log( _n_ + 1) + _Hdp_ bits. However, as in the proof of Theorem A.3, Charles’s entire information state is derived from
Bob. Therefore, introducing Charles does not alter the lower bound established for the case without CoT.


Now we prove the lower bound for log-linear attention on Eva.


_Proof of Theorem A.5._ For the case without CoT, during the _L_ -round communication process, Alice sends Bob a total
of _O_ ( _LHd_ [2] _p_ log _n_ ) bits, while Bob does not send any message to Alice. This communication can distinguish at most
_N_ = 2 _[O]_ [(] _[LHd]_ [2] _[p]_ [ log] _[ n]_ [)] different objects.


If _N < n_ _[n]_, or equivalently, if _LHd_ [2] _p <_ Θ( _n_ ), there should be two distinct functions _f_ 1 _, f_ 2 : [ _n_ ] _→_ [ _n_ ] that are
indistinguishable for Bob (that is, the messages that Alice sends to Bob when the input is _f_ 1 and such messages when the
input is _f_ 2 coincide). Since _f_ 1 _̸_ = _f_ 2, there exists an element _x ∈_ [ _n_ ] such that _f_ 1( _x_ ) _̸_ = _f_ 2( _x_ ). If Bob’s input is _x_, then it is
impossible to produce the correct output, as Bob cannot determine whether the answer should be _f_ 1( _x_ ) or _f_ 2( _x_ ).


In the CoT case, Charles’s entire knowledge is derived from Bob. Therefore, if Charles can compute _f_ ( _x_ ), then Bob must
also be able to compute it. This contradicts the lower bound established for the non-CoT case.


**A.4. Lower bound for log attention and log-linear attention on** PerCom


Analogous to the previous results, we establish communication models for RNNs and log-linear attention to solve PerCom.

**Theorem A.6.** _An L-layer linear attention cannot solve_ PerCom _whenever LHd_ ( _d_ + 1) _p <_ log( _n_ !) = Θ( _n_ log _n_ ) _, while a_
_single-layer full attention solves it with Hdp_ = _O_ (poly log _n_ ) _. The same holds for linear attention with CoT._


For RNN, we prove the following theorem, which, combining with Lemma 2.2 and the construction in Section C, implies
Theorem A.6.

**Theorem A.7.** _An L-layer RNN with H heads, hidden dimension m and precision p cannot solve_ PerCom _whenever_
_LHmp <_ log( _n_ !) _. The same result holds for linear attention with CoT._


_Proof of Theorem A.7._ Similarly, during the _L_ -round communication process, Alice sends a total of _LHmp_ bits, while Bob
does not send any message to Alice. For Bob to distinguish all possible permutations _σ_, one should have 2 _[LHmp]_ _≥_ _n_ !.
Applying Stirling’s formula (Stirling, 1753)



_√_
_n_ ! _∼_




  - _n_
2 _πn_

_e_




- _n_
as _n →∞_



we conclude that _LHmp_ = Ω( _n_ log _n_ ) is necessary for the RNN to solve PerCom. For the CoT case, the same argument as
in the proof of Theorem A.4 proves the desired result.


14


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**


log( _n_ !)
**Theorem A.8.** _An L-layer log-linear attention cannot solve_ PerCom _whenever LHd_ [2] _p <_ log _n_ = Θ( _n_ ) _, while a_

_single-layer full attention solves the task with Hdp_ = _O_ (poly log _n_ ) _. The same holds for log-linear attention with CoT._


_Proof of Theorem A.8._ We consider the following communication model:



For a correct output, Bob must identify the permutation _σ_ . To distinguish all possibilities, one must have 2 _[O]_ [(] _[Hd]_ [2] _[p]_ [ log] _[ n]_ [)] _≥_ _n_ !.
This implies _Hd_ [2] _p ≥_ Ω - log(log _n n_ !) - = Θ( _n_ ) as in the proof of Theorem A.7.


**A.5. Lower bound for log attention and log-linear attention on** 2 **-Sum**


**Theorem A.9.** _An L-layer linear attention cannot solve_ 2 _-Sum whenever LHd_ ( _d_ + 1) _p <_ Θ( _n_ log _n_ ) _, and an L-layer_
_log-linear attention cannot solve_ 2 _-Sum whenever LHd_ [2] _p <_ Θ( _n_ ) _, while a single-layer full attention solves the task with_
_Hdp_ = _O_ (poly log _n_ ) _. The same holds for linear attention and log-linear attention with CoT._


_Proof of Theorem A.9._ We first establish the lower bound for linear attention. Consider the 2-Sum task with an input
sequence of length _n_ . We focus on the last token _xn_ and its corresponding output _yn_ . The value of _yn_ depends on whether
there exists an index _j < n_ such that _xj_ + _xn ≡_ 0 mod _n_ .


We take _M_ = _n_ [2] in the task. We adopt a communication model as follows.





If _LHd_ ( _d_ + 1) _p <_ Θ( _n_ log _n_ ) such that 2 _[LHd]_ [(] _[d]_ [+1)] _[p]_ _<_ - _nn−_ 21�, there exist two sequences of the first _n −_ 1 tokens, say _S_ 1

and _S_ 2, that yield identical hidden states but form different sets _S_ [¯] 1 and _S_ [¯] 2 of values. Therefore, there exists a value _v_ that
appears in _S_ 1 but not in _S_ 2 (or vice versa). Without loss of generality, assume _v_ appears in _S_ 1 but not in _S_ 2. Set Bob’s token
to _xn_ = _−v_ mod _M_ .


For _S_ 1, we have _yn_ = 1 because there exists _j_ with _xj_ = _v_ such that _xj_ + _xn ≡_ 0 mod _n_ . For _S_ 2, since _v_ does not
occur, no such _j_ exists, so _yn_ = 0. However, because the hidden state is the same for both sequences, the model must
produce the same output, leading to an error in (at least) one case. Hence, linear attention cannot solve 2-Sum whenever
_LHd_ ( _d_ + 1) _p <_ Θ( _n_ log _n_ ).


For log-linear attention, we employ a similar communication model. As in the proof of Theorem A.5, the hidden state


15


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**


passed from Alice to Bob has size _O_ ( _LHd_ [2] _p_ log _n_ ) bits. If _LHd_ [2] _p_ log _n <_ Θ( _n_ log _n_ ), i.e., _LHd_ [2] _p <_ Θ( _n_ ), then by an
analogous argument, there exist two sequences _S_ 1 and _S_ 2 of the first _n −_ 1 tokens that induce the same hidden state form
different sets of values. Choosing _xn_ = _−v_ mod _n_ for a value _v_ that appears in _S_ 1 but not in _S_ 2 forces a contradiction,
as the model must output the same _yn_ for both sequences while the correct outputs differ. Therefore, log-linear attention
cannot solve 2-Sum whenever _LHd_ [2] _p <_ Θ( _n_ ).



The upper bound—that a single-layer Transformer decoder with full attention solves 2-Sum with _Hdp_ = _O_ (log _n_ )—follows
from (Sanford et al., 2023), Theorem 6.


For the CoT variants, the same lower bounds apply, as all information Charles receives originates from Bob (If Charles
could compute the output, Bob could as well, contradicting the lower bounds established without CoT). This completes the
proof of Theorem A.9.


**A.6. Lower bound for sparse attention**


_Proof of Theorem 1.2._ Without loss of generality, we assume that _B_ divides _n_, and we prove that a sparse attention
mechanism with small parameters cannot correctly output the expected value at position _n_ + 1. We define the following
communication model for a ( _B, k_ )-sparse attention to calculate this output.













In this model, each block corresponds to a player, and the compressed tokens form the messages sent to the last token.


For the last player to output the correct answer, each of the first players must communicate the set of its _B_ tokens via a
compressed message of _Hdp_ bits. In fact, if this is not the case, that is, there exists two tuples ( _ai_ ) _i∈_ [ _B_ ] and ( _bi_ ) _i∈_ [ _B_ ] such
that _f_ ( _a_ 1 _, · · ·, aB_ ) = _f_ ( _b_ 1 _, · · ·, bB_ ), but _{a_ 1 _, · · ·, aB} ̸_ = _{b_ 1 _, · · ·, bB}_, we can consider the case that the input of each
block is either ( _ai_ ) _i∈_ [ _B_ ] or ( _bi_ ) _i∈_ [ _B_ ].


In this case, all the compressed tokens would be identical, and so would the selection scores for each block. Let _n −_ _xn_ +1
be an element of _{a_ 1 _, · · ·, aB} \ {b_ 1 _, · · ·, bB}_, and consider the case that all the _k_ selected blocks have input ( _bi_ ) _i∈_ [ _B_ ]. In
such case, the last player cannot distinguish between ( _ai_ ) _i∈_ [ _B_ ] and ( _bi_ ) _i∈_ [ _B_ ] for the remaining blocks. Hence, it cannot
correctly output the answer.


Therefore, the compressed message of _Hdp_ bits should communicate the set of _B_ tokens, which is a subset of [ _n_ ] of size at
most _B_, so we have




_∼_ _[n][B]_

_B_ ! _[,]_



2 _[Hdp]_ _≥_ 

_j≤B_




- _n_ - - _n_

_≥_

_j_ _B_



this implies _Hdp_ = Ω( _B_ log _n_ ) and completes the proof.



16


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**


**B. Hybrid Communication Lower Bound**


In this section, we prove Theorem 1.1. We select parameters following the framework of (Chen et al., 2025) and adapt their
techniques of constructing an indistinguishable decomposition to derive the result.


**B.1. Parameters and strateggy**


To prove Theorem 1.1, we construct parameters of the task such that the input length satisfies _n ≤_ ( _Hdp_ ) [4] _[·]_ [16] _[L]_, yet the task
cannot be solved by an ( _L,_ 2 [3] _[L]_ [2] _, · · ·,_ 2 [3] _[L]_ [2] )-hybrid Transformer. We use the following parameters throughout this section:




        _K_ = ( _HdpL_ ) [8] _·_ 8 [2] _[L]_ [2] _,_ _m_ = _K_



_ℓ∈_ [0: _L−_ 1] [8] _[ℓ]_ [+1] _,_ (5)



_nℓ_ = _K_ [4] _[·]_ [8] _[L][−][ℓ][−]_ [1] _, ∀ℓ_ _∈_ [ _L −_ 1] _._ (6)


We assume that _Hdp ≥_ 2. Recall from Definition 2.5 that


         _Nℓ_ = _m ·_ _nℓ′_ _∀ℓ_ _∈_ [0 : _L −_ 1] _._ (7)


_ℓ_ _[′]_ _∈_ [ _ℓ_ ]


We also define the following auxiliary parameters:


_xℓ_ = _K_ [8] _[L][−][ℓ][−]_ [1] _, ∀ℓ_ _∈_ [0 : _L −_ 1] (8)

_Aℓ_ =                       - _Nℓ_ _[N]_ _−_ _[ℓ]_ 1 _[−]_ [1]                       - _, ∀ℓ_ _∈_ [ _L_ ] (9)

~~_√_~~
∆ _ℓ_ = 2 [4] _K_ ( _x_ 0 _...xℓ−_ 2) _·_ ( _n_ 1 _...nL−_ 1) _, ∀ℓ_ _∈_ [2 : _L_ ]) (10)

Θ _ℓ_ = 8 _[−][Lℓ]_ ( _x_ 0 _. . . xℓ_ ) _·_ ( _n_ 1 _. . . nℓ−_ 1) _, ∀ℓ_ _∈_ [ _L −_ 1] _._ (11)


For notational convenience, we also define _A−_ 1 = [�] _i_ _[L]_ =1 _[−]_ [1][[] _[n][i]_ []][ and] _[ A]_ [0][ = [] _[m]_ []][. Recall that we denote the query] _[ w]_ [ by] _[ z][−]_ [1][.]

Thus, player _i_ receives an input from the set _Ai_ for every _i ∈_ [ _−_ 1 : _L_ ]. Note that we view an element of _Aℓ_ = - _Nℓ_ _[N]_ _−_ _[ℓ]_ 1 _[−]_ [1] - can

be viewed as a function [ _Nℓ−_ 1] _→_ [ _Nℓ−_ 1].


We first prove that the task has the desired input size.


**Lemma B.1.** _For the L-sequential function composition task with the parameters defined in_ (6) _, the input prompt length_
_n_ = 2 + _N_ 0 + _N_ 1 + _· · ·_ + _NL−_ 1 _satisfies n ≤_ ( _Hdp_ ) [4] _[·]_ [16] _[L]_ _._


_Proof._ From the parameter definitions in (6) and (7), it follows that 2 _≤_ _N_ 0 and 2 _Nℓ−_ 1 _≤_ _Nℓ_ for all _ℓ_ _∈_ [1 _, L −_ 1].
Consequently, the total input length can be bounded as follows:


_n_ = 2 + _N_ 0 + _N_ 1 + _· · ·_ + _NL−_ 1



_ℓ∈_ [1: _L−_ 1] [4] _[·]_ [8] _[L][−][ℓ][−]_ [1]



(2 _≤_ _N_ 0 _,_ 2 _Nℓ−_ 1 _≤_ _Nℓ_ ) _≤_ 2 _NL−_ 1 = 2 _m ·_ - _nℓ_ = 2 _· K_ 

_ℓ∈_ [ _L−_ 1]



_ℓ∈_ [0: _L−_ 1] [8] _[ℓ]_ [+1+][�]



7 _[·]_ [8] _[L][−]_ [1][+] 7 [2]




  - 7
= 2 _·_ ( _HdpL_ ) [8] _·_ 8 [2] _[L]_ [2][�] [12]



7



7 _[·]_ [8] _[L][−]_ [1][+] 7 [2]



( _Hdp ≥_ 2) _≤_ ( _Hdp_ ) [1+][(] [12] 7



7 [2] [)][(8 log] _[ L]_ [+8+6] _[L]_ [2][)]



_≤_ ( _Hdp_ ) [2] _[·]_ [8] _[L][−]_ [1][(8 log] _[ L]_ [+8+6] _[L]_ [2][)]

(1 + log _L ≤_ _L_ ) _≤_ ( _Hdp_ ) [2] _[·]_ [8] _[L][−]_ [1][(8] _[L]_ [+6] _[L]_ [2][)]

(2 _L ≤_ 2 _[L]_ _, L_ [2] _≤_ 2 _[L]_ ) _≤_ ( _Hdp_ ) [2] _[·]_ [8] _[L][−]_ [1] _[·]_ [10] _[·]_ [2] _[L]_ _≤_ ( _Hdp_ ) [4] _[·]_ [16] _[L]_


It remains to show that the task cannot be solved by an ( _L,_ 2 [3] _[L]_ [2] _, · · ·,_ 2 [3] _[L]_ [2] )-hybrid Transformer. Indeed, we prove the
following stronger result.


**Theorem B.2.** _No deterministic_ ( _L, a_ 1 _, · · ·, aL_ ) _-hybrid communication protocol solves L-_ FuncComp _under the following_
_assumptions._


17


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**



_1. a_ 1 _≤_ 1 _,_


_2. For all ℓ_ _∈_ [ _L −_ 1] _, we have_

_√_
_Hd_ ( _d_ + 1) _p_ ( _a_ 1 + _· · ·_ + _aℓ_ +1) _≤_



_K_ ( _x_ 0 _· · · xℓ−_ 1) _·_ ( _n_ 1 _· · · nL−_ 1) _._ (12)



We now show that our ( _L,_ 1 _,_ 2 [3] _[L]_ [2] _, · · ·,_ 2 [3] _[L]_ [2] ) satisfies the assumptions of Theorem B.2. Consequently, Theorem B.2 implies
Theorem 1.1, since an ( _L_ _−_ 1 _,_ 2 [3] _[L]_ [2] _, · · ·,_ 2 [3] _[L]_ [2] )-hybrid Transformer can be viewed as an ( _L,_ 1 _,_ 2 [3] _[L]_ [2] _, · · ·,_ 2 [3] _[L]_ [2] )-Transformer
with trivial MLP layer _x_ [(] _i_ _[ℓ]_ [)] = _g_ ( _x_ [(] _i_ _[ℓ][−]_ [1)] _, yi_ [(] _[ℓ]_ [)][) :=] _[ x]_ _i_ [(] _[ℓ][−]_ [1)] for the first full attention layer as well as its following linear
attention layer.

**Lemma B.3.** _The assumptions in_ (12) _are satisfied with a_ 1 = 1 _, a_ 2 = _· · ·_ = _aL_ = 2 [3] _[L]_ [2] _._


_Proof of Lemma B.3._ For any _ℓ_ _∈_ [ _L −_ 1], we have


_Hd_ ( _d_ + 1) _p_ ( _a_ 1 + _a_ 2 + _· · ·_ + _aℓ_ +1) _≤_ 2 _HdpL_ 2 [3] _[L]_ [2] _≤_ 8 _[L]_ [2] ( _HdpL_ ) [4] _,_



while for the right-hand side, we have
_√_ _√_

_K_ ( _x_ 0 _· · · xℓ−_ 1) _·_ ( _n_ 1 _· · · nL−_ 1) _≥_



_K_ = 8 _[L]_ [2] ( _HdpL_ ) [4] _≥_ _Hd_ ( _d_ + 1) _p_ ( _a_ 1 + _a_ 2 + _· · ·_ + _aℓ_ +1) _._



**Notation.** For notational convenience, we use _z−_ 1 and _w_ interchangeably to denote player _−_ 1’s input. In the following, we
elaborate on several key definitions that will be crucial to our proof.


  - (Soft transcript Π [(] _j,i_ _[ℓ]_ [)][) For any] _[ i][ ∈]_ [[] _[−]_ [1 :] _[ L][ −]_ [1]][,] _[ j][ ∈]_ [[] _[i]_ [ + 1 :] _[ L]_ []][,] _[ ℓ]_ _[∈]_ [[] _[L]_ []][, recall that][ Π] _j,i_ [(] _[ℓ]_ [)] [denotes the soft transcript sent]
from player _j_ to player _i_ at the _ℓ_ -th epoch of communication. Its value is determined by the inputs of players [ _i_ : _L_ ]
(i.e., _zL, . . ., zi_ ) and is independent of the the inputs of players [ _−_ 1 : _i −_ 1] (i.e., _zi−_ 1 _, . . ., z−_ 1).

For any fixed inputs � _zL ∈_ [ _NL−_ 1] _[N][L][−]_ [1] _, . . .,_   - _zi ∈_ [ _Ni−_ 1] _[N][i][−]_ [1], let Π [(] _j,i_ _[ℓ]_ [)][(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z][i]_ [)][ denote the soft transcript when]
player _t_ receives input _zt_ = � _zt_ ( _t ∈_ [ _i_ : _L_ ]).

  - (Linear transcript Σ [(] _i_ +1 _[ℓ]_ [)] _[,m]_ [) For any] _[ i][ ∈]_ [[] _[−]_ [1] _[, L][ −]_ [1]][,] _[ l][ ∈]_ [[] _[L]_ []][, and] _[ m][ ∈]_ [[0] _[, a][ℓ]_ _[−]_ [1]][, recall][ Σ] _i_ [(] +1 _[ℓ]_ [)] _[,m]_ is the linear transcript
sent from player _i_ + 1 to player _i_ in the ( _m_ + 1)-th linear round of the _ℓ_ -th epoch of communication. Its value is
determined by the inputs of players [ _i_ + 1 : _L_ ] (i.e., _zL, . . ., zi_ +1) and is independent of the the inputs of players

[ _−_ 1 : _i_ ] (i.e., _zi, . . ., z−_ 1).

For any fixed inputs � _zL ∈_ [ _NL−_ 1] _[N][L][−]_ [1] _, . . .,_   - _zi ∈_ [ _Ni−_ 1] _[N][i][−]_ [1], let Σ [(] _i_ +1 _[ℓ]_ [)] _[,m]_ [(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z][i]_ [)][ denote the transcript when player]
_t_ receives input _zt_ = � _zt_ ( _t ∈_ [ _i_ : _L_ ]).


  - (The partial composition value) For any _ℓ_ _∈_ [0 : _L_ ], the value of _iℓ_ is determined by _w, z_ 0 _, . . ., zℓ_ . We write
_iℓ_ ( � _w,_    - _z_ 0 _, . . .,_    - _zℓ_ ) to denote its value when _w_ = � _w, z_ 0 = � _z_ 0 _, . . ., zℓ_ = � _zℓ_ .


**Indistinguishable decomposition.** Our key concept for the proof is _indistinguishable decomposition_ introduced in (Chen
et al., 2025). A indistinguishable decomposition is formed by two sets _R≥ℓ_ and _Z<ℓ_, where _R≥ℓ_ is a set of input assignments
to players [ _ℓ_ : _L_ ] and _Z<ℓ_ is a set of input assignments to players [ _−_ 1 : _ℓ_ _−_ 1]). The key property is that for any fixed input
_z<ℓ_ _∈_ _Z<ℓ_ for the first _ℓ_ players, all assignments to _R≥ℓ_ are indistinguishable to players [ _−_ 1 : _ℓ_ _−_ 1] on inputs _z<ℓ_ after _ℓ_
epochs, because they produce identical communication transcripts. Formally, we adapt the definition in (Chen et al., 2025)
as follows.

**Definition B.4** (Indistinguishable decomposition) **.** Let _ℓ_ _∈_ [2 : _L_ ], an indistinguishable decomposition is a pair of sets
_R≥ℓ_ _⊆_ _AL × AL−_ 1 _× · · · × Aℓ_ and _Z<ℓ_ = _Z−_ 1 _× · · · × Zℓ−_ 1 with _Z−_ 1 = _A−_ 1 _, Z_ 0 _⊆_ _A_ 0 _, · · ·, Zℓ−_ 1 _⊆_ _Aℓ−_ 1, such that
for every � _z<ℓ_ _∈_ _Z<ℓ_, and for every � _α≥ℓ,_ _β_ [�] _≥ℓ_ _∈_ _R≥ℓ_, it satisfies:

Π [(] _j,i_ _[ℓ][′]_ [)][(] _[z]_ [�] _[<ℓ][,]_ [ �] _[α][≥][ℓ]_ [) = Π] _j,i_ [(] _[ℓ][′]_ [)][(] _[z]_ [�] _[<ℓ][,]_ [ �] _[β][≥][ℓ]_ [)]

for every _j ∈_ [ _ℓ_ : _L_ ], _i ∈_ [ _−_ 1 : _ℓ_ _−_ 1], and _ℓ_ _[′]_ _∈_ [ _ℓ_ ], and

Σ [(] _i_ +1 _[ℓ][′]_ [)] _[,m]_ ( _z_            - _<ℓ,_            - _α≥ℓ_ ) = Σ [(] _i_ +1 _[ℓ][′]_ [)] _[,m]_ ( _z_            - _<ℓ,_ _β_ [�] _≥ℓ_ )

for every _i ∈_ [ _−_ 1 : _ℓ_ _−_ 1], _ℓ_ _[′]_ _∈_ [ _ℓ_ ], and _m ∈_ [0 _, aℓ′ −_ 1].


18


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**


The utility of an indistinguishable decomposition becomes clear when _ℓ_ = _L_ . In this case, for every input assignment
from _Z<L_ to players [ _−_ 1 : _L −_ 1], player _−_ 1 (the final output player) observes identical communication transcripts after _L_
epochs (i.e., at the end of the protocol) regardless of which input _⃗zL ∈_ _R≥L_ is assigned to player _L_ . Consequently, for every
_z_ - _<L ∈_ _Z<L_, the output of the protocol _L_ -FuncComp( _z_ - _<L,_ - _zL_ ) must be the same for every � _zL ∈_ _R≥L_ . We will carefully
define the set _R≥ℓ_ and _Z<ℓ_ so that satisfying this requirement leads to a contradiction, thereby establishing the desired
lower bound.


For a subset _Z<ℓ_, we define the set _Iℓ−_ 1( _Z<ℓ_ ) of reachable partial composition values after the ( _ℓ_ _−_ 1)-th epoch as


_Iℓ−_ 1( _Z<ℓ_ ) := _{iℓ−_ 1( _z_           - _−_ 1 _,_           - _z_ 0 _, . . .,_           - _zℓ−_ 1) _|_ ( _z_           - _−_ 1 _,_           - _z_ 0 _, . . .,_           - _zℓ−_ 1) _∈_ _Z<ℓ}._


In words, _Iℓ−_ 1( _Z<ℓ_ ) is the set of all possible values for the intermediate composition when the inputs to players [ _−_ 1 : _ℓ_ _−_ 1]
are restricted to _Z<ℓ_ .


The following lemma, as in (Chen et al., 2025), shows that the desired lower bound follows from a good enough indistinguishable configuration for _ℓ_ = _L_ .

**Lemma B.5.** _An L-epoch hybrid communication protocol does not solve L-_ FuncComp _if there is an indistinguishable_
_decomposition R≥L and Z<L with |R≥L| ≥|AL|/_ ∆ _L and |IL−_ 1( _Z<L_ ) _| ≥_ Θ _L−_ 1 _._


The proof of Theorem B.2 is then completed by constructing the required decomposition via an inductive argument, which
we will present in the next subsection. This construction is formalized in the following lemma.

**Lemma B.6.** _For every_ ( _L, a_ 1 _, . . ., aL_ ) _-hybrid communication protocol under assumptions of_ (12) _, there is an indistin-_
_guishable decomposition R≥L and Z<L satisfying the requirements of Theorem B.5._


The remainder of this section is devoted to proving Lemma B.6. We establish a more general inductive claim in Theorem B.7
below. The case _ℓ_ = _L_ directly implies Theorem B.6.


**Lemma B.7** (Main Lemma) **.** _For any ℓ_ _∈_ [2 : _L_ ] _, we can construct_


  - _a pair of sets_ ( _R≥ℓ, Z<ℓ_ ) _, where R≥ℓ_ _⊆_ _AL × AL−_ 1 _× · · · × Aℓ_ _and Z<ℓ_ = _Z−_ 1 _× Z_ 0 _× · · · × Zℓ−_ 1 _, with_
_Z−_ 1 = [ _n_ 1 _· · · nL−_ 1] _, Z_ 0 _⊆_ _A_ 0 _, Zi ⊆_ _Ai, and |Zi|_ = _xi for i ∈_ [0 : _ℓ_ _−_ 1] _._


  - _the soft transcript from players_ [ _ℓ_ : _L_ ] _to_ [ _−_ 1 : _ℓ_ _−_ 1] _for the first ℓ_ _epochs, when the players_ [ _−_ 1 : _ℓ_ _−_ 1] _take input_
_from Z<ℓ. i.e.,_




   -    Λ [(] _[ℓ]_ [)] := Λ [(] _j,i_ _[ℓ,ℓ][′]_ [)]



_j∈_ [ _ℓ_ : _L_ ] _,i∈_ [ _−_ 1: _ℓ−_ 1] _,ℓ_ _[′]_ _∈_ [ _ℓ_ ]



_where_




    -    Λ _j,i_ [(] _[ℓ,ℓ][′]_ [)] := Λ [(] _j,i_ _[ℓ,ℓ][′]_ [)] ( _z_ - _ℓ−_ 1 _, . . .,_ - _zi_ )



_z_ - _ℓ−_ 1 _∈Zℓ−_ 1 _,...,z_ - _i∈Zi_ _and_ Λ [(] _j,i_ _[ℓ,ℓ][′]_ [)] ( _z_ - _ℓ−_ 1 _, . . .,_ - _zi_ ) _∈_ domain(Π [(] _j,i_ _[ℓ][′]_ [)][)]




- _the linear transcript from player ℓ_ _to ℓ_ _−_ 1 _for the first ℓ_ _epochs, when the players_ [ _−_ 1 : _ℓ_ _−_ 1] _take input from Z<ℓ. i.e.,_




            -            Ξ [(] _[ℓ]_ [)] := Ξ [(] _ℓ_ _[ℓ][′]_ [)] _[,m]_


_such that we have the following properties:_


  - _(_ **Consistency of soft transcripts** _)_



_where_ Ξ [(] _ℓ_ _[ℓ][′]_ [)] _[,m]_ _∈_ domain(Σ [(] _ℓ_ _[ℓ][′]_ [)] _[,m]_ )
_ℓ_ _[′]_ _∈_ [ _ℓ_ ] _,m∈_ [0 _,aℓ′_ _−_ 1]



Π [(] _j,i_ _[ℓ][′]_ [)][(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z][i]_ [) = Λ][(] _j,i_ _[ℓ,ℓ][′]_ [)] ( _z_ - _ℓ−_ 1 _, . . .,_ - _zi_ )

_∀j ∈_ [ _ℓ_ : _L_ ] _, i ∈_ [ _−_ 1 : _ℓ_ _−_ 1] _, ℓ_ _[′]_ _∈_ [ _ℓ_ ] _,_ - _z≥ℓ_ _∈_ _R≥L,_ - _zℓ−_ 1 _∈_ _Zℓ−_ 1 _, . . ._ - _zi ∈_ _Zi,_




- _(_ **Consistency of linear transcripts** _)_

Σ [(] _ℓ_ _[ℓ][′]_ [)] _[,m]_ ( _z_        - _L, . . .,_        - _zℓ_ ) = Ξ [(] _ℓ_ _[ℓ][′]_ [)] _[,m]_ _, ∀ℓ_ _[′]_ _∈_ [ _ℓ_ ] _, m ∈_ [0 _, aℓ′ −_ 1] _,_        - _z≥ℓ_ _∈_ _R≥L,_


- _|R≥ℓ| ≥|AL| · · · |Aℓ|/_ ∆ _ℓ_ _and |Iℓ−_ 1( _Z<ℓ_ ) _| ≥_ Θ _ℓ−_ 1 _._


19


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**


**B.2. The Initial step**


We first prove Lemma B.7 for the base case _ℓ_ = 2.


**Step 1: Choosing** _Z_ 0 _, Z_ 1 **.** We set _Z_ 0 = [ _x_ 0]. The next step is to select the set _Z_ 1 _⊆_ _A_ 1. Consider all possible first epoch
messages from the player 1 to the player _−_ 1, denoted by




   -    Ψ [(1)] 1 _,−_ 1 [=] Ψ [(1)] 1 _,−_ 1 [(] _[z]_ [�] _[−]_ [1][)] where Ψ [(1)] 1 _,−_ 1 [(] _[z]_ [�] _[−]_ [1][)] _[ ∈{]_ [0] _[,]_ [ 1] _[}]_ [2] _[Hdp][.]_

_z_              - _−_ 1 _∈Z−_ 1



The total number of of distinct such message patterns Ψ [(1)] 1 _,−_ 1 [is][ 2][2] _[Hdp][·|][Z][−]_ [1] _[|]_ [ = 2][2] _[Hdp]_ [(] _[n]_ [1] _[···][n][L][−]_ [1][)][. By the pigeonhole principle,]

there exists a message pattern Ψ [�] [(1)] 1 _,−_ 1 _[∈{]_ [0] _[,]_ [ 1] _[}]_ [2] _[Hdp][·]_ [(] _[n]_ [1] _[···][n][L][−]_ [1][)][ such that]


_S_ := _{z_ �1 _∈_ _A_ 1 : Π [(1)] 1 _,−_ 1 [(] _[z]_ [�][1] _[,]_ [ �] _[z][−]_ [1][) = Ψ][(1)] 1 _,−_ 1 [(] _[z]_ [�] _[−]_ [1][)] _[ ∀][z]_ [�] _[−]_ [1] _[ ∈]_ _[Z][−]_ [1] _[} ⊆]_ _[A]_ [1] (13)


satisfies _|S| ≥|A_ 1 _| ·_ 2 _[−]_ [2] _[Hdp][·]_ [(] _[n]_ [1] _[···][n][L][−]_ [1][)] . Note the first epoch message depends only on � _z_ 1 and � _z−_ 1, hence we denote it as
Π [(1)] 1 _,−_ 1 [(] _[z]_ [�][1] _[,]_ [ �] _[z][−]_ [1][)][. The following lemma, as in (][Chen et al.][,][ 2025][), allows us to select a suitable subset] _[ Z]_ [1] _[ ⊆]_ _[S]_ [.]

**Lemma B.8** ((Chen et al., 2025), Lemma 4.6) **.** _There exists a subset Z_ 1 _⊆_ _S with |Z_ 1 _|_ = _x_ 1 _such that_


_|{z_ �1( _i_ 0) : � _z_ 1 _∈_ _Z_ 1 _, i_ 0 _∈_ _Z_ 0 _}| ≥_ 8 _[−][L]_ _x_ 0 _x_ 1 = Θ1 _._ (14)


We take the subset _Z_ 1 given by Lemma B.8. The next step is to fix the transcripts from players _j ∈_ [2 : _L −_ 1] to players
_i_ = _−_ 1 _,_ 0 _,_ 1 at the first two epochs.


**Step 2.1: Fixing the transcript to player** _−_ 1 **.** We begin with the first epoch. Our goal is to fix Λ [(2] _j,−_ _[,]_ [1)] 1 [(] _[z]_ [�][1] _[,]_ [ �] _[z]_ [0] _[,]_ [ �] _[z][−]_ [1][)][ for]
every � _z_ 1 _∈_ _Z_ 1 _,_ - _z_ 0 _∈_ _Z_ 0 _,_ - _z−_ 1 _∈_ _Z−_ 1. The first epoch message from player _j_ to player _−_ 1 depends only on � _z−_ 1 and player
_j_ ’s input, and is independent of � _z_ 1 _,_ - _z_ 0. Therefore, it suffices to fix a pattern




    -    Φ [(1)] _j,−_ 1 [=] Φ [(1)] _j,−_ 1 [(] _[z]_ [�] _[−]_ [1][)] where Φ [(1)] _j,−_ 1 [(] _[z]_ [�] _[−]_ [1][)] _[ ∈{]_ [0] _[,]_ [ 1] _[}]_ [2] _[Hdp][.]_

_z_              - _−_ 1 _∈Z−_ 1



and then define
Λ [(2] _j,−_ _[,]_ [1)] 1 [(] _[z]_ [�][1] _[,]_ [ �] _[z]_ [0] _[,]_ [ �] _[z][−]_ [1][) = Φ][(1)] _j,−_ 1 [(] _[z]_ [�] _[−]_ [1][)] _∀z_ �1 _∈_ _Z_ 1 _,_         - _z_ 0 _∈_ _Z_ 0 _,_         - _z−_ 1 _∈_ _Z−_ 1

The total number of possible such transcript patterns is at most 2 [2] _[Hdp][·|][Z][−]_ [1] _[|]_ = 2 [2] _[Hdp][·]_ [(] _[n]_ [1] _[···][n][L][−]_ [1][)] . By the pigeonhole principle,
we can choose a fixed pattern _{_ Λ [(2] _j,−_ _[,]_ [1)] 1 _[}][j][∈]_ [[2:] _[L]_ []][, such that]


_C_ 1 :=    - ( _z_    - _L, . . .,_    - _z_ 2) _∈_ _AL × · · · × A_ 2 :    - _._
Π [(1)] _j,−_ 1 [(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z]_ [0] _[,]_ [ �] _[z][−]_ [1][) = Λ][(2] _j,−_ _[,]_ [1)] 1 [(] _[z]_ [�][1] _[,]_ [ �] _[z]_ [0] _[,]_ [ �] _[z][−]_ [1][)] _[ ∀][z]_ [�][1] _[ ∈]_ _[Z]_ [1] _[,]_ [ �] _[z]_ [0] _[ ∈]_ _[Z]_ [0] _[,]_ [ �] _[z][−]_ [1] _[ ∈]_ _[Z][−]_ [1] _[, j][ ∈]_ [[2 :] _[ L]_ []]


satisfies _|C_ 1 _| ≥|AL| · · · |A_ 2 _| ·_ 2 _[−]_ [2] _[Hdp][·]_ [(] _[n]_ [1] _[···][n][L][−]_ [1][)] _[·][L]_ _._

For the second epoch, the goal is to fix Λ [(2] _j,−_ _[,]_ [2)] 1 [(] _[z]_ [�][1] _[,]_ [ �] _[z]_ [0] _[,]_ [ �] _[z][−]_ [1][)][ for every][ �] _[z]_ [1] _[ ∈]_ _[Z]_ [1] _[,]_ [ �] _[z]_ [0] _[ ∈]_ _[Z]_ [0] _[,]_ [ �] _[z][−]_ [1] _[ ∈]_ _[Z][−]_ [1][. Crucially, this transcript]

depends only on the information state _X−_ [(1)] 1 [and] _[ X]_ _j_ [(1)], which are themselves independent of the choice of _z_ 1 _∈_ _Z_ 1. This

independence holds because the only component of _X−_ [(1)] 1 [and] _[ X]_ _j_ [(1)] that depends on _z_ 1 is the first epoch message from player

1 to _−_ 1 (recall that _a_ 1 _≤_ 1, (12)), which is fixed to Ψ [(1)] 1 _,−_ 1 [(] _[z]_ [�] _[−]_ [1][)][ (][13][) for all][ �] _[z]_ [1] _[ ∈]_ _[Z]_ [1][. Therefore, it suffices to fix]




    -    Φ [(2)] _j,−_ 1 [:=] Φ [(2)] _j,−_ 1 [(] _[z]_ [�][0] _[,]_ [ �] _[z][−]_ [1][)]



_z_ �0 _∈Z_ 0 _,z_ - _−_ 1 _∈Z−_ 1



and then define Λ [(2] _j,−_ _[,]_ [2)] 1 [(] _[z]_ [�][1] _[,]_ [ �] _[z]_ [0] _[,]_ [ �] _[z][−]_ [1][) = Φ][(2)] _j,−_ 1 [(] _[z]_ [�][0] _[,]_ [ �] _[z][−]_ [1][)] _[,][ ∀][z]_ [�][1] _[ ∈]_ _[Z]_ [1] _[,]_ [ �] _[z]_ [0] _[ ∈]_ _[Z]_ [0] _[,]_ [ �] _[z][−]_ [1] _[ ∈]_ _[Z][−]_ [1][. The total number of transcripts are]

at most 2 [2] _[Hdp][·][x]_ [0] _[·]_ [(] _[n]_ [1] _[···][n][L][−]_ [1][)] . Therefore, we can choose _{_ Λ [(2] _j,−_ _[,]_ [2)] 1 _[}][j][∈]_ [[2:] _[L][−]_ [1]][, such that]



_C_ 2 := - ( _z_ - _L, . . .,_ - _z_ 2) _∈_ _C_ 1 : - _._
Π [(2)] _j,−_ 1 [(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z]_ [0] _[,]_ [ �] _[z][−]_ [1][) = Λ][(2] _j,−_ _[,]_ [2)] 1 [(] _[z]_ [�][1] _[,]_ [ �] _[z]_ [0] _[,]_ [ �] _[z][−]_ [1][)] _[ ∀][z]_ [�][1] _[ ∈]_ _[Z]_ [1] _[,]_ [ �] _[z]_ [0] _[ ∈]_ _[Z]_ [0] _[,]_ [ �] _[z][−]_ [1] _[ ∈]_ _[Z][−]_ [1] _[, j][ ∈]_ [[2 :] _[ L]_ []]


20


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**


satisfies _|C_ 2 _| ≥|C_ 1 _| ·_ 2 _[−]_ [2] _[Hdp][·][x]_ [0][(] _[n]_ [1] _[···][n][L][−]_ [1][)] _[·][L]_ _≥|AL · · · A_ 2 _| ·_ 2 _[−]_ [4] _[HdpL][·][x]_ [0][(] _[n]_ [1] _[···][n][L][−]_ [1][)] .

**Step 2.2: Fixing the transcript to player** 0 **.** The total number of _{_ Λ [(2] _j,_ 0 _[,ℓ][′]_ [)] _}j∈_ [2: _L_ ] _,ℓ′∈_ [2] is at most 2 [2] _[Hdp][·][x]_ [0] _[x]_ [1] _[·]_ [2] _[L]_ . We can
fix its value so that



_C_ 3 :=




- ( _z_ - _L, . . .,_ - _z_ 2) _∈_ _C_ 2 : Π [(] _j,_ _[ℓ]_ 0 _[′]_ [)][(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z]_ [0][) = Λ] _j,_ [(2] 0 _[,ℓ][′]_ [)] ( _z_ �1 _,_ - _z_ 0) _∀z_ �1 _∈_ _Z_ 1 _,_ - _z_ 0 _∈_ _Z_ 0 _, j ∈_ [2 : _L_ ] _, ℓ_ _[′]_ _∈_ [2]



_._



satisfies _|C_ 3 _| ≥|C_ 2 _| ·_ 2 _[−]_ [2] _[Hdp][·][x]_ [0] _[x]_ [1] _[·]_ [2] _[L]_ _≥|AL · · · A_ 2 _| ·_ 2 _[−]_ [6] _[HdpL][·][x]_ [0][(] _[n]_ [1] _[···][n][L][−]_ [1][)] .

**Step 2.3: Fixing the transcript to player** 1 **.** The total number of _{_ Λ [(2] _j,_ 1 _[,ℓ][′]_ [)] _}j∈_ [2: _L_ ] _,ℓ′∈_ [2] is at most 2 [2] _[Hdpm][·][x]_ [1] _[·]_ [2] _[L]_, and the

total number of _{_ Ξ [(] 2 _[ℓ][′]_ [)] _[,m]_ _}ℓ′∈_ [2] _,m∈_ [0 _,aℓ′_ _−_ 1] is at most 2 _[Hd]_ [(] _[d]_ [+1)] _[p]_ [(] _[a]_ [1][+] _[a]_ [2][)], and we can fix the value so that



_C_ 4 :=


satisfies the following bound




- ( _z_  - _L, . . .,_  - _z_ 2) _∈_ _C_ 3 : Σ [(] 2 _[ℓ][′]_ [)] ( _z_  - _L, . . .,_  - _z_ 1) = Ξ [(] 2 _[ℓ][′]_ [)] _, ∀ℓ_ _[′]_ _∈_ [2] and
Π [(] _j,_ _[ℓ]_ 1 _[′]_ [)][(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z]_ [1][) = Λ] _j,_ [(2] 1 _[,ℓ][′]_ [)] ( _z_ �1) _∀z_ �1 _∈_ _Z_ 1 _, j ∈_ [2 : _L_ ] _, ℓ_ _[′]_ _∈_ [2] _,_



_,_



_C_ 4 _≥_ _C_ 3 _·_ 2 _[−]_ [2] _[Hdpm][·][x]_ [1] _[·]_ [2] _[L]_ _·_ 2 _[−]_ [2] _[Hdp]_ _≥|AL| · · · |A_ 2 _| ·_ 2 _[−]_ [8] _[HdpL][·][x]_ [0][(] _[n]_ [1] _[···][n][L][−]_ [1][)] _[−][Hd]_ [(] _[d]_ [+1)] _[p]_ [(] _[a]_ [1][+] _[a]_ [2][)]



~~_√_~~
_≥|AL| · · · |A_ 2 _| ·_ 2 _[−]_ [4]



_Kx_ 0( _n_ 1 _···nL−_ 1)
= _|AL| · · · |A_ 2 _|/_ ∆2 (15)



Here, the second step follows from the choice of parameters (see Eq. (6)(8)(12)) and the last step follows from the definition
of ∆2 (see Eq. (10)).


Combining Lemma B.8 and Eq. (15), we conclude the proof for the base case _ℓ_ = 2.


**B.3. Inductive step**


Suppose Theorem B.7 holds up to _ℓ_ _∈_ [2 : _L −_ 1]. We prove it continues to hold for _ℓ_ + 1.


The key insight is that _Zℓ_ is indistinguishable to players [ _−_ 1 : _ℓ_ _−_ 1] after _ℓ_ epochs, when these players take input from _Z<ℓ_ .
Hence, the ( _ℓ_ + 1)-th epoch transcripts to players [ _−_ 1 : _ℓ_ _−_ 1] are independent of the choice of _zℓ_ _∈_ _Zℓ_ .


**Step 1: Choosing the set** _Zℓ_ **.** Recall that the size of _R≥ℓ_ satisfies _|R≥ℓ| ≥|AL| × · · · × |Aℓ|/_ ∆ _ℓ_ . We would like to select
a rectangular subset from _R≥ℓ_ . As in (Chen et al., 2025), we have the following lemma.

**Lemma B.9** ((Chen et al., 2025), Lemma 4.7) **.** _There exists a subset S_ [(] _[ℓ]_ [)] _⊆_ _R≥ℓ_ _such that_


  - _S_ [(] _[ℓ]_ [)] = _S_ 1 [(] _[ℓ]_ [)] _× S_ 2 [(] _[ℓ]_ [)] _[, where][ S]_ 1 [(] _[ℓ]_ [)] _⊆_ _AL × · · · × Aℓ_ +1 _, S_ 2 [(] _[ℓ]_ [)] _⊆_ _Aℓ, such that_


_|S_ 1 [(] _[ℓ]_ [)] _[| ≥|][A][L][| · · · |][A][ℓ]_ [+1] _[|][/]_ [∆] _ℓ_ [2] _[x][ℓ]_ _and_ _|S_ 2 [(] _[ℓ]_ [)] _[|]_ [ =] _[ x][ℓ][.]_


  - _|{iℓ_ : _iℓ_ = � _zℓ_ ( _w_  - _ℓ−_ 1 _,_ [�] _iℓ−_ 1) _for some_  - _wℓ−_ 1 _∈_ [ _nℓ−_ 1] _,_ [�] _iℓ−_ 1 _∈Iℓ−_ 1 _,_  - _zℓ_ _∈_ _S_ 2 [(] _[ℓ]_ [)] _[}| ≥]_ [Θ] _[ℓ][.]_


With Lemma B.9 in hand, we take _Zℓ_ = _S_ 2 [(] _[ℓ]_ [)] and _S≥ℓ_ +1 = _S_ 1 [(] _[ℓ]_ [)][.]

Next, we are going to fix the transcript Λ [(] _[ℓ]_ [+1)] . Recall that we need to fix all transcripts from players [ _ℓ_ + 1 : _L_ ] to players

[ _−_ 1 : _ℓ_ ] in the first _ℓ_ + 1 epochs, when players [ _−_ 1 : _ℓ_ ] receive input from _Z≤ℓ_ = _Z−_ 1 _× Z_ 0 _× · · · × Zℓ_ . We proceed in a
few steps.


**Step 2.1: Fixing the transcript to players** [ _−_ 1 : _ℓ_ _−_ 1] **in the first** _ℓ_ **epochs.** We simply use Λ [(] _[ℓ]_ [)], that is, for any
_z_ - _ℓ_ _∈_ _Zℓ, . . .,_ - _zi ∈_ _Zi_,

Λ [(] _j,i_ _[ℓ]_ [+1] _[,ℓ][′]_ [)] ( _z_      - _ℓ, . . .,_      - _zi_ ) = Λ [(] _j,i_ _[ℓ,ℓ][′]_ [)] ( _z_      - _ℓ−_ 1 _, . . .,_      - _zi_ ) _._ _∀j ∈_ [ _ℓ_ + 1 : _L_ ] _, i ∈_ [ _−_ 1 : _ℓ_ _−_ 1] _, ℓ_ _[′]_ _∈_ [ _ℓ_ ] (16)


As in (Chen et al., 2025), _S≥ℓ_ +1 _⊆_ _AL × · · · × Aℓ_ +1 is consistent with Λ [(] _[ℓ]_ [+1)] up to this point.


21


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**


**Lemma B.10.** _The set S≥ℓ_ +1 _is consistent with {_ Λ [(] _j,i_ _[ℓ,ℓ][′]_ [)] _}j∈_ [ _ℓ_ +1: _L_ ] _,i∈_ [ _−_ 1: _ℓ−_ 1] _,ℓ′∈_ [ _ℓ_ ] _. Formally, for any_ - _z≥ℓ_ +1 _∈_ _S≥ℓ_ +1 _and_
_z_ - _<ℓ_ +1 _∈_ _Z<ℓ_ +1 _, one has_

Π [(] _j,i_ _[ℓ][′]_ [)][(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z][i]_ [) = Λ] _j,i_ [(] _[ℓ]_ [+1] _[,ℓ][′]_ [)] ( _z_           - _ℓ, . . .,_           - _zi_ ) _._


_for any j ∈_ [ _ℓ_ + 1 : _L_ ] _, i ∈_ [ _−_ 1 : _ℓ_ _−_ 1] _, ℓ_ _[′]_ _∈_ [ _ℓ_ ] _._


**Step 2.2: Fixing the transcript to players** [ _−_ 1 : _ℓ_ _−_ 1] **at the** ( _ℓ_ + 1) **-th epoch.** Our key insight is that _Zℓ_ is
indistinguishable to players [ _−_ 1 : _ℓ_ _−_ 1] when they take input from _Z≤ℓ−_ 1. Hence, their transcripts are independent of the
choice of _zℓ_ _∈_ _Zℓ_ . We consider




    -    Φ [(] _[ℓ]_ [+1)] = Φ [(] _j,i_ _[ℓ]_ [+1)]



_j∈_ [ _ℓ_ +1: _L_ ] _,i∈_ [ _−_ 1: _ℓ−_ 1]



where




    -    Φ [(] _j,i_ _[ℓ]_ [+1)] = Φ [(] _j,i_ _[ℓ]_ [+1)] ( _z_ - _ℓ−_ 1 _, . . .,_ - _zi_ )



_z_ - _ℓ−_ 1 _∈Zℓ...,z_ - _i∈Zi_ and Φ [(] _j,i_ _[ℓ]_ [+1)] ( _z_ - _ℓ−_ 1 _, . . .,_ - _zi_ ) _∈_ domain(Π [(] _j,i_ _[ℓ]_ [+1)] )



Comparing Λ [(] _j,i_ _[ℓ]_ [+1] _[,ℓ]_ [+1)] with Φ [(] _j,i_ _[ℓ]_ [+1)], we note that Φ [(] _j,i_ _[ℓ]_ [+1)] is independent of � _zℓ_ _∈_ _Zℓ_ . For any Φ [(] _[ℓ]_ [+1)], define








_S_ (Φ [(] _[ℓ]_ [+1)] ) :=








( _z_        - _L, . . .,_        - _zℓ_ +1) _∈_ _S≥ℓ_ +1 :
Π [(] _j,i_ _[ℓ]_ [+1)] ( _z_  - _L, . . .,_  - _zi_ ) = Φ [(] _j,i_ _[ℓ]_ [+1)] ( _z_  - _ℓ−_ 1 _, . . .,_  - _zi_ )
_∀z_ - _ℓ_ _∈_ _Zℓ, . . .,_ - _zi ∈_ _Zi, j ∈_ [ _ℓ_ + 1 : _L_ ] _, i ∈_ [ _−_ 1 : _ℓ_ _−_ 1]







(17)





In words, _S_ (Φ [(] _[ℓ]_ [+1)] ) includes all ( _z_ - _L, . . .,_ - _zℓ_ +1) _∈_ _S≥ℓ_ +1 that are consistent with the transcript Φ [(] _[ℓ]_ [+1)] . The proof of the
following lemma differs from (Chen et al., 2025) because in our hybrid communication model, we must account for linear
transcripts.


**Lemma B.11.** _We have_

         - _S_ (Φ [(] _[ℓ]_ [+1)] ) = _S≥ℓ_ +1 _._


Φ [(] _[ℓ]_ [+1)]


_Proof._ It suffices to prove that, for any ( _z_ - _L, . . .,_ - _zℓ_ +1) _∈_ _S≥ℓ_ +1, � _zℓ−_ 1 _∈_ _Zℓ−_ 1 _, . . .,_ - _zi ∈_ _Zi_, _j ∈_ [ _ℓ_ +1 : _L_ ] _, i ∈_ [ _−_ 1 : _ℓ−_ 1],
the transcript Π [(] _j,i_ _[ℓ]_ [+1)] ( _z_ - _L, . . .,_ - _zℓ_ +1 _, zℓ,_ - _zℓ−_ 1 _, . . ._ - _zi_ ) is the same for every _zℓ_ _∈_ _Zℓ_ .

To prove this, note that the transcript Π [(] _j,i_ _[ℓ]_ [+1)] is determined by the information states _Xj_ [(] _[ℓ]_ [)] and _Xi_ [(] _[ℓ]_ [)][. It is clear that] _[ X]_ _j_ [(] _[ℓ]_ [)]
does not change with the choice of _zℓ_ _∈_ _Zℓ_ since _j > ℓ_ . It remains to prove that _Xi_ [(] _[ℓ]_ [)] also does not change with _zℓ_ _∈_ _Zℓ_ .
We prove that all information states _{Xr_ [(] _[ℓ][′]_ [)] _[,m]_ _}r∈_ [ _i_ : _ℓ−_ 1] _,ℓ′∈_ [ _ℓ_ ] do not change with _zℓ_ .

Recall we have fixed the values of ( _z_ - _L, . . .,_ - _zℓ_ +1) _∈_ _S≥ℓ_ +1 and � _zℓ−_ 1 _∈_ _Zℓ−_ 1 _, . . .,_ - _zi ∈_ _Zi_ . We prove this by downward
induction on _r_, from _r_ = _ℓ_ _−_ 1 to _r_ = _i_ . When _r_ = _ℓ_ _−_ 1, the information state _Xℓ_ [(] _−_ _[ℓ][′]_ 1 [)] [is determined by][ �] _[z][ℓ][−]_ [1][,]

Π [(] _t,ℓ_ _[ℓ][′′]_ _−_ [)] 1 [(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z][ℓ]_ [+1] _[, z][ℓ][,]_ [ �] _[z][ℓ][−]_ [1][)][, and][ Σ][(] _ℓ_ _[ℓ][′′]_ [)] _[,m]_ ( _z_ - _L, . . .,_ - _zℓ_ +1 _, zℓ,_ - _zℓ−_ 1) ( _t ∈_ [ _ℓ_ : _L_ ] _, ℓ_ _[′′]_ _∈_ [ _ℓ_ _[′]_ ] _, m ∈_ [0 _, aℓ′′ −_ 1]).

Since ( _z_ - _L, . . .,_ - _zℓ_ +1 _, zℓ_ ) _∈_ _R≥ℓ_ for every _zℓ_ _∈_ _Zℓ_, we have that Π [(] _t,ℓ_ _[ℓ][′′]_ _−_ [)] 1 [(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z][ℓ]_ [+1] _[, z][ℓ][,]_ [ �] _[z][ℓ][−]_ [1][) = Λ][(] _t,ℓ_ _[ℓ,ℓ]_ _−_ _[′′]_ 1 [)][(] _[z]_ [�] _[ℓ][−]_ [1][)][ and]

Σ _ℓ_ [(] _[ℓ][′′]_ [)] _[,m]_ ( _z_ - _L, . . .,_ - _zℓ_ +1 _, zℓ,_ - _zℓ−_ 1) = Ξ [(] _ℓ_ _[ℓ][′′]_ [)] _[,m]_ ( _z_ - _ℓ−_ 1), which are the same for every _zℓ_ _∈_ _Zℓ_ . This finishes the proof of the base

case. Now suppose the assertion holds for _r_ +1. Then, for _r_, _Xr_ [(] _[ℓ][′]_ [)] is determined by � _zr_, Π [(] _t,r_ _[ℓ][′′]_ [)][(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z][ℓ]_ [+1] _[, z][ℓ][,]_ [ �] _[z][ℓ][−]_ [1] _[, . . .,]_ [ �] _[z][r]_ [)][,]

and Σ _r_ [(] _[ℓ]_ +1 _[′′]_ [)] _[,m]_ ( _z_ - _L, . . .,_ - _zℓ_ +1 _, zℓ,_ - _zℓ−_ 1 _, . . .,_ - _zr_ ) ( _t ∈_ [ _r_ + 1 : _L_ ] _, ℓ_ _[′′]_ _∈_ [ _ℓ_ _[′]_ ] _, m ∈_ [0 _, aℓ′′ −_ 1]).



For _t ∈_ [ _ℓ_ : _L_ ], since ( _z_ - _L, . . .,_ - _zℓ_ +1 _, zℓ_ ) _∈_ _R≥ℓ_, we have Π [(] _t,r_ _[ℓ][′′]_ [)][(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z][ℓ]_ [+1] _[, z][ℓ][,]_ [ �] _[z][ℓ][−]_ [1] _[, . . .,]_ [ �] _[z][r]_ [) = Λ] _t,r_ [(] _[ℓ,ℓ][′′]_ [)] ( _z_ - _ℓ−_ 1 _, . . .,_ - _zr_ ),

which is the same for every _zℓ_ _∈_ _Zℓ_ . For _t ∈_ [ _r_ + 1 : _ℓ_ _−_ 1], we have proved that _Xt_ [(] _[ℓ][′′]_ [)] are the same for every _zℓ_ _∈_ _Zℓ_, so
does Π [(] _t,r_ _[ℓ][′′]_ [)][(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z][ℓ]_ [+1] _[, z][ℓ][,]_ [ �] _[z][ℓ][−]_ [1] _[, . . .,]_ [ �] _[z][r]_ [)][. Moreover,][ Σ] _r_ [(] _[ℓ]_ +1 _[′′]_ [)] _[,m]_ ( _z_ - _L, . . .,_ - _zℓ_ +1 _, zℓ,_ - _zℓ−_ 1 _, . . .,_ - _zr_ ) depends only on _Xr_ [(] _[ℓ]_ +1 _[′′]_ [)] _[,m]_,
which is the same for every _zℓ_ _∈_ _Zℓ_ . This completes the induction and finishes the proof.


Now as in (Chen et al., 2025), we obtain


22


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**



**Lemma B.12.** _There exists_ Φ [(] _[ℓ]_ [+1)] _such that_

[�]



~~_√_~~
_|S_ (Φ [�] [(] _[ℓ]_ [+1)] ) _| ≥|AL| · · · |Aℓ_ +1 _| ·_ 2 _[−]_ [2]



_K·_ ( _x_ 0 _···xℓ−_ 1) _·_ ( _n_ 1 _···nL−_ 1)



_The set T≥ℓ_ +1 = _S_ (Φ [�] [(] _[ℓ]_ [+1)] ) _⊆_ _S≥ℓ_ +1 _is consistent with the transcripts_ (Λ [(] _j,i_ _[ℓ]_ [+1] _[,ℓ][′]_ [)] ) _i∈_ [ _−_ 1: _ℓ−_ 1] _,j∈_ [ _ℓ_ +1: _L_ ] _,ℓ′∈_ [ _ℓ_ +1] _. Formally,_
_for any_ ( _z_ - _L, . . .,_ - _zℓ_ +1) _∈_ _T≥ℓ_ +1 _and_ - _z<ℓ_ +1 _∈_ _Z<ℓ_ +1 _, one has_

Π [(] _j,i_ _[ℓ][′]_ [)][(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z][i]_ [) = Λ] _j,i_ [(] _[ℓ]_ [+1] _[,ℓ][′]_ [)] ( _z_           - _ℓ, . . .,_           - _zi_ ) _._ (18)

_for any j ∈_ [ _ℓ_ + 1 : _L_ ] _, i ∈_ [ _−_ 1 : _ℓ_ _−_ 1] _, ℓ_ _[′]_ _∈_ [ _ℓ_ + 1] _. Moreover, we have_


~~_√_~~
_|T≥ℓ_ +1 _| ≥|AL × · · · × Aℓ_ +1 _| ·_ 2 _[−]_ [2] _K·_ ( _x_ 0 _···xℓ−_ 1) _·_ ( _n_ 1 _···nL−_ 1) _._ (19)


**Step 2.3: Fixing the transcript to player** _ℓ_ **.** This follows a greedy selection strategy. Let




  -   Ψ = Ψ [(] _j,ℓ_ _[ℓ][′]_ [)][(] _[z]_ [�] _[ℓ]_ [)]



where Ψ [(] _j,ℓ_ _[ℓ][′]_ [)][(] _[z]_ [�] _[ℓ]_ [)] _[ ∈]_ [domain][(Π][(] _j,ℓ_ _[ℓ][′]_ [)][)]
_j∈_ [ _ℓ_ +1: _L_ ] _,ℓ_ _[′]_ _∈_ [ _ℓ_ +1] _,z_ - _ℓ∈Zℓ_



Define



_T_ (Ψ) :=




- ( _z_ - _L, . . .,_ - _zℓ_ +1) _∈_ _T≥ℓ_ +1 : Π [(] _j,ℓ_ _[ℓ][′]_ [)][(] _[z]_ [�] _[L][, . . .,]_ [ �] _[z][ℓ]_ [) = Ψ][(] _j,ℓ_ _[ℓ][′]_ [)][(] _[z]_ [�] _[ℓ]_ [)] _∀z_ - _ℓ_ _∈_ _Zℓ, ℓ_ _[′]_ _∈_ [ _ℓ_ + 1] _, j ∈_ [ _ℓ_ + 1 : _L_ ]



(20)



As in (Chen et al., 2025), we can upper bound the number of different Ψ and use the pigeonhole principle to obtain the
following lemma.



~~_√_~~
**Lemma B.13.** _The total number of_ Ψ _is at most_ 2



**Lemma B.13.** _The_ ~~_√_~~ _total number of_ Ψ _is at most_ 2 _K·_ ( _x_ 0 _···xℓ−_ 1) _·_ ( _n_ 1 _···nL−_ 1) _. Hence, there exists_ �Ψ _such that |T_ (Ψ)� _| ≥_

_|AL| · · · |Aℓ_ +1 _|_ 2 _[−]_ [3] _K·_ ( _x_ 0 _···xℓ−_ 1) _·_ ( _n_ 1 _···nL−_ 1) _._



_K·_ ( _x_ 0 _···xℓ−_ 1) _·_ ( _n_ 1 _···nL−_ 1) _._



Given Lemma B.13, we fix the transcripts from players _j ∈_ [ _ℓ_ + 1 : _L_ ] to players _ℓ_ during the first _ℓ_ + 1 epochs using Ψ. In

[�]
particular, we take

Λ [(] _j,ℓ_ _[ℓ]_ [+1] _[,ℓ][′]_ [)] ( _z_         - _ℓ_ ) = Ψ [�] [(] _j,ℓ_ _[ℓ][′]_ [)][(] _[z]_ [�] _[ℓ]_ [)] _[,]_ _∀j ∈_ [ _ℓ_ + 1 : _L_ ] _, ℓ_ _[′]_ _∈_ [ _ℓ_ + 1] _,_         - _zi ∈_ _Zi._ (21)




     -     Next, we fix Ξ _[ℓ]_ _ℓ_ +1 _[′]_

_ℓ_ _[′]_ _∈_ [ _ℓ_ +1] [. The total number of choices is][ 2][2] _[Hdp]_ [(] _[a]_ [1][+] _[a]_ [2][+] _[···]_ [+] _[a][ℓ]_ [+1][)][, so there exists a choice for which the]

size of its consistency set is at least


~~_√_~~
_|AL| · · · |Aℓ_ +1 _| ·_ 2 _[−]_ [3] _K·_ ( _x_ 0 _···xℓ−_ 1) _·_ ( _n_ 1 _···nL−_ 1) _−Hd_ ( _d_ +1) _p_ ( _a_ 1+ _a_ 2+ _···_ + _aℓ_ +1) _≥|AL| · · · |Aℓ_ +1 _|/_ ∆ _ℓ_ +1


by (12). We can then take _R≥ℓ_ +1 to be this consistency set, and this completes the induction step.


**C. Upper bound for full attention**


This section provides constructive proofs for the upper bounds stated in Theorems A.3, A.6, A.5, and A.8. Specifically, we
design Transformer decoders equipped with full attention that successfully solve the respective tasks. Central to our design
is a retrieval head mechanism, adapted from (Chen et al., 2025). We note that an alternative, more implicit construction
leveraging nearly orthogonal vectors (Bhattamishra et al., 2024) could similarly be employed.


We now detail the implementation of this retrieval operation using a single attention head.


23


**A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention**


_Implementation of the retrieval head._ We set the value projection _V_ to be _bi_, and the key projection _K_ for position _i ∈_ [ _n_ ]
to be log [2] ( _n_ ) _·_ ( _ai,_ _[⃗]_ 1 _−_ _ai_ ) for position _i ∈_ [ _n_ ], where _[⃗]_ 1 _∈{_ 0 _,_ 1 _}_ _[D]_ denotes the all-ones vector of length _D_, and _[⃗]_ 1 _−_ _ai_
denotes element-wise subtraction; the query projection _Q_ at position _n_ + 1 is taken to be log [2] ( _n_ ) _·_ ( _a,_ _[⃗]_ 1 _−_ _a_ ). The attention
score (before softmax) satisfies


              - log [2] ( _n_ ) _D_ _ai_ = _a_
_⟨Qx_ [(] _n_ _[ℓ]_ +1 [)] _[, Kx]_ [(] _i_ _[ℓ]_ [)] _[⟩]_ [=]
_≤_ log [2] ( _n_ ) _D −_ log [2] ( _n_ ) _ai ̸_ = _a._


Hence, if there is exactly one position _i ∈_ [ _n_ ] that satisfies _ai_ = _a_, then the attention probabilities satisfy


exp(log [2] ( _n_ )) _n_
_αn_ +1 _,i ≥_ exp(log [2] ( _n_ )) + _n −_ 1 _[≥]_ [1] _[ −]_ _n_ [log(] _[n]_ [)] _[,]_


which is indistinguishable from 1 under the assumption of precision _p_ = Θ(log( _n_ )), and


1
_αn_ +1 _,j ≤_ _n_ [log(] _[n]_ [)] _[,]_


for all _j ̸_ = _i_, which is indistinguishable from 0 under the assumption of precision _p_ = Θ(log( _n_ )). We conclude that, under
_p_ = Θ(log _n_ )-bit precision, the attention head will attend exclusively to position _i_ and retrieve the value _bi_ .


Below, we explain how this mechanism enables the solution for Eva and PerCom.


**Construction for** Eva **.** The function evaluation task can be formulated as a retrieval task. Here, the last token of the input
represents the query _a_ = _x ∈_ [ _n_ ]. For each preceding position _i ∈_ [ _n_ ].(whose token encodes the value _f_ ( _i_ )), we set _ai_ = _i_
and _bi_ = _f_ ( _i_ ). The objective is to identify the unique position _i_ satisfying _i_ = _x_ and output _f_ ( _i_ ). Applying the retrieval
head from the previous subsection directly yields _f_ ( _x_ ).


**Construction for** PerCom **.** PerCom can be viewed as _n_ retrieval tasks. The final _n_ tokens of the input represent the
elements _τ_ (1) _, · · ·, τ_ ( _n_ ). Each of these tokens provides a query _a_ for the retrieval operation at its respective position. For
every preceding token _j ∈_ [ _n_ ], the _j_ -th token corresponds to the _σ_ ( _j_ ), and it sets _aj_ = _j_ and _bj_ = _σ_ ( _j_ ). To calculate
_σ_ ( _τ_ ( _i_ )), the model performs the retrieval task to find the unique position _j ∈_ [ _n_ ] such that _j_ = _τ_ ( _i_ ) and then outputs the
value _σ_ ( _j_ ). Therefore, the retrieval head implementation accomplishes PerCom.


**D. Some Missing Proof**


_Proof of Lemma 2.2._ Suppose that we have a linear attention head of dimension _d_ and precision _p_, with query, key, and
value matrices _Q_, _K_, and _V_ . Let _Si_ = _Si−_ 1 + _V xi ⊗_ _φ_ ( _Kxi_ ) _, Zi_ = _Zi−_ 1 + _φ_ ( _Kxi_ ) with _S_ 0 = 0 and _Z_ 0 = 0, we have


_yi_ = _[φ]_ [(] _[Qx][i]_ [)] _[⊤][S][i]_ _._

_φ_ ( _Qxi_ ) _[⊤]_ _Zi_


Let h0 = ( _S_ 0 _, Z_ 0) _∈_ R [2] _[d]_, _g_ ( _x,_ h = ( _S, Z_ )) = ( _S_ + _V x ⊗_ _φ_ ( _Kx_ ) _, Z_ + _φ_ ( _Kx_ )) and


_f_ ( _x,_ h = ( _S, Z_ )) = _[φ]_ [(] _[Qx]_ [)] _[⊤][S]_

_φ_ ( _Qx_ ) _[⊤]_ _Z_


where both _g_ and _f_ are independent of the time step _i_, one easily verifies that the corresponding RNN computes the same
output as the given linear attention layer.


24


