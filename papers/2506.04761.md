## **Log-Linear Attention**

**Han Guo** [1] _[∗]_ **Songlin Yang** [1] _[∗]_ **Tarushii Goel** [1] **Eric P. Xing** [3] **Tri Dao** [2] **Yoon Kim** [1]


1Massachusetts Institute of Technology 2Princeton University, Together AI

3Carnegie Mellon University, Mohamed bin Zayed University of AI, GenBio AI

```
                hanguo@mit.edu

```

**Abstract**


The attention mechanism in Transformers is an important primitive for accurate
and scalable sequence modeling. Its quadratic-compute and linear-memory complexity however remain significant bottlenecks. Linear attention and state-space
models enable linear-time, constant-memory sequence modeling and can moreover
be trained efficiently through matmul-rich parallelization across sequence length.
However, at their core these models are still RNNs, and thus their use of a fixed-size
hidden state to model the context is a fundamental limitation. This paper develops
log-linear attention, an attention mechanism that balances linear attention’s efficiency and the expressiveness of softmax attention. Log-linear attention replaces
the fixed-size hidden state with a logarithmically growing set of hidden states. We
show that with a particular growth function, log-linear attention admits a similarly
matmul-rich parallel form whose compute cost is log-linear in sequence length.
Log-linear attention is a general framework and can be applied on top of existing
linear attention variants. As case studies, we instantiate log-linear variants of two
recent architectures—Mamba-2 and Gated DeltaNet—and find they perform well
compared to their linear-time variants. [2]


**1** **Introduction**


The attention layer [4] is a core building block of modern deep learning architectures, most notably in
the Transformer architecture [61]. For training, attention can be parallelized across sequence length
through reformulating the computation as a series of matrix-matrix multiplications (matmuls), which
can enable efficient training on modern accelerators such as GPUs and TPUs. However, the compute
cost of attention grows quadratically and its memory cost grows linearly with respect to sequence
length; despite the wallclock efficiency improvements obtained from hardware-optimized attention
implementations [15, 11, 55, 34, 32], this quadratic-compute linear-memory cost is a fundamental
limitation in enabling new applications and serves as a significant bottleneck in existing ones.


Linear attention [28] replaces the softmax kernel with a simple linear kernel (i.e., dot product) to
derive the “attention” scores. The use of a linear kernel makes it possible to reformulate linear
attention as a linear RNN with matrix-valued hidden states, and thus linear attention enables lineartime, constant-memory sequence modeling. [3] For training, linear attention can be parallelized across
sequence length via a chunking mechanism where a sequence is split up into chunks and the
computations across chunks are performed in parallel [23, 58, 67, 12]. The complexity of this
chunkwise parallel algorithm is subquadratic in sequence length but still rich in matmuls, [4] leading
to hardware-efficient implementations [64, 49, 6] that obtain practical wallclock improvements


_∗_ Equal contribution.
2Code available at `[https://github.com/HanGuo97/log-linear-attention](https://github.com/HanGuo97/log-linear-attention)` .
3Thus there are three senses in which linear attention is _linear_ : the use of a linear kernel, its reformulation as
a linear RNN where the hidden state is a linear function of the previous state, and its linear-time complexity.
4Unlike parallel scan [8] which can also parallelize linear attention across sequence length but consists mostly
of elementwise operations instead of matmuls.


Preprint. Under review.


**Model** **A** **M (Data Dependent?)** **Training Algorithm / Time** **Decoding Time and Space**


Attention _σ_ ( **QK** _[⊤]_ ) Mask (✗) FlashAttention _O_ ( _T_ [2] ) _O_ ( _T_ ) _O_ ( _T_ )
Linear Attention [28] **QK** _[⊤]_ Mask (✗) Chunk-recurrent _O_ ( _T_ ) _O_ (1) _O_ (1)
RetNet [58] **QK** _[⊤]_ Semiseparable (✗) Chunk-recurrent _O_ ( _T_ ) _O_ (1) _O_ (1)
Mamba-2 [12] **QK** _[⊤]_ Semiseparable (✓) Chunk-recurrent _O_ ( _T_ ) _O_ (1) _O_ (1)
Multi-Hyena [36] **QK** _[⊤]_ Toeplitz (✗) FFT _O_ ( _T_ log _T_ ) _O_ (log [2] _T_ ) _O_ ( _T_ )
DeltaNet [53, 68] _T_ **K** ( **QK** _[⊤]_ ) Mask (✗) Chunk-recurrent _O_ ( _T_ ) _O_ (1) _O_ (1)
Gated DeltaNet [66] _T_ **K** ( **QK** _[⊤]_ ) Semiseparable (✓) Chunk-recurrent _O_ ( _T_ ) _O_ (1) _O_ (1)


Log-Linear Mamba-2 **QK** _[⊤]_ Hierarchical (✓) Chunk-scan _O_ ( _T_ log _T_ ) _O_ (log _T_ ) _O_ (log _T_ )
Log-Linear Gated DeltaNet _T_ **K** ( **QK** _[⊤]_ ) Hierarchical (✓) Chunk-scan _O_ ( _T_ log _T_ ) _O_ (log _T_ ) _O_ (log _T_ )


**Table 1:** Summary of efficient attention mechanisms under the unified formulation: **P** = **A** _⊙_ **M** _,_ **O** = **PV** .
**M** is a lower-triangle (causal) matrix. We use symbol _T_ **K** ( **A** ) = ( **A** _⊙_ **L** ) - **I** + **KK** _[⊤]_ _⊙_ ( **I** _−_ **L** )� _−_ 1 for
notational brevity, where **L** is a lower-triangular matrix of 1s. Here decoding time is the time per step, and
decoding space refers to the overall memory complexity during generation.


over optimized implementations of softmax attention. While early versions of linear attention
generally underperformed softmax attention [27, 44, 35, 47, 58], modern variants with data-dependent
multiplicative gates [67, 50, 42]—which include state-space models (SSMs) such as Mamba [19,
12]—and delta-rule-based structured transition matrices [53, 67, 66, 18, 57] have led to significant
improvements. However, despite these improvements linear attention’s use of a fixed-sized hidden
state is a fundamental limitations when it comes to certain capabilities such as associative recall
over a given context [2]. And empirically, although many recent linear RNNs claim to match or
outperform softmax attention, these results are often based on training and evaluation in short-context
settings. In practice, their performance degrades when exposed to longer contexts [59, 33].


This paper develops _log-linear attention_ as a middle ground between linear attention and full softmax
attention. Instead of using a single hidden state matrix to represent the history (as in linear attention/SSMs), log-linear attention maintains a _growing set_ of hidden states where the set size grows
logarithmically with respect to sequence length. With a particular choice of the growth function,
we show that log-linear attention admits a matmul-rich “parallel form” for training which involves
replacing the lower-triangular causal mask in ordinary linear attention with a data-dependent _hier-_
_archical matrix_, which enables subquadratic training; in particular we show that the compute cost
of log-linear attention is log-linear in sequence length (hence the name), while its memory cost is
logarithmic. Log-linear attention is a general framework for sequence modeling and can be used to
generalize existing linear attention models. As case studies, we use the framework on two popular
recent models, Mamba-2 [12] and Gated DeltaNet [66] to derive log-linear variants of both models,
and find that these variants perform well compared to their original linear variants.


**2** **Background: A Structured Matrix View of Efficient Attention Variants**


Given an input sequence of length _T_ and the corresponding key, query, value matrices
**K** _,_ **Q** _,_ **V** _∈_ R _[T][ ×][d]_, softmax attention obtains the output **O** _∈_ R _[T][ ×][d]_ for all time steps via
**O** = softmax( **QK** _[⊤]_ _⊙_ **M** ) **V**, where **M** _∈{−∞,_ 0 _}_ _[T][ ×][T]_ is a causal masking matrix. This incurs _O_ ( _T_ [2] ) compute and _O_ ( _T_ ) memory, which makes it costly to apply to long sequences. As a
response, there has been much recent work on efficient alternatives with sub-quadratic compute and
sub-linear memory, including linear attention, state-space models, and long convolution models.
Despite their differences, many of these approaches can be captured by the following equation:
**P** = **A** _⊙_ **M** _,_ **O** = **PV** _,_ (1)
where **A** _∈_ R _[T][ ×][T]_ is an attention-like matrix (e.g., **QK** _[⊤]_ in the case of ordinary linear attention)
and **M** _∈_ R _[T][ ×][T]_ is a lower-triangular causal masking matrix (e.g., **M** _∈{_ 0 _,_ 1 _}_ _[T][ ×][T]_ for linear
attention). By separating out the interaction terms **A** and the (potentially data-dependent) masking
matrix **M** (which typically models the “decay factor” between two positions), this abstraction reveals
commonalities across a broad class of models, as shown in Table 1. Moreover, different structures
imposed on **M** can lead to efficient training and inference algorithms. We now describe key models
that fit within this framework.


**Linear attention.** Linear attention [28] simply removes the softmax operation, resulting in the
following parallel form [5]

**O** = ( **QK** _[⊤]_ _⊙_ **M** ) **V** _,_ **M** _ij_ = **1** _{i ≤_ _j}._


5Here we work linear attention without any feature maps or normalization, since most recent works have
found them to be unnecessary (although see [25, 9, 2]).


2


Linear attention can be reparameterized into the following “recurrent form” for inference,

**S** _t_ = **S** _t−_ 1 + _**v**_ _t_ _**k**_ _t_ _[⊤][,]_ _**o**_ _t_ = **S** _t_ _**q**_ _t,_


which enables linear-time constant-memory sequence modeling.


**Linear attention with (data-dependent) gates.** Vanilla linear attention lacks a forgetting mechanism, which has been shown to be crucial in ordinary RNNs. One way to incorporate such a
mechanism is through a scalar gate _αt ∈_ (0 _,_ 1), which results in recurrence **S** _t_ = _αt_ **S** _t−_ 1 + _**v**_ _t_ _**k**_ _t_ _[⊤]_ [.]
This has the following corresponding parallel form:



**O** = ( **QK** _[⊤]_ _⊙_ **M** ) **V** _,_ **M** _ij_ =



_i_

- _αk._ (2)


_k_ = _j_ +1



Originally introduced by Peng et al. [44], gated linear attention has enjoyed a resurgence in recent
years [50, 42, 65, 29] and are an instance of time-varying SSMs [19, 12]. Well-known models
in this family include RetNet [58], which uses a data- _in_ dependent gate _αt_ = _α_, and Mamba-2

[12], which uses the above data-dependent gate. Dao and Gu [12] show that with a scalar gating
factor, **M** has a 1-semiseparable structure where every submatrix in the lower triangular portion
has rank at most 1. Some works such as Mamba [19] and GLA [67] work with data-dependent
gating _matrices_, i.e., **S** _t_ = **G** _t ⊙_ **S** _t−_ 1 + _**v**_ _t_ _**k**_ _t_ _[⊤]_ [where] **[ G]** _[t]_ _[∈]_ [(0] _[,]_ [ 1)] _[d][×][d]_ [. When] **[ G]** _[t]_ [has rank-one]
structure (e.g., as in GLA and GateLoop [29]), it is still possible to have an “efficient” representation
���    via **O** = ( **Q** _⊙_ **B** ) ( **K** _/_ **B** ) _[⊤]_ [��] _⊙_ **M** ) ( **V** _/_ **D** ) _⊙_ **D** where **B** _,_ **D** _∈_ R _[L][×][d]_ capture the per-step

low-rank factorizations of **G** _t_, and **M** _∈{_ 0 _,_ 1 _}_ _[T][ ×][T]_ is now a simple causal masking matrix (see Yang
et al. [67, §C] for the derivation). This type of representation is (to the best of our knowledge) not
possible with the full rank **G** _t_ used by Mamba.


**Linear attention with the delta rule.** DeltaNet [53] is a type of linear attention layer which updates
the hidden state via the delta rule [62], [6] where the recurrent form is given by [7]

**S** _t_ = **S** _t−_ 1         - **I** _−_ _**k**_ _t_ _**k**_ _t_ _[⊤]_         - + _**v**_ _t_ _**k**_ _t_ _[⊤][,]_ _**o**_ _t_ = **S** _t_ _**q**_ _t._


While the original work used a purely recurrent form, Yang et al. [68] recently show that it is
possible to parallelize DeltaNet across sequence length through leveraging a compact representation
of Householder matrices [7, 24], resulting in the following parallel form (cf. [68, §3.2]):



**O** =





 [�] **QK** _[⊤]_ _⊙_ **L** �� **I** + **KK** _[⊤]_ _⊙_ ( **L** _−_ **I** )� _−_ 1

~~�~~  - ~~�~~ ~~�~~
**A**






_⊙_ **M**  **V**



where **L** and **M** are lower-triangular matrices consisting of 1s. Since **A** itself is already lowertriangular, the causal masking matrix **M** is not strictly necessary in the above. However, by changing
**M** to have 1-semiseparable structure as in Mamba-2, we can recover Gated DeltaNet [66], whose
recurrence is given by **S** _t_ = _αt_ **S** _t−_ 1( **I** _−_ _**k**_ _t_ _**k**_ _t_ _[⊤]_ [) +] _**[ v]**_ _[t]_ _**[k]**_ _t_ _[⊤]_ [. Linear attention with such data-dependent]
structured transition matrices has been shown to be theoretically more expressive than linear attention
with multiplicative gates when it comes to certain types of _state-tracking_ tasks [38, 18, 57, 43], which
make these layers attractive targets to generalize via our log-linear attention framework.


**Long convolution models.** Long-convolution sequence models, where the convolution kernel size
equals the sequence length, can also be cast into this framework. For example Toeplitz neural network

[48] and MultiHyena [36] layers are given by **O** = ( **QK** _[⊤]_ _⊙_ **T** _h_ ) **V**, where **T** _h_ is a causal Toeplitz
matrix generated by a long convolution kernel _**h**_ _∈_ R _[T]_, i.e., **T** _h_ [ _i, j_ ] = _**h**_ [ _i −_ _j_ ] for _i ≥_ _j_ and
0 otherwise. Other long convolutional variants like H3 [17] and Hyena [45] also admit a precise
attention-style formulation, which has already been shown in past work [45, 36]. While the decoding
speed of long convolution models can be improved from _O_ ( _T_ ) to _O_ (log [2] _T_ ) per step [41], their
memory cost remains linear, i.e., the same as in softmax attention. However, some long convolution
models such as S4 [20] admit a reparameterization into a time-invariant SSM and thus enjoy constantmemory inference. There has also been efforts to distill long convolution models into RNNs [36, 46],
but these inherit the memory bottleneck of RNNs.


6Linear attention with the delta rule is also an instance of a fast-weight programmer [54].
7The actual DeltaNet recurrence is given by **S** _t_ = **S** _t−_ 1( **I** _−_ _βt_ _**k**_ _t_ _**k**_ _t⊤_ [) +] _**[ k]**_ _t_ _**[v]**_ _t_ _[⊤]_ [where] _[ β]_ _t_ [is a data-dependent]
scalar value in either (0 _,_ 1) or (0 _,_ 2), but we set _βt_ = 1 here for notational brevity.


3


**Relationship between masking structure and efficient algorithms.** Using an unstructured **M**
(e.g., a random lower-triangular matrix) degrades both compute and memory complexity to softmax
attention-levels, despite the absence of softmax; i.e., the _structure_ of **M** is essential for training/inference efficiency, not just the removal of softmax. In linear attention where **M** is a lower-triangular
matrix of 1’s, we can compute **O** chunkwise, leading to an _O_ ( _T_ ) algorithm. [8] This algorithm generalizes to the gated case where **M** has 1-semiseparable structure as shown in the state-space duality
framework [12]. Long convolution models can use FFT to bring down the cost to _O_ ( _T_ log _T_ ).


**3** **Log-Linear Attention**


The preceding section shows that the structure of the
masking matrix **M** in **O** = ( **A** _⊙_ **M** ) **V** plays a key role
in determining compute and memory costs. Our _log-_
_linear attention_ mechanism places a particular structure
on **M** that enables the compute cost to be log-linear
in _T_ (i.e., _O_ ( _T_ log _T_ )) and the memory cost to be logarithmic (i.e., _O_ (log _T_ )). Log-linear attention only
modifies the masking matrix **M** and therefore can be
used to generalize linear attention models whose **A**
matrix can have different structure. As case studies, we
show how to derive log-linear variants of Mamba-2 and
Gated DeltaNet based on our framework.


Log-linear attention employs a Fenwick tree–based
scheme [16] to hierarchically partition the input into
power-of-two-sized segments. Each position summa
**Figure 1:** Standard linear attention (top) vs. log
rizes a range ending at that point, enabling queries to

linear attention (bottom). The input consists of

attend to a logarithmic number of hidden states that

query, key, and value vectors.

capture past context at multiple temporal scales. This
structure naturally emphasizes recent tokens through finer segmentation and supports _O_ (log _T_ ) time
and space complexity during decoding. For training, we show that this formulation corresponds to a
structured **M** that yields a parallel algorithm with _O_ ( _T_ log _T_ ) time and _O_ ( _T_ ) space complexity.


**3.1** **Fenwick Tree Partitioning for Linear Attention**


We begin with the simplest form of linear attention and show how log-linear attention
generalizes it by encoding distinct recurrent memories across different temporal segments.

For effective hierarchical segmentation, the method used to
partition the prefix [0 _, t_ ) for a query _**q**_ _t_ at step _t_ is critical.
A straightforward approach assigns each token _s ∈_ [ _t_ ]
to a level _ℓ_ = _⌊_ log2 _s⌋_, based on its absolute position.
However, in autoregressive decoding, this leads to overly
coarse granularity for the most recent tokens—precisely
the ones most crucial for accurate prediction. Intuitively,
recent context should be modeled with higher resolution.


To address this, we adopt a partitioning scheme based
on the Fenwick tree structure [52, 16], which divides
the prefix [0 _, t_ ) into up to _L_ = _⌈_ log _t⌉_ + 1 disjoint
buckets. This decomposition is guided by the function
lssb( _t_ ) = max _{ℓ_ _∈_ N _|_ 2 _[ℓ]_ divides _t}_, which identifies **Figure 2:** Fenwick tree bucket assignments.
the least significant set bit in the binary representation of _t_ . Conceptually, the partitioning proceeds greedily, at each step subtracting the largest power of two that fits within the remaining segment
of the prefix, asgiven below and shown in Fig. 2,








∅ otherwise



_{b_ [(0)] _t_ _[}]_ if _ℓ_ = 0



_b_ [(] _t_ _[i]_ [)][=]




- _t_ if _i_ = 0

_b_ [(] _t_ _[i][−]_ [1)] _−_ 2lssb� _b_ [(] _t_ _[i][−]_ [1)]  - otherwise _,_ _Bt_ [(] _[ℓ]_ [)][=]




            -            _{b_ [(] _t_ _[i]_ [+1)] _, · · ·, b_ [(] _t_ _[i]_ [)] _[−]_ [1] _[}]_ if _ℓ_ = lssb _b_ [(] _t_ _[i]_ [)] +1



8This algorithm depends on the chunk size _C_, but since _C_ is a hyperparameter this is still linear in _T_ .


4


Each bucket _Bt_ [(] _[ℓ]_ [)] has (at most) a power-of-two length: _|Bt_ [(] _[ℓ]_ [)] _[|]_ [ = 2] _[ℓ][−]_ [1][ for] _[ ℓ]_ _[≥]_ [1][, with a sentinel]
bucket of size _|Bt_ [(0)] _|_ = 1. Then, to obtain the output _**o**_ _t_, log-linear attention computes the recurrent
memory separately for each bucket, and weight the output by a data-dependent scalar _λ_ [(] _t_ _[ℓ]_ [)] _≥_ 0, which
modulates the contribution of its corresponding bucket to the output. These weights are parameterized
as functions of the input _**x**_ _t_ via a linear projection, allowing the model to adaptively attend to different
temporal scales. Concretely, the output is given by,



_L−_ 1

- _λ_ [(] _t_ _[ℓ]_ [)] _**[q]**_ _t_ _[⊤]_ **[S]** [(] _t_ _[ℓ]_ [)] _[,]_ (3)

_ℓ_ =0





_**v**_ _s_ _**k**_ _s_ _[⊤]_  =






 


_s∈Bt_ [(] _[ℓ]_ [)]




_**o**_ _t_ =



_L−_ 1

- _λ_ [(] _t_ _[ℓ]_ [)] _**[q]**_ _t_ _[⊤]_

_ℓ_ =0



where **S** [(] _t_ _[ℓ]_ [)] _∈_ R _[d][×][d]_ is hidden state that summarizes all the information in level _ℓ_ . We observe that
when all _λ_ [(] _t_ _[ℓ]_ [)] are the same (or more generally when the _λ_ [(] _t_ _[ℓ]_ [)] and _λ_ [(] _t_ _[ℓ][′]_ [)] are linearly related across
time) log-linear attention collapses to linear attention. Allowing distinct _λ_ [(] _t_ _[ℓ]_ [)] is therefore essential for
capturing multi-scale temporal structure.


**Parallel form.** While Eq. 3 is conceptually intuitive, it involves primarily matrix-vector products,
which are inefficient on modern hardware optimized for matrix-matrix operations. To better leverage hardware acceleration and enable parallel computation across time steps, we reformulate the
computation into a matmul-friendly form as in Sec. 2:



**O** = - **QK** _[⊤]_ _⊙_ **M** _[H]_ [�]


~~�~~ �� ~~�~~
**A**



**V** _,_ **M** _[H]_ _ts_ [=]




_λt_ _[ℓ]_ [(] _[t,s]_ [)] if _s ≤_ _t_ (4)
0 otherwise



Here, _ℓ_ ( _t, s_ ) denotes the level to which token _s_ belongs at time _t_ under the Fenwick tree partitioning.
For brevity, we omit the explicit dependence on ( _t, s_ ) when the context is clear. Notably, the matrix
**A** exhibits a structured low-rank pattern induced by the Fenwick tree partitioning, as shown below.
In §3.2 we show how we can exploit this structure to derive a _O_ ( _T_ log _T_ ) parallel training algorithm.






































|λ( 00)q 0⊤ k0<br> λ( 11)q 1⊤ k0 λ( 10)q 1⊤ k1<br>|Col2|Col3|<br><br><br>|
|---|---|---|---|
|~~~~<br>~~"~~<br>_λ_(2)<br>2<br>**_q_**2<br>_λ_(2)<br>3<br>**_q_**3<br>~~# ~~<br>**_k_**0<br>**_k_**1<br>_⊤_|_λ_(0)<br>2<br>**_q_**_⊤_<br>2 **_k_**2<br>_λ_(1)<br>3<br>**_q_**_⊤_<br>3 **_k_**2<br>_λ_(0)<br>3<br>**_q_**_⊤_<br>3 **_k_**3||~~~~|
|~~~~<br><br><br>_λ_(3)<br>4<br>**_q_**4<br>_λ_(3)<br>5<br>**_q_**5<br>_λ_(3)<br>6<br>**_q_**6<br>_λ_(3)<br>7<br>**_q_**7<br><br><br><br><br>**_k_**0<br>**_k_**2<br>**_k_**3<br>**_k_**1<br><br><br>_⊤_|~~~~<br><br><br>_λ_(3)<br>4<br>**_q_**4<br>_λ_(3)<br>5<br>**_q_**5<br>_λ_(3)<br>6<br>**_q_**6<br>_λ_(3)<br>7<br>**_q_**7<br><br><br><br><br>**_k_**0<br>**_k_**2<br>**_k_**3<br>**_k_**1<br><br><br>_⊤_|_λ_(0)<br>4<br>**_q_**_⊤_<br>4 **_k_**4<br>_λ_(1)<br>5<br>**_q_**_⊤_<br>5 **_k_**4<br>_λ_(0)<br>5<br>**_q_**_⊤_<br>5 **_k_**5|~~~~|
|~~~~<br><br><br>_λ_(3)<br>4<br>**_q_**4<br>_λ_(3)<br>5<br>**_q_**5<br>_λ_(3)<br>6<br>**_q_**6<br>_λ_(3)<br>7<br>**_q_**7<br><br><br><br><br>**_k_**0<br>**_k_**2<br>**_k_**3<br>**_k_**1<br><br><br>_⊤_|~~~~<br><br><br>_λ_(3)<br>4<br>**_q_**4<br>_λ_(3)<br>5<br>**_q_**5<br>_λ_(3)<br>6<br>**_q_**6<br>_λ_(3)<br>7<br>**_q_**7<br><br><br><br><br>**_k_**0<br>**_k_**2<br>**_k_**3<br>**_k_**1<br><br><br>_⊤_|~~"~~<br>_λ_(2)<br>6<br>**_q_**6<br>_λ_(2)<br>7<br>**_q_**7<br>~~# ~~<br>**_k_**4<br>**_k_**5<br>~~~~_⊤_|_λ_(0)<br>6<br>**_q_**_⊤_<br>6 **_k_**6<br>_λ_(1)<br>7<br>**_q_**_⊤_<br>7 **_k_**6<br>_λ_(0)<br>7<br>**_q_**_⊤_<br>7 **_k_**7<br>~~~~|



**Memory-efficient decoding.** Incremental token-by-token decoding proceeds as follows. Recall that lssb( _t_ ) determines the index of the least significant set bit in the binary representation of _t_ . The new set of hidden states _{_ **S** [(] _t_ _[ℓ]_ [)] _[}][ℓ]_ [are then given by the below equation.]



This recurrence reflects the core structure of the Fenwickpartitioned memory: at each timestep, the current memory
term _**v**_ _t_ _**k**_ _t_ _[⊤]_ [is inserted into the finest-resolution bucket]
( _ℓ_ = 0), while all buckets up to and including lssb( _t_ )
are merged and promoted into a coarser-resolution bucket.
When _t_ is a power of two, a new level is added. This maintains _O_ (log _T_ ) memory during inference. This decoding
process follows the same principle as the online update
and query mechanism in Fenwick trees.








**S** [(] _t_ _[ℓ]_ [)][=]



_**v**_ _t_ _**k**_ _t_ _[⊤]_ if _ℓ_ =0
0 if 0 _<ℓ≤_ lssb( _t_ )

- _ℓℓ−_ _[′]_ =01 **[S]** [(] _t−_ _[ℓ][′]_ [)] 1 if _ℓ_ = lssb( _t_ )+1

**S** [(] _t−_ _[ℓ]_ [)] 1 if _ℓ>_ lssb( _t_ )+1







**Remark.** The matrix **M** _[H]_ (and **A** ) is a lower-triangular instance of a hierarchical ( _H_ ) matrix—specifically, of the HODLR (Hierarchically Off-Diagonal Low-Rank) type. When constructed
using schemes like the Fenwick tree, it inherits the recursive partitioning and low-rank off-diagonal
blocks that define _H_ matrices. This establishes a direct connection between log-linear attention and
hierarchical matrices: the attention operator corresponds to structured matrix multiplication with an
_H_ matrix. We refer to **M** _[H]_ as a quasi- _H_ matrix—a specialized class lying between general _H_ and
semiseparable matrices, designed to support _O_ (log _T_ )-space recurrence. See Section B.1 for details.


5


**Figure 3: Left** : Decomposition of the matrix **M** _[H]_ . **Right** : Chunkwise algorithm (Algorithm 1). Level 0 handles
intra-chunk computations using a quadratic (in chunk size) algorithm, which is efficient due to small chunk
sizes. Levels 1 and above perform inter-chunk computations by invoking existing inter-chunk primitives multiple
times, with overall complexity logarithmic in the number of chunks.


**3.2** **Efficient Algorithm for Training**


The chunkwise parallel algorithm for linear attention [58, 65, 12] splits a sequence into chunks of
length _C_ and performs the computations for all chunks in parallel, while passing information across
chunks when necessary. This offers a balance between the fully parallel and recurrent forms by
reducing the computational cost of global attention while enabling greater sequence-level parallelism
than strict recurrent computations. We show how primitives for efficient chunkwise computation of
linear attention can be adapted to the log-linear case. First observe that the matrix **M** _[H]_ exhibits a
low-rank structure in its off-diagonal blocks, enabling a decomposition of the form:



**M** _[H]_ = **D** +



_L−_ 1

- **M** [(] _[ℓ]_ [)] _,_ **M** [(] _ts_ _[ℓ]_ [)] [=]

_ℓ_ =1




_λ_ [(] _t_ _[ℓ]_ [)] _[,]_ if _s ∈Bt_ [(] _[ℓ]_ [)][,]
0 _,_ otherwise.



_T_

Here, **D** is a block-diagonal matrix with _C_ _[T]_ [blocks] _[ {]_ **[D]** [[] _[k]_ []] _[}]_ _kC_ =1 [, each encoding intra-chunk interactions.]

Each block **D** [[] _[i]_ []] _∈_ R _[C][×][C]_ is lower triangular, where **D** [[] _ts_ _[i]_ []] [=] _[ λ]_ _iC_ [(] _[ℓ]_ [)] + _t_ [.] **[ M]** [(] _[ℓ]_ [)][ captures inter-chunk]
dependencies at level _ℓ_ through a blockwise low-rank structure. See Fig. 3 (left) for an illustration.



Here, **D** is a block-diagonal matrix with _[T]_




_[T]_

_C_ [blocks] _[ {]_ **[D]** [[] _[k]_ []] _[}]_



Building on this structure, we develop a chunkwise log-linear attention algorithm (Algorithm 1). As
shown in Fig. 3 (right), this chunkwise strategy adds a logarithmic overhead on top of linear attention.
This algorithm processes the interactions in two stages.


**Intra-chunk computations (** _ℓ_ =0 **)** : For the block diagonal component **D**, each block is treated as
dense unstructured block, resulting in an ordinary _O_ ( _C_ [2] ) matrix multiplication for the _C_ _[T]_ [diagonal]

blocks, thus incurring _O_ ( _TC_ ) cost in total.

**Inter-chunk computations (** _ℓ>_ 0 **)** : For the blocks corresponding to _{_ **M** [(] _[ℓ]_ [)] _}_ _[L]_ _ℓ_ =1 _[−]_ [1][, the dependencies]
between chunks are handled via a sequence of linear attention passes. Owing to the hierarchical
matrix structure and its decomposition (Eq. 3.2), each level reduces to a computation involving a
sequentially semi-separable (SSS) matrix. When an efficient (linear-time) state-passing primitive is
available—such as those used in Mamba-2 or Gated DeltaNet—inter-chunk computation requires
only _O_ (log _C_ _[T]_ [)][ invocations of this primitive. Each invocation costs] _[ O]_ [(] _[T]_ [)][ in both time and memory,][9]

and thus the total cost of these operations are _O_ ( _T_ log _T_ ).


This method extends the classical scan algorithm to the hierarchical domain, which we term a
_chunkwise parallel scan_ . Unlike token-level scans—often hindered by memory bandwidth limitations
and high I/O overhead during training [65]—chunk scan restructures recurrent memory updates into
parallel operations across chunks. Specifically, it performs _O_ (log _T_ ) independent scans, one per
memory level, each implementable using standard parallel techniques such as the Blelloch scan [8].
Layer-specific weights (e.g., the _λ_ [(] _t_ _[ℓ]_ [)] terms from **M** _[H]_ ) are directly embedded in these scans, enabling
efficient and scalable computation throughout the hierarchy.


9At level _ℓ_, the matrix **M** ( _ℓ_ ) contains 2 _[ℓ][−]_ _T_ [1] _C_ [chunks, each of size][ 2] _[ℓ][−]_ [1] _[C]_ [. By skipping redundant operations,]
the total cost can be reduced by a constant factor of two.


6


- _i_

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
|~~0~~<br> g<br>  r<br> ba<br>  o<br> x<br> n<br>**a**<br> e<br>  an<br> <br> (<br> e<br> n<br> tt<br>_ in_<br>_⊙_<br><br> n<br> <br> tt<br>**nt**<br> d<br> o<br>  i<br>  u<br> s<br>**a**<br>** ve**<br>  at<br> <br>** A**<br> w<br> <br>**  ls**<br> e<br> <br> i<br> r<br> m<br> ll<br> bl|~~6~~<br> t<br>  os<br> -<br>  n.<br> p<br> d<br>**r**<br> d<br> <br>  S<br> S<br> E<br> <br> e<br>_ e_<br><br>_⊙_<br> i<br>  e<br> e<br>**a**<br> t<br> u<br>  n<br>  r<br> e<br>**t**<br> <br>  a<br>** C**<br> <br> e<br> <br>**  .**<br> ls<br> D<br> c<br>  c<br> p<br> y<br> e|hro<br>  s v<br> 2 p<br>  T<br> eri<br>  ch<br>** G**<br> lo<br>  be<br>  ec.<br> SS<br> q.<br>  mat<br> nti<br>_ ar_ <br>**M**_S_<br>**L**<br> <br> llu<br>  ffc<br> nti<br>**tio**<br> he<br> tp<br>  g s<br>  pa<br> nt i<br>**ion**<br>** lin**<br>  -de<br> _t_ h<br> int<br> ver<br> a<br> <br>, r<br> LR<br> es [<br>  orr<br> uta<br>  im<br> st|~~8~~<br> u<br>  a<br>  ri<br>  he<br> m<br>  un<br>** e**<br> g<br>   a<br> <br> )<br>  2<br>  r<br> o<br> <br> <br><br> s<br>  i<br> o<br>**n**<br>  c<br> er<br>  e<br>  ss<br> n<br>**s**<br>** e**<br>  p<br> a<br> o<br>,<br> n<br>A<br> o<br> <br> 2<br>  e<br> ti<br> <br> r|g<br>  r<br>  m<br> <br> e<br> <br>** n**<br> -<br>   p<br>  2<br> s<br>  ).<br>  i<br> n<br> <br><br> �<br> tr<br>  e<br> n<br><br>  h<br> f<br>  t<br>  e<br> <br><br>** a**<br>  e<br> s<br> <br>  it<br> d <br>s<br> u<br> <br>1<br>  s<br> o<br>  p<br> u|~~2~~<br> h<br>  y<br> <br>   t<br> n<br>  k<br>** e**<br> li<br>   p<br> ,<br> t<br> <br>  x <br> <br> a<br><br> <br>**I**<br> a<br>  n<br> <br> <br> o<br>  u<br>  s<br>  t<br>** r**<br> <br> <br> k<br> <br> <br> s<br> g<br>  m<br>]<br>  p<br> <br>  r<br> c|S<br> put<br>  ing<br>  itiv<br>   hro<br> ts<br>  siz<br>** ral**<br> ne<br>   lie<br>   bo<br> ruc<br>  Th<br>** A** <br>  ma<br> m<br>**M**<br> + <br> tes<br>  t c<br>  ma<br>  unk<br> rm<br>  ps,<br>  T<br>  he<br>**  R**<br>  nde<br> ide<br> ey/<br>  is s<br>** M** <br> ho<br> hly<br>  atr<br>. S<br>  on<br> nal<br>  ove<br> tur|~~63~~<br>equ<br>  (l<br>   se<br>  es,<br>   ug<br>  we<br>  e 6<br>** iz**<br> ar<br>   d<br>   th<br> tu<br>  e m<br>.<br>  sk<br> ba-<br>_H_<br>** K**<br>  a<br>  hu<br>  sk<br>  w<br> s<br>   th<br>  ra<br>  Tr<br>**  N**<br>  nt<br> nt<br> qu<br>   til<br> to<br> wn<br>  h<br>  ic<br> tr<br>  din<br>  co<br>  p<br> e t|~~4~~<br><br>ence<br>  eft;<br>   quen<br>  whi<br>   hput<br>  re r<br>   4.<br>** atio**<br>  atte<br>   to t<br>   mo<br> re i<br>   ain<br> Our<br>  wit<br> 2 a<br><br>**V**<br>** K**_⊤_<br>  gen<br>  nk<br>  wit<br>  ise<br>  Flas<br>   rou<br>  nsfo<br>  ansf<br>**  Ns.**<br>  tran<br> ity-<br> ery<br>   l po<br> hig<br> in<br>  alf<br>  es,<br> ong<br>  g r<br>  st,<br>  erfo<br>  hro|



10More precisely, the elementwise product of an SSS matrix and an _H_ matrix remains an _H_ matrix. We
separate them here for clarity.


7


**4** **Experiments**


**4.1** **Synthetic Benchmark**


We first experiment on multi-query associative recall (MQAR) [1], a standard diagnostic benchmark
for evaluating the (in-context) recall capabilities of
architectures. We train for 100 epochs on a dataset
of 10K samples and sweep over learning rates. We
only experiment with (non-gated) DeltaNet as this
variant of linear attention performs best on MQAR.
Our models use two layers, each with 1 head.
















|1.0 Model dimension = 32|Model dimension = 32|
|---|---|
|~~128~~<br>~~256~~<br>~~51~~<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br> <br>Log~~-~~Linear DeltaNet<br>DeltaNet<br>Sequence<br>Accuracy|<br>Log~~-~~Linear DeltaNet<br>DeltaNet|
|~~128~~<br>~~256~~<br>~~51~~<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br> <br>Log~~-~~Linear DeltaNet<br>DeltaNet<br>Sequence<br>Accuracy|Log~~-~~Linear DeltaNet<br>DeltaNet|



**Results.** As shown in Fig. 5, we find that as the **Figure 5:** Experiments on MQAR. The number of
sequence length and number of key-value pairs in- key-value pairs in the input sequence is equal to
creases, the performance of DeltaNet degrades signif- the sequence length divided by 4.
icantly, while Log-Linear DeltaNet maintains high accuracy. Note that softmax attention obtains full
accuracy on all settings.


**4.2** **Language Modeling**


We perform academic-scale language modeling pretraining from scratch using 50B tokens on the
Long-Data-Collections dataset, [11] using a sequence length of 16K. All models have 21 layers and
use a hidden size of 1536. We use a Transformer with 16 attention heads and a RoPE base of 500K,
a modified Mamba-2 with 48 heads and MLP layers, and a Gated DeltaNet with 6 heads. The
Transformer, Mamba-2, and Gated DeltaNet models contain 693M, 802M, and 793M parameters,
respectively. For the _log-linear_ variants, we apply a linear layer on top of the hidden states to compute
the per-head values _λ_ [(] _t_ _[ℓ]_ [)][. This adds less than][ 3%][ additional parameters for Mamba-2 (825M) and less]
than 0 _._ 4% for Gated DeltaNet (796M). Since Mamba-2 and Gated DeltaNet have more parameters
than ordinary Transformers, we also include a (roughly) parameter-matched Transformer variant
with 24 layers (778M parameters) for comparison. For our log-linaer variants, we use the default
hyperparameters from the baselines (§D).


**Model** **Wiki.** **LMB.** **LMB.** **PIQA** **Hella.** **Wino.** **ARC-e** **ARC-c** **Avg.**
ppl _↓_ ppl _↓_ acc _↑_ acc _↑_ acc_n _↑_ acc _↑_ acc _↑_ acc_n _↑_


**Table 2:** Performance comparison on language modeling and zero-shot commonsense reasoning.


**Standard benchmarks.** Following prior work [12, 66], we evaluate models on WikiText perplexity
and several zero-shot commonsense reasoning benchmarks (Table 2). These are short-context tasks
and are therefore largely insensitive to model state size. As such, we generally expect the log-linear
variants to perform comparably to their linear counterparts. Log-Linear Mamba-2 improves upon
its linear counterpart in perplexity and in half of the commonsense reasoning tasks. Log-Linear
Gated DeltaNet shows stronger gains, outperforming its linear version in perplexity and in all but one
reasoning benchmark. Notably, it also outperforms a layer-matched Transformer across all metrics
and a parameter-matched Transformer on half of them.


**Per-position loss.** Following Lin et al. [33], we report the model’s loss at each token position to
evaluate its ability to handle long contexts (Fig. 6). If the loss steadily decreases as the token position
increases, it suggests the model is effectively using the full context. However, if the loss levels off
after a certain point, it indicates the model struggles to make use of information that is too far back


11 `[https://huggingface.co/datasets/togethercomputer/Long-Data-Collections](https://huggingface.co/datasets/togethercomputer/Long-Data-Collections)` .


8


**Figure 6:** Per-position loss on Book3 samples (about 39M tokens) with running average of window size 501.























**Figure 7:** Needle-In-A-Haystack experiments. See Table 6 for details.


in the sequence. For this analysis, we use 39M tokens from Book-3. [12] To improve visualization,
we apply a running average with a window size of 501. We observe that extending both Mamba-2
and Gated DeltaNet to their log-linear counterparts consistently reduces the (smoothed) loss across
various positions, indicating improved long-range context utilization. Log-Linear Gated DeltaNet
also closely tracks the performance of the layer-matched Transformer, although a performance gap
remains when compared to the parameter-matched Transformer.


**Needle-In-A-Haystack.** We use the Needle-In-A-Haystack (NIAH, Fig. 7) benchmark from
RULER [22], where the model must retrieve a value (the “needle”) based on a key hidden in a
long context (the “haystack”). In the simpler single-needle tasks, the log-linear variant of Mamba-2
outperformed its linear counterpart on 8 out of 9 metrics. Gated DeltaNet, which already achieved
perfect accuracy in several cases, saw improvements in 3 metrics, with 3 remaining unchanged. For
the more challenging multi-needle tasks, Log-Linear Mamba-2 again improved in 8 out of 9 metrics,
while Log-Linear Gated DeltaNet achieved improvements across all metrics.


**In-Context Retrieval.** Following Arora et al. [2, 1], we evaluate models on real-world, recallintensive tasks (Table 3). Since these benchmarks were originally designed for short sequences ( _≤_ 2K
tokens), we report results at sequence lengths of 512, 1024, 2048, and (except NQ) 16K. We find that
Log-Linear Mamba-2 yields improvements on roughly half of the tasks (SQuAD, TriviaQA, and NQ).
In contrast, Log-Linear Gated DeltaNet shows more consistent gains, matching or outperforming
Gated DeltaNet across all tasks except DROP.


12 `victor-wu/book3`


9


**SWDE** **SQuAD** **FDA**
**Model** 512 1024 2048 16k 512 1024 2048 16k 512 1024 2048 16k


**TriviaQA** **Drop** **NQ**
**Model** 512 1024 2048 16k 512 1024 2048 16k 512 1024 2048


**Table 3:** Accuracy on retrieval tasks w/ input truncated to different lengths.


**Single-Doc QA** **Multi-Doc QA** **Summarization** **Few-shot** **Code**
**Model** NQA QQA MFQ HQA 2WM Mus GvR QMS MNs TRC TQA SSM LCC RBP


**Table 4:** Accuracy on LongBench tasks [5]: Narrative QA, QasperQA, MultiField QA, HotpotQA, 2WikiMultiQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SamSum, LCC, and RepoBench-P.


**Long context understanding.** Finally, we evaluated the models’ performance on LongBench [5]
(Table 4). We observe that both _Log-Linear_ Mamba-2 and Gated DeltaNet outperforms the baseline
Mamba-2 Gated DeltaNet in 8 out of 14 evaluation tasks.


**5** **Discussion and Limitations**


While log-linear attention improves upon linear attention in many cases, there are still quite a few
tasks where it did not improve upon the linear attention baselines. Due to compute resources we were
unable to experiment with different parameterizations of the _λ_ terms (or hyperparameters in general),
and it is possible that optimal parameterization of _λ_ could lead to improved results. We also still
observe a significant performance gap compared to Transformers also persists across all benchmarks.


The engineering complexity of log-linear attention is higher. Inter-chunk computations conceptually
resemble multiple applications of linear attention primitives, but intra-chunk operations require
bespoke implementations. These intra-chunk mechanisms are a primary factor behind the speed
differences. Additionally, the backward pass is more intricate, as it requires (manually) computing
the gradients not only for the standard attention components but also for the additional _λ_ terms.


Finally, the use of Fenwick-tree partitioning (§3.1) introduces an inductive bias: recent tokens are
allocated more fine-grained memory, while distant tokens are compressed more aggressively. This
design reflects a natural assumption rooted in hierarchical matrix which posits that interactions
between distant elements can be approximated in low-rank form. While intuitive and inspired by
physical phenomena, this inductive bias may not be optimal for all applications. Future work could
explore extensions that enable more flexible structures while preserving computational efficiency.


**6** **Related Work**


Matrix multiplication serves as the computational backbone of modern deep learning. Contemporary
architectures typically consist of a token mixing layer and a channel mixing layer, both of which


10


heavily depend on matrix multiplications. A growing body of research has investigated replacing
dense matrices with _structured matrices_ . For channel mixing, efforts include Butterfly matrices [13],
Monarch matrices [14], and more recently, Block Tensor-Train matrices [51]. Token mixing has been
exemplified by the family of linear attention models [28] and their various kernelizations [63]. Dao
and Gu [12] generalize these approaches by extending low-rank structures to semi-separable matrices,
enabling efficient recurrent inference and subsuming many recent recurrent models. Another line
of work employs sparse attention patterns such as sliding-window attention (SWA), and several
hybrid approaches have also emerged [40, 3, 39]. In this work, we introduce a hierarchical matrix
formulation that supports state expansion while maintaining hardware-efficient training and inference.


Several prior efforts have focused on reducing the quadratic cost of attention to log-linear time complexity [30, 56, 10, 48, 17]. Reformer [30] employs locality-sensitive hashing (LSH) to efficiently
cluster similar queries and keys. Multi-resolution attention [69] adopts a hierarchical approach,
progressively refining attention scores from coarse to fine granularity, while Fast Multipole Attention [26] adapts the classical fast multipole method to efficiently model long-range interactions. In
our work, we leverage the Fenwick tree data structure—a specialized binary indexed tree that enables
efficient prefix sum calculations and updates in logarithmic time—to design an efficient attention
layer during both training and decoding phases. While Zhu and Soricut [71] also employ hierarchical
matrices for attention, their formulation is fully parallel and targeted at modest sequence lengths. In
contrast, our approach adopts a chunkwise-parallel strategy with a custom Triton implementation
optimized for long-sequence training. Derived from linear attention, our design imposes a structured
_H_ -matrix constraint that ensures _O_ (log _T_ ) inference and _O_ ( _T_ log _T_ ) training complexity.


**7** **Conclusion**


We introduced Log-Linear Attention, a general framework that extends a broad class of linear attention
and state-space models to their log-linear counterparts—models with logarithmically growing state
size. This framework offers both theoretical insights and practical benefits, linking structured matrix
theory with hardware-efficient computation. As a case study, we applied this approach to two recent
architectures: Mamba-2 and Gated DeltaNet.


**Acknowledgments**


We thank Tianyuan Zhang, Jyothish Pari, and Adam Zweiger for helpful discussion. This study was
supported by the MIT-Google Program for Computing Innovation, MIT-IBM Watson AI Lab, and the
AI2050 program at Schmidt Sciences (Grant G-25-67980). HG was supported by a Microsoft PhD
Fellowship.


**References**


[1] S. Arora, S. Eyuboglu, A. Timalsina, I. Johnson, M. Poli, J. Zou, A. Rudra, and C. Ré. Zoology:
Measuring and improving recall in efficient language models. _arXiv preprint arXiv:2312.04927_,
2023.


[2] S. Arora, S. Eyuboglu, M. Zhang, A. Timalsina, S. Alberti, D. Zinsley, J. Zou, A. Rudra, and
C. Ré. Simple linear attention language models balance the recall-throughput tradeoff. In
_Proceedings of ICML_, 2024.


[3] S. Arora, S. Eyuboglu, M. Zhang, A. Timalsina, S. Alberti, D. Zinsley, J. Zou, A. Rudra, and
C. Ré. Simple linear attention language models balance the recall-throughput tradeoff, 2025.
URL `[https://arxiv.org/abs/2402.18668](https://arxiv.org/abs/2402.18668)` .


[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align
and translate. In _Proceedings of ICLR_, 2014.


[5] Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al.
Longbench: A bilingual, multitask benchmark for long context understanding. _arXiv preprint_
_arXiv:2308.14508_, 2023.


11


[6] M. Beck, K. Pöppel, P. Lippe, and S. Hochreiter. Tiled flash linear attention: More efficient
linear rnn and xlstm kernels. _arXiv preprint arXiv:2503.14376_, 2025.


[7] C. H. Bischof and C. V. Loan. The WY representation for products of householder matrices.
In _SIAM Conference on Parallel Processing for Scientific Computing_, 1985. URL `[https:](https://api.semanticscholar.org/CorpusID:36094006)`
`[//api.semanticscholar.org/CorpusID:36094006](https://api.semanticscholar.org/CorpusID:36094006)` .


[8] G. E. Blelloch. Prefix sums and their applications. 1990.


[9] J. Buckman, C. Gelada, and S. Zhang. Symmetric Power Transformers.


[10] H. J. Cunningham, G. Giannone, M. Zhang, and M. P. Deisenroth. Reparameterized multiresolution convolutions for long sequence modelling. In _The Thirty-eighth Annual Conference_
_on Neural Information Processing Systems_, 2024. URL `[https://openreview.net/forum?](https://openreview.net/forum?id=RwgNbIpCpk)`
`[id=RwgNbIpCpk](https://openreview.net/forum?id=RwgNbIpCpk)` .


[11] T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In
_Proceedings of ICLR_, 2024.


[12] T. Dao and A. Gu. Transformers are SSMs: Generalized models and efficient algorithms through
structured state space duality. In _Proceedings of ICML_, 2024.


[13] T. Dao, A. Gu, M. Eichhorn, A. Rudra, and C. Ré. Learning fast algorithms for linear transforms
using butterfly factorizations, 2020. URL `[https://arxiv.org/abs/1903.05895](https://arxiv.org/abs/1903.05895)` .


[14] T. Dao, B. Chen, N. S. Sohoni, A. D. Desai, M. Poli, J. Grogan, A. Liu, A. Rao, A. Rudra,
and C. Ré. Monarch: Expressive structured matrices for efficient and accurate training. In
K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvári, G. Niu, and S. Sabato, editors, _International_
_Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_,
volume 162 of _Proceedings of Machine Learning Research_, pages 4690–4721. PMLR, 2022.
URL `[https://proceedings.mlr.press/v162/dao22a.html](https://proceedings.mlr.press/v162/dao22a.html)` .


[15] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient
exact attention with IO-awareness. In _Proceedings of NeurIPS_, 2022.


[16] P. M. Fenwick. A new data structure for cumulative frequency tables. _Software: Practice and_
_Experience_, 24, 1994. URL `[https://api.semanticscholar.org/CorpusID:7519761](https://api.semanticscholar.org/CorpusID:7519761)` .


[17] D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re. Hungry hungry hippos: Towards language modeling with state space models. In _The Eleventh International Conference on_
_Learning Representations_, 2023. URL `[https://openreview.net/forum?id=COZDy0WYGg](https://openreview.net/forum?id=COZDy0WYGg)` .


[18] R. Grazzi, J. Siems, J. K. Franke, A. Zela, F. Hutter, and M. Pontil. Unlocking state-tracking in
linear RNNs through negative eigenvalues. In _Proceedings of ICLR_, 2025.


[19] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. In
_Proceedings of CoLM_, 2024.


[20] A. Gu, K. Goel, and C. Ré. Efficiently modeling long sequences with structured state spaces. In
_Proceedings of ICLR_, 2022.


[21] W. Hackbusch, B. N. Khoromskij, and R. Kriemann. Hierarchical matrices based on a weak
admissibility criterion. _Computing_, 73:207–243, 2004.


[22] C.-P. Hsieh, S. Sun, S. Kriman, S. Acharya, D. Rekesh, F. Jia, Y. Zhang, and B. Ginsburg.
Ruler: What’s the real context size of your long-context language models? _arXiv preprint_
_arXiv:2404.06654_, 2024.


[23] W. Hua, Z. Dai, H. Liu, and Q. Le. Transformer quality in linear time. In _International_
_conference on machine learning_, pages 9099–9117. PMLR, 2022.


[24] T. Joffrain, T. M. Low, E. S. Quintana-Ortí, R. A. van de Geijn, and F. G. V. Zee. Accumulating
householder transformations, revisited. _ACM Trans. Math. Softw._, 32:169–179, 2006. URL
`[https://api.semanticscholar.org/CorpusID:15723171](https://api.semanticscholar.org/CorpusID:15723171)` .


12


[25] P. Kacham, V. Mirrokni, and P. Zhong. Polysketchformer: Fast transformers via sketching
polynomial kernels. _arXiv preprint arXiv:2310.01655_, 2023.


[26] Y. Kang, G. Tran, and H. D. Sterck. Fast multipole attention: A divide-and-conquer attention
mechanism for long sequences, 2024. URL `[https://arxiv.org/abs/2310.11960](https://arxiv.org/abs/2310.11960)` .


[27] J. Kasai, H. Peng, Y. Zhang, D. Yogatama, G. Ilharco, N. Pappas, Y. Mao, W. Chen, and N. A.
Smith. Finetuning pretrained transformers into rnns. In _Proceedings of EMNLP_, 2021.


[28] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive
transformers with linear attention. In _Proceedings of ICML_, 2020.


[29] T. Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling. _arXiv_
_preprint arXiv:2311.01927_, 2023.


[30] N. Kitaev, Ł. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In _Proceedings of_
_ICLR_, 2020.


[31] D. Kressner, S. Massei, and L. Robol. Low-rank updates and a divide-and-conquer method for
linear matrix equations. _SIAM Journal on Scientific Computing_, 41(2):A848–A876, 2019.


[32] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica.
Efficient memory management for large language model serving with pagedattention. In
_Proceedings of SOSP_, 2023.


[33] Z. Lin, E. Nikishin, X. He, and A. Courville. Forgetting transformer: Softmax attention with a
forget gate. In _The Thirteenth International Conference on Learning Representations_, 2025.
URL `[https://openreview.net/forum?id=q2Lnyegkr8](https://openreview.net/forum?id=q2Lnyegkr8)` .


[34] H. Liu, M. Zaharia, and P. Abbeel. Ring attention with blockwise transformers for near-infinite
context. In _Proceedings of ICLR_, 2024.


[35] H. H. Mao. Fine-Tuning Pre-trained Transformers into Decaying Fast Weights. In _Proceedings_
_of EMNLP_, pages 10236–10242.


[36] S. Massaroli, M. Poli, D. Y. Fu, H. Kumbong, R. N. Parnichkun, D. W. Romero, A. Timalsina,
Q. McIntyre, B. Chen, A. Rudra, C. Zhang, C. Re, S. Ermon, and Y. Bengio. Laughing hyena
distillery: Extracting compact recurrences from convolutions. In _Thirty-seventh Conference on_
_Neural Information Processing Systems_, 2023. URL `[https://openreview.net/forum?id=](https://openreview.net/forum?id=OWELckerm6)`
`[OWELckerm6](https://openreview.net/forum?id=OWELckerm6)` .


[37] S. Massei, L. Robol, and D. Kressner. hm-toolbox: Matlab software for hodlr and hss matrices.
_SIAM Journal on Scientific Computing_, 42(2):C43–C68, 2020.


[38] W. Merrill, J. Petty, and A. Sabharwal. The illusion of state in state-space models. In _Proceedings_
_of ICML_, 2024.


[39] T. Munkhdalai, M. Faruqui, and S. Gopal. Leave no context behind: Efficient infinite context
transformers with infini-attention, 2024. URL `[https://arxiv.org/abs/2404.07143](https://arxiv.org/abs/2404.07143)` .


[40] T. Nguyen, V. Suliafu, S. Osher, L. Chen, and B. Wang. Fmmformer: Efficient and flexible
transformer via decomposed near-field and far-field attention. In _Proceedings of NeurIPS_, 2021.


[41] C.-A. Oncescu, S. Purandare, S. Idreos, and S. M. Kakade. Flash inference: Near linear time
inference for long convolution sequence models and beyond. In _The Thirteenth International_
_Conference on Learning Representations_, 2025. URL `[https://openreview.net/forum?](https://openreview.net/forum?id=cZWCjan02B)`
`[id=cZWCjan02B](https://openreview.net/forum?id=cZWCjan02B)` .


[42] B. Peng, D. Goldstein, Q. Anthony, A. Albalak, E. Alcaide, S. Biderman, E. Cheah, T. Ferdinan,
H. Hou, P. Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic
recurrence. _arXiv preprint arXiv:2404.05892_, 3, 2024.


[43] B. Peng, R. Zhang, D. Goldstein, E. Alcaide, X. Du, H. Hou, J. Lin, J. Liu, J. Lu, W. Merrill, et al.
Rwkv-7" goose" with expressive dynamic state evolution. _arXiv preprint arXiv:2503.14456_,
2025.


13


[44] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong. In _Proceedings of_
_ICLR_, 2021.


[45] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and
C. Ré. Hyena hierarchy: Towards larger convolutional language models. In A. Krause,
E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, _International Conference_
_on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202
of _Proceedings of Machine Learning Research_, pages 28043–28078. PMLR, 2023. URL
`[https://proceedings.mlr.press/v202/poli23a.html](https://proceedings.mlr.press/v202/poli23a.html)` .


[46] Z. Qin and Y. Zhong. Accelerating toeplitz neural network with constant-time inference
complexity. In H. Bouamor, J. Pino, and K. Bali, editors, _Proceedings of the 2023 Conference_
_on Empirical Methods in Natural Language Processing_, pages 12206–12215, Singapore, Dec.
2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.750.
URL `[https://aclanthology.org/2023.emnlp-main.750/](https://aclanthology.org/2023.emnlp-main.750/)` .


[47] Z. Qin, W. Sun, H. Deng, D. Li, Y. Wei, B. Lv, J. Yan, L. Kong, and Y. Zhong. cosformer:
Rethinking softmax in attention. In _Proceedings of ICLR_, 2022.


[48] Z. Qin, X. Han, W. Sun, B. He, D. Li, D. Li, Y. Dai, L. Kong, and Y. Zhong. Toeplitz neural
network for sequence modeling. In _Proceedings of ICLR_, 2023.


[49] Z. Qin, W. Sun, D. Li, X. Shen, W. Sun, and Y. Zhong. Lightning attention-2: A free lunch for
handling unlimited sequence lengths in large language models. _arXiv preprint arXiv:2401.04658_,
2024.


[50] Z. Qin, S. Yang, W. Sun, X. Shen, D. Li, W. Sun, and Y. Zhong. HGRN2: Gated Linear RNNs
with State Expansion. In _Proceedings of CoLM_, 2024.


[51] S. Qiu, A. Potapczynski, M. Finzi, M. Goldblum, and A. G. Wilson. Compute better spent:
Replacing dense layers with structured matrices. _ArXiv_, abs/2406.06248, 2024. URL `[https:](https://api.semanticscholar.org/CorpusID:270371652)`
`[//api.semanticscholar.org/CorpusID:270371652](https://api.semanticscholar.org/CorpusID:270371652)` .


[52] B. Y. Ryabko. A fast on-line adaptive code. _IEEE Trans. Inf. Theory_, 38:1400–1404, 1992.
URL `[https://api.semanticscholar.org/CorpusID:206392294](https://api.semanticscholar.org/CorpusID:206392294)` .


[53] I. Schlag, K. Irie, and J. Schmidhuber. Linear Transformers Are Secretly Fast Weight Programmers. In _Proceedings of ICML_, 2021.


[54] J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent
networks. _Neural Computation_, 4(1):131–139, 1992.


[55] J. Shah, G. Bikshandi, Y. Zhang, V. Thakkar, P. Ramani, and T. Dao. FlashAttention-3: Fast
and accurate attention with asynchrony and low-precision. In _Proceedings of NeurIPS_, 2024.


[56] J. Shi, K. A. Wang, and E. B. Fox. Sequence modeling with multiresolution convolutional
memory, 2023. URL `[https://arxiv.org/abs/2305.01638](https://arxiv.org/abs/2305.01638)` .


[57] J. Siems, T. Carstensen, A. Zela, F. Hutter, M. Pontil, and R. Grazzi. Deltaproduct: Improving
state-tracking in linear rnns via householder products. _arXiv preprint arXiv:2502.10297_, 2025.


[58] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei. Retentive network: A
successor to transformer for large language models. _arXiv preprint arXiv:2307.08621_, 2023.


[59] Y. Sun, X. Li, K. Dalal, J. Xu, A. Vikram, G. Zhang, Y. Dubois, X. Chen, X. Wang, S. Koyejo,
T. Hashimoto, and C. Guestrin. Learning to (learn at test time): Rnns with expressive hidden
states, 2025. URL `[https://arxiv.org/abs/2407.04620](https://arxiv.org/abs/2407.04620)` .


[60] P. Tillet, H.-T. Kung, and D. Cox. Triton: an intermediate language and compiler for tiled neural
network computations. In _Proceedings of the 3rd ACM SIGPLAN International Workshop on_
_Machine Learning and Programming Languages_, pages 10–19, 2019.


[61] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I. Polosukhin. Attention is all you need. In _Proceedings of NeurIPS_, 2017.


14


[62] B. Widrow, M. E. Hoff, et al. Adaptive switching circuits. In _IRE WESCON convention record_,
volume 4, pages 96–104. New York, 1960.


[63] Y. Xiong, Z. Zeng, R. Chakraborty, M. Tan, G. Fung, Y. Li, and V. Singh. Nyströmformer: A
nyström-based algorithm for approximating self-attention. In _Proceedings of AAAI_, 2021.


[64] S. Yang and Y. Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, Jan. 2024. URL `[https://github.com/fla-org/](https://github.com/fla-org/flash-linear-attention)`
`[flash-linear-attention](https://github.com/fla-org/flash-linear-attention)` .


[65] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. Gated linear attention transformers with
hardware-efficient training. _arXiv preprint arXiv:2312.06635_, 2023.


[66] S. Yang, J. Kautz, and A. Hatamizadeh. Gated delta networks: Improving mamba2 with delta
rule. In _Proceedings of ICLR_, 2024.


[67] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. In _Proceedings of ICML_, 2024.


[68] S. Yang, B. Wang, Y. Zhang, Y. Shen, and Y. Kim. Parallelizing linear transformers with the
delta rule over sequence length. In _Proceedings of NeurIPS_, 2024.


[69] Z. Zeng, S. Pal, J. Kline, G. M. Fung, and V. Singh. Multi resolution analysis (mra) for
approximate self-attention, 2022. URL `[https://arxiv.org/abs/2207.10284](https://arxiv.org/abs/2207.10284)` .


[70] Y. Zhang and S. Yang. Flame: Flash language modeling made easy, Jan. 2025. URL `[https:](https://github.com/fla-org/flame)`
`[//github.com/fla-org/flame](https://github.com/fla-org/flame)` .


[71] Z. Zhu and R. Soricut. H-transformer-1d: Fast one-dimensional hierarchical attention for
sequences, 2021. URL `[https://arxiv.org/abs/2107.11906](https://arxiv.org/abs/2107.11906)` .


**A** **Generalizing Log-Linear Attention to More Expressive Linear RNNs**


The main paper adopts the following unified view of efficient attention (Eq. 1):


**P** = **A** _⊙_ **M** _,_ **O** = **PV** _,_


This formulation reveals that the key difference between linear and log-linear attention lies in the
structure of the mask matrix **M** _∈_ R _[T][ ×][T]_ . Variations among linear attention models—such as
Mamba-2 and Gated DeltaNet—stem from different parameterizations of **A** . While this perspective
offers a unifying and intuitive framework that captures a wide range of attention mechanisms, it
comes with an important limitation: the state-transition terms are restricted to be scalars (in the case
of Mamba-2) or identity-plus-rank-one matrices (in the case of Gated DeltaNet).


In this section, we introduce a more general framework that relaxes this scalar constraint by allowing
state-transition terms (including the thus _λ_ [(] _t_ _[ℓ]_ [)] terms) to be matrix-valued. This extension enables
richer and more expressive attention mechanisms while preserving computational efficiency.


**Linear Attention as an SSS Tensor.** Consider the standard linear attention mechanism with
data-dependent gating and an SSS (sequentially semiseparable) mask **M** _[S]_ :


**P** = **QK** _[⊤]_ _⊙_ **M** _[S]_ _,_ **O** = **PV** _._


In the main paper, we extend the SSS mask **M** _[S]_ to a hierarchical form **M** _[H]_ . Notice that in Mamba-2,
the resulting matrix **P** also inherits the same structural property, with its SSS-rank governed by the
hidden dimension _d_ :

**P** _t,s_ = **Q** _t_ ( **C** _t · · ·_ **C** _s_ +1) **K** _[⊤]_ _s_ _[,]_ where **C** _t_ = _αt_ **I** _._

We now define a 4D tensor **M** _[S]_ _∈_ R [(] _[T][ ×][T]_ [ )] _[×]_ [(] _[d][×][d]_ [)] such that:

**P** _t,s_ = **Q** _t_ **M** _t,s_ **K** _[⊤]_ _s_ _[,]_ where **M** _t,s_ = **C** _t · · ·_ **C** _s_ +1 _._

Each entry **M** _t,s ∈_ R _[d][×][d]_ is a matrix, making **M** _[S]_ a 4D tensor. We refer to this as an SSS tensor due
to its sequentially semiseparable-like structure along the temporal dimension, though this term is not
yet formalized in the literature.


15


_I_ = _{_ 1 _,_ 2 _,_ 3 _,_ 4 _,_ 5 _,_ 6 _,_ 7 _,_ 8 _}_


_I_ 1 [(2)] = _{_ 1 _,_ 2 _}_ _I_ 2 [(2)] = _{_ 3 _,_ 4 _}_ _I_ 3 [(2)] = _{_ 5 _,_ 6 _}_ _I_ 4 [(2)] = _{_ 7 _,_ 8 _}_


_I_ 1 [(1)] = _{_ 1 _} I_ 2 [(1)] = _{_ 2 _} I_ 3 [(1)] = _{_ 3 _} I_ 4 [(1)] = _{_ 4 _} I_ 5 [(1)] = _{_ 5 _} I_ 6 [(1)] = _{_ 6 _} I_ 7 [(1)] = _{_ 7 _} I_ 8 [(1)] = _{_ 8 _}_


_ℓ_ = 3 _ℓ_ = 2 _ℓ_ = 1 _ℓ_ = 0


**Figure 8:** Visualization adapted from [37, 31]: This example illustrates a cluster tree of depth 3 along with the
corresponding block partitions at each level. Blocks marked with stripes are stored as low-rank matrices in the
HODLR format, while those filled with solid color represent dense matrices.


This tensor-centric view naturally accommodates matrix-valued state transitions **C** _t ∈_ R _[d][×][d]_ with
arbitrary structure, offering a richer representation than scalar- or identity-plus-rank-one-based
approaches. In particular, models such as Mamba-2 and Gated DeltaNet can be interpreted as
operating on 4D tensors with different hidden-dimension structures, while still preserving temporal
semiseparability. [13]



_s_ +1

- _αt′_ [�] **I** _−_ _βt′_ _**k**_ _t′_ _**k**_ _t_ _[⊤][′]_ 
_t_ _[′]_ = _t_



Mamba-2: **M** _[S]_ _t,s_ [=]



_s_ +1


_αt′_ **I** _,_ Gated DeltaNet: **M** _[S]_ _t,s_ [=]
_t_ _[′]_ = _t_



**Log-Linear Attention as an** _H_ **Tensor.** We can apply our _log-linear_ attention to these more flexible
(linear) RNNs by incorporating matrix-valued, level- and data-dependent terms **Λ** [(] _t_ _[ℓ]_ [)] _∈_ R _[d][×][d]_ :



_s_ +1

- _αt′_ [�] **I** _−_ _βt′_ _**k**_ _t′_ _**k**_ _t_ _[⊤][′]_ 
_t_ _[′]_ = _t_



Mamba-2: **M** _[H]_ _t,s_ [=] **[ Λ]** _t_ [(] _[ℓ]_ [)]



_s_ +1


_αt′_ **I** _,_ Gated DeltaNet: **M** _[H]_ _t,s_ [=] **[ Λ]** [(] _t_ _[ℓ]_ [)]
_t_ _[′]_ = _t_



This formulation highlights a key insight: both Mamba-2 and Gated DeltaNet share a common
semiseparable structure in the temporal dimension, but differ in how they structure the hidden
dimension. Mamba-2 relies on scaled identities, while Gated DeltaNet applies identity-minus-rankone modifications. Table 5 summarizes these distinctions.


**Model** **Temporal Structure** **Hidden Size Structure**


Mamba-2 Semiseparable Scaled Identity
Gated DeltaNet Semiseparable Identity plus Low-Rank
_Log-Linear_ Mamba-2 Hierarchical Scaled Identity
_Log-Linear_ Gated DeltaNet Hierarchical Identity plus Low-Rank


**Table 5:** Structural comparison of different attention variants.


**B** **Log-Linear Attention as** _H_ **Matrices**


We begin by introducing two classes of Hierarchical matrices ( _H_ matrices) following Massei et al.

[37]: HODLR (Hierarchically Off-Diagonal Low-Rank) matrices and HSS (Hierarchically SemiSeparable) matrices. We then show how Log-Linear Attention corresponds to a specific subclass of
_H_ matrices that occupies an intermediate position between these two. Finally, we discuss a further
variant of _H_ matrices that, in principle, allows for more refined partitioning—potentially enhancing
approximation quality at the cost of increased (though constant-factor) computational complexity.


13Strictly speaking, Gated DeltaNet also need to include a term _βt_ from _βt_ _**v**_ _t_ _**k**_ _t⊤_ [. For clarity, we omit it here,]
as it can be absorbed into other terms.


16


**B.1** **HODLR Matrices**


HODLR (Hierarchically Off-Diagonal Low-Rank) matrices are structured matrices built via recursive
partitioning, where off-diagonal blocks are low-rank at every level. This structure is formalized
using a cluster tree [37]. Let _T_ be the matrix dimension, and let _T_ be a perfectly balanced binary
tree of depth _L_ whose nodes are subsets of _{_ 1 _, . . ., T_ _}_ . We say _T_ is a cluster tree if: (1) the root
is _I_ = _{_ 1 _, . . ., T_ _}_ ; (2) each level partitions indices into contiguous blocks; (3) every node _I_ [(] _[ℓ]_ [)] _i_ at
level _ℓ_ has two children _I_ 2 [(] _[ℓ]_ _i−_ _[−]_ 1 [1)] [and] _[ I]_ 2 [(] _[ℓ]_ _i_ _[−]_ [1)] that form a disjoint partition of the parent. See Fig. 8 for a
visual example of such a hierarchical partitioning.

Now, let **M** _∈_ R _[T][ ×][T]_ be a square matrix and _T_ a cluster tree as described above. We say that **M** is a
( _T, k_ )-HODLR matrix if,


         -          rank **M** [ _Ii_ [(] _[ℓ]_ [)] _, Ij_ [(] _[ℓ]_ [)][]] _≤_ _k,_ _∀Ii_ [(] _[ℓ]_ [)] _, Ij_ [(] _[ℓ]_ [)] _∈_ sibilings ( _T_ )


This hierarchical low-rank structure enables efficient _O_ ( _T_ log _T_ ) storage and matrix-vector multiplication, making HODLR matrices a core component in fast algorithms for dense matrix computations.
HODLR belongs to the broader class of rank-structured matrices known as Hierarchical matrices ( _H_
matrices).


**B.2** **HSS Matrices**


The _O_ ( _T_ log _T_ ) memory complexity of HODLR matrices arises from their recursive structure: they
consist of _O_ (log _T_ ) levels, each storing low-rank factorizations that require _O_ ( _T_ ) space. In cases
where these low-rank factors exhibit linear dependencies across levels, it is possible to exploit these
relationships through nested hierarchical low-rank representations, potentially reducing the memory
complexity to _O_ ( _T_ ) by eliminating the logarithmic factor [37].


Let _Ii_ [(] _[ℓ]_ [)] and _Ij_ [(] _[ℓ]_ [)] denote a pair of sibling clusters at level _ℓ_ in the cluster tree _T_ . Define _n_ [(] _[ℓ]_ [)] = 2 _[ℓ][−]_ [1] as
the block size at level _ℓ_ . The off-diagonal block corresponding to these clusters can be parameterized
as:


            -            - _⊤_
**M** [ _Ii_ [(] _[ℓ]_ [)] _, Ij_ [(] _[ℓ]_ [)][] =] **[ U]** _i_ [(] _[ℓ]_ [)] **[Σ]** [(] _i,j_ _[ℓ]_ [)] **V** _j_ [(] _[ℓ]_ [)] _,_ where **U** [(] _i_ _[ℓ]_ [)] _[,]_ **[ V]** _j_ [(] _[ℓ]_ [)] _∈_ R _[n]_ [(] _[ℓ]_ [)] _[×][k]_ _,_ **Σ** [(] _i,j_ _[ℓ]_ [)] _[∈]_ [R] _[k][×][k]_


We call **M** matrix a Hierarchically Semiseparable matrices (HSS) if low-rank factors at different
levels are linearly related through some “translation operators” **T** [(] **U** _[ℓ]_ [)] _[,]_ **[ T]** **V** [(] _[ℓ]_ [)] _[∈]_ [R][2] _[k][×][k]_ [ such that,]











**U** [(] _i_ _[ℓ]_ [)] =




**U** [(] _i_ 1 _[ℓ][−]_ [1)] 0
0 **U** [(] _i_ 2 _[ℓ][−]_ [1)]



**T** [(] **U** _[ℓ]_ [)] _,i_ _[,]_ **V** _j_ [(] _[ℓ]_ [)] =




**V** _j_ [(] _[ℓ]_ 1 _[−]_ [1)] 0
0 **V** _j_ [(] _[ℓ]_ 2 _[−]_ [1)]



**T** [(] **V** _[ℓ]_ [)] _,j_



More broadly, HSS matrices belong to a subclass of _H_ matrices known as _H_ [2] matrices.


**B.3** **Quasi-Hierarchical Matrix.**


As discussed above, when the low-rank basis matrices **U** [(] _[ℓ]_ [)] and **V** [(] _[ℓ]_ [)] exhibit linear relationships
across levels _ℓ_, the matrix **M** reduces to a semiseparable form. In this case, both storage and
matrix-vector multiplication complexities can be reduced to _O_ ( _T_ ). Otherwise, **M** retains the general
hierarchical structure with _O_ ( _T_ log _T_ ) complexity.


We define a _Quasi-Hierarchical Matrix_ as one in which only one of the basis sequences, either **U** [(] _[ℓ]_ [)]

or **V** [(] _[ℓ]_ [)], satisfies such a linear nesting property across levels, while the other does not. The matrix
**M** _[H]_ used in the Log-Linear model (Eq. 4) is an instance of this structure.


Both Hierarchical and Quasi-Hierarchical matrices incur _O_ ( _T_ log _T_ ) complexity for storage and
computation during training. However, the use of Quasi-Hierarchical matrices plays a crucial role
in enabling _O_ (log _T_ ) complexity during inference. We are not aware of a recurrent algorithm for
general Hierarchical matrices that achieves logarithmic inference complexity. [14]


14In fact, our initial attempts involved using fully Hierarchical matrices, but we were unable to derive a
recurrent formulation with _O_ (log _T_ ) complexity. This motivated the design of Quasi-Hierarchical matrices
specifically to support efficient recurrence.


17


**Figure 9: Left** : _H_ matrices with strong admissibility. **Right** : _H_ matrices with weak admissibility.


**Reparameterization.** More precisely, Eq. 4 represents a Quasi-Hierarchical matrix that has been
specifically re-parameterized as a composition of the scalar weights _λ_ [(] _[ℓ]_ [)] and a sequentially semiseparable (SSS) matrix **M** _[S]_ . This reparameterization serves two purposes: first, to highlight the
connection between our use of _H_ matrices and the SSS format adopted in prior work; and second, to
enable the block decomposition into a hierarchy of SSS matrices, as shown in Eq. 3.2.


We present this re-parameterization below, along with its 4D tensor variant discussed in §A, where
we additionally assume that the matrices **U** _i_ and **V** _j_ are invertible.


**Matrix:** **Tensor:**



_j_ +1

- **C** _t_


_t_ = _i_



**M** _[H]_ _i,j_ [:=] _[ τ]_ [ (] _i_ _[ℓ]_ [)] _uivj ⇔_ _λ_ [(] _i_ _[ℓ]_ [)]



_i_

- _αt_ **M** _[H]_ _i,j_ [:=] **[ T]** [(] _i_ _[ℓ]_ [)] **[U]** _[i]_ **[V]** _j_ _[⊤]_ _[⇔]_ **[Λ]** [(] _i_ _[ℓ]_ [)]

_t_ = _j_ +1



_j_



_t_ =0



_j_

- **C** _[−]_ _t_ [1]


_t_ =0



0

- **C** _t,_ **V** _j_ _[⊤]_ [:=]


_t_ = _i_



_⇒_ _τi_ [(] _[ℓ]_ [)] := _λ_ [(] _i_ _[ℓ]_ [)] _[, u][i]_ [ :=]



_i_

- _αt, vj_ :=


_t_ =0



1
**T** [(] _i_ _[ℓ]_ [)] := **Λ** [(] _i_ _[ℓ]_ [)] _[,]_ **[ U]** _[i]_ [ :=]
_αt_



_⇐_ _λ_ [(] _i_ _[ℓ]_ [)] := _τi_ [(] _[ℓ]_ [)] _uivi,_ _at_ := _[r][t][−]_ [1] **Λ** [(] _i_ _[ℓ]_ [)] := **T** [(] _i_ _[ℓ]_ [)] **[U]** _[i]_ **[V]** _i_ _[⊤][,]_ **C** _t_ := **R** _[−]_ _t_ [1] **[R]** _[t][−]_ [1]

_rt_



**B.4** _H_ **Matrices with Strong and Weak Admissibility**


In the recurrent formulation of Log-Linear Attention, although there are _O_ (log _T_ ) states corresponding to different hierarchical levels, roughly half of them are zero in practice. This sparsity arises from
the specific structure of HODLR matrices, which belong to a broader class of _H_ matrices known as
_weakly admissible_ [21].


Figures 10 and 9 illustrate an alternative structure based on strong (or standard) admissibility. Unlike
the weakly admissible variant, strongly admissible _H_ matrices allow for finer-grained partitioning of
the matrix, and their corresponding recurrent forms utilize all hierarchical levels.


While strong admissibility can yield more accurate approximations, it comes with a significant computational cost [21]. In our early experiments, using strong admissibility in a `Triton` implementation
resulted in up to a `4x` slowdown, with only marginal improvements in accuracy. As a result, we adopt
the weakly admissible structure throughout this work and refer to it simply as the _H_ -matrix.


**Figure 10: Left** : _H_ matrices with strong admissibility. **Right** : _H_ matrices with weak admissibility.


18


**C** **Implementations**


1 `import` `torch`
2 `import` `numpy as np`
3 `import` `torch.nn.functional` `as F`
4
5
6 `def` `segsum(x):`
7 `T = x.size (-1)`
8 `x_cumsum = torch.cumsum(x, dim=-1)`
9 `x_segsum = x_cumsum [..., :, None] - x_cumsum [..., None, :]`
10 `mask = torch.tril(torch.ones(T, T, device=x.device, dtype=` `bool` `))`
11 `x_segsum = x_segsum.masked_fill (~mask, -torch.inf)`
12 `return` `x_segsum`
13
14
15 `def` `level_mask(level, T):`
16 `if level == 0:`
17 `return` `torch.eye(T, dtype=torch.` `bool` `)`
18
19 `i, j = torch.meshgrid(torch.arange(T), torch.arange(T), indexing=` `"ij"` `)`
20 `half = 1 << (level - 1)`
21 `clipped = i - (i %`
22 `valid = (i %`
23 `return` `valid`
24
25
26 `def` `construct_H_matrix (a, L):`
27 `T = a.size (-1)`
28 `A = torch.exp(segsum(a))`
29 `return` `sum` `([A * L[..., level, :]. unsqueeze (-1) * level_mask(level, T) for` `level in range` `(` `int` `(np.`
```
     log2(T)) + 1)])
```

30
31
32 `def` `hattention(X, A, B, C, L, block_len =8):`
33 `"""`
34 `Arguments:`
35 `X: (batch, length, n_heads, d_head)`
36 `A: (batch, length, n_heads)`
37 `B: (batch, length, n_heads, d_state)`
38 `C: (batch, length, n_heads, d_state)`
39 `L: (batch, length, n_heads, num_levels) where` `num_levels = log2(length) + 1`
40 `Return:`
41 `Y: (batch, length, n_heads, d_head)`
42 `"""`
43 `T = X.shape [1]`
44 `assert X.dtype == A.dtype == B.dtype == C.dtype`
45 `assert X.shape [1] %`
46 `input_shape = X.shape`
47 `# Rearrange` `into` `blocks/chunks`
48 `b, cl = X.shape [0], X.shape [1]`
49 `c = cl //` `block_len`
50 `X, A, B, C, L = [x.reshape(b, c, block_len, *x.shape [2:])` `for x in (X, A, B, C, L)]`
51 `A = A.permute (0, 3, 1, 2)` `# (batch, n_heads, c, block_len)`
52 `A_cumsum = torch.cumsum(A, dim=-1)` `# (batch, n_heads, c, block_len)`
53
54 `num_intra_chunk_levels = int` `(np.log2(block_len)) + 1`
55 `num_inter_chunk_levels = int` `(np.log2(T)) + 1 - num_intra_chunk_levels`
56 `# Partition` `the` `lambda` `into intra -chunk` `and inter -chunk` `lambda`
57 `L_intra, L_inter = L[..., : num_intra_chunk_levels ], L[...,` `num_intra_chunk_levels :]`
58 `L_intra = L_intra.permute (0, 3, 1, 4, 2)` `# (batch, n_heads, num_chunks, num_levels, block_len)`
59
60 `# Intra -chunk` `Computation`
61 `H = construct_H_matrix (A, L_intra)` `# Materialize` `the H matrix as a dense` `matrix`
62 `Y_diag = torch.einsum(` `"bclhn,bcshn,bhcls,bcshp ->bclhp"` `, C, B, H, X)`
63
64 `# Inter -chunk` `Computation`
65 `decay_states = torch.exp (( A_cumsum [...,` `-1:] - A_cumsum))`
66 `states = torch.einsum(` `"bclhn,bhcl,bclhp ->bchpn"` `, B, decay_states, X)`
67 `decay_chunk = F.pad(torch.exp(segsum(A_cumsum [...,` `-1])), (0, 0, 1, 0))[..., :-1, :]`
68 `state_decay_out = torch.exp(A_cumsum)`
69
70 `def` `compute_Y_off_level (states, level):`
71 `mask = level_mask(level + 1, c).unsqueeze (0).unsqueeze (0)`
72 `decay_chunk_level = decay_chunk * mask`
73 `states = torch.einsum(` `"bhzc,bchpn ->bzhpn"` `, decay_chunk_level, states)`
74 `Y_off = torch.einsum(`
75 `"bclhn,bchpn,bhcl,bclh ->bclhp"` `,`
76 `C,`
77 `states,`
78 `state_decay_out,`
79 `L_inter [...,` `level],`
80 `)`
81 `return` `Y_off`
82
83 `Y_off = torch.zeros_like(Y_diag)`
84 `for i in range` `( num_inter_chunk_levels ):`
85 `Y_off +=` `compute_Y_off_level (states, i)`
86
87 `Y = (Y_off + Y_diag).reshape( input_shape)`
88 `return Y`


19


**Algorithm 1** Chunkwise Log-Linear Attention Algorithm


1: **for** _t ∈_ [ _T/C_ ] **do**
2: **Y** [ _t_ ] = - **Q** [ _t_ ] **K** _[⊤]_ [ _t_ ] _[⊙]_ **[M]** [ _[H]_ _t_ ]� **V** [ _t_ ]

3: **end for**
4:
5: **for** _ℓ_ _∈_ [log2 ( _T/C_ )] **do**
6: **for** _t ∈_ [ _T/C_ ] **do**



7: **Y** [ _t_ ] = **Y** [ _t_ ] + `mask` [(] **Q** _[ℓ]_ [)]




- 
**Λ** [(] [ _t_ _[ℓ]_ ] [)] _[⊙]_ **[Q]** [[] _[t]_ []] **[S]** [[] _[t]_ []]



8: **S** [ _t_ +1] = `mask` [(] **A** _[ℓ]_ [)]

9: **end for**
10: **end for**
11: **return Y**




- **A** [ _t_ ] **S** [ _t_ ]� + `mask` [(] **K** _[ℓ]_ [)]




- **K** [ _t_ ] **V** [ _[⊤]_ _t_ ]�



A naive implementation computes each level independently using a Mamba-2-style primitive, then
sums the outputs—leading to redundant memory access and kernel launches. To improve efficiency,
we fuse computation across four levels into a single Triton kernel, which we found optimal given
SRAM constraints on an H100.


For backpropagation, we unify gradient computation across all levels for _∇_ **K** and _∇_ **V** by analytically
factoring their dependencies. This reduces kernel count and improves memory efficiency, achieving
over 3× speedup compared to the naive multi-level version.


**D** **Additional Experiment Details**


For the implementation benchmarks, all experiments were conducted on an H100 GPU with a batch
size of 2, using 48 attention heads, a head dimension of 64, and a chunk size of 64. In Mamba-2-style
models, the attention heads are applied to **V** (MVA pattern), whereas in FlashAttention-2, we adopt
GQA-style attention by applying heads to **Q** . The dimensions of the **Q** and **K** states are set to 128,
aligning with common training configurations.


For the language modeling experiments, each run was performed on 8 _×_ A100 or 8 _×_ H100 GPUs over
the course of several days. We do not tie word embeddings, use a vocabulary size of 32 _,_ 000, and set
the initializer range to 0 _._ 006. Training is performed with a global batch size of approximately 524K
tokens for 95K steps (roughly 50B tokens). We use the `flash-linear-attention` and `flame`
libraries [64, 70], following most of their default configurations.


**Detailed Experimental Results.** Table 6 provide detailed results on the Needle-In-A-Haystack task.


**S-NIAH-1** **S-NIAH-2** **S-NIAH-3**
(pass-key retrieval) (number in haystack) (uuid in haystack)
**Model** 4K 8K 16K 4K 8K 16K 4K 8K 16K


**MK-NIAH-1** **MQ-NIAH** **MV-NIAH**
(multi-key line retrieval) (multi-query) (multi-value)
**Model** 4K 8K 16K 4K 8K 16K 4K 8K 16K


**Table 6:** NIAH experiments, including three single-needle tasks—S-NIAH-1 (passkey retrieval), S-NIAH-2
(numerical needle), and S-NIAH-3 (UUID-based needle)—and three multi-needle variants: MK-NIAH-1 (multikey line retrieval), MQ-NIAH (multi-query), and MV-NIAH (multi-value).


20


