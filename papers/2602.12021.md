##### **Improved State Mixing in Higher-order and Block Diagonal** **Linear Recurrent Networks**

**Igor Dubinin** [1 2] **Antonio Orvieto** [* 3 4 5] **Felix Effenberger** [* 1 2]



**Abstract**


Linear recurrent networks (LRNNs) and linear
state space models (SSMs) promise computational and memory efficiency on long-sequence
modeling tasks, yet their diagonal state transitions
limit expressivity. Dense and/or nonlinear architectures (e.g., LSTMs) on the other hand are provably more expressive, but computationally costly.
Here, we explore how expressivity in LRNNs can
be increased via richer state mixing across time
and channels while maintaining competitive efficiency. Specifically, we introduce two structured
LRNN architectures: (i) Higher-order Linear Recurrent Units (H-LRU), which generalize firstorder recurrence to _m_ -th order, mixing multiple
past states, and (ii) Block-Diagonal LRUs (BDLRU), which enable dense intra-block channel
mixing. Per-channel (H-LRU) / per-row (BDLRU) L1-normalization of selective gates stabilizes training and allows for scaling window/block
sizes. A parallel-scan implementation of the proposed architectures keeps the throughput competitive with diagonal LRNNs for moderate orders (H-LRU) and block sizes (BD-LRU). In synthetic sequence modeling tasks, the performance
of BD-LRU matches or exceeds those of linear
SSMs (Mamba), low-rank LRNNs (DeltaNet) and
LSTM baselines, while H-LRU is found to be the
most parameter-efficient in compression task. In
both synthetic sequence modeling and language
modeling, our results indicate that the structure
of state mixing rather than width alone shapes expressivity of LRNNs, offering a practical route to
closing the efficiency–expressivity gap in linear
sequence models.


1NISYS, Frankfurt am Main 2Ernst Strungmann Institut, Frank-¨
furt am Main [3] ELLIS Institute Tubingen¨ [4] MPI for Intelligent Systems [5] Tubingen AI Center. Correspondence to: Igor Dubinin¨
_<_ igor.dubinin@esi-frankfurt.de _>_ .


_Preprint. February 13, 2026._



**1. Introduction**


Recent studies have highlighted fundamental limitations
of linear recurrent networks (LRNNs) by showing that the
structure of the state-transition matrix results in a trade-off
between efficiency and expressivity (Merrill & Sabharwal,
2023; Cirone et al., 2024; Merrill et al., 2024). Architectures
based on diagonal matrices enable an efficient implementation but are inherently limited in expressive power, while
dense models are provably more expressive yet computationally prohibitive. To bridge this gap, several LRNN architectures have been proposed: efficient structured architectures
such as ones with diagonal-plus-low-rank matrices (Yang
et al., 2024a; Peng et al., 2025) and their products (Siems
et al., 2025), ones based on approximations of dense matrices at test time (Sun et al., 2024; Movahedi et al., 2025;
von Oswald et al., 2025), and other solutions that are _de_
_facto_ equivalent to block-diagonal architectures (e.g., oscillatory blocks (Rusch & Rus, 2024) and complex-valued
states (Orvieto et al., 2023; De et al., 2024)). Together,
these studies suggest that exploring the configuration space
between diagonal and dense transition matrices may yield
more expressive LRNN models.


When designing block-diagonal recurrences, the immediate
issue one faces is that of dynamical stability and forward
pass normalization – a crucial element that is well studied
and discussed in diagonal LRNNs (Orvieto et al., 2023;
Wang & Li, 2023; Zucchet & Orvieto, 2024), yet requires
additional care in non-diagonal linear architectures where
eigenvalues are not readily available. Traditionally, stability has been ensured by parameterizations that constrain
eigenvalues of the transition matrix inside the complex unit
disk (Arjovsky et al., 2016; Helfrich et al., 2018), a strategy
that effectively mitigates vanishing and exploding gradients.
More recently, similar conditions have been applied to derive efficient reparameterizations that ensure stability in diagonal linear recurrent units (Orvieto et al., 2023; De et al.,
2024). In both selective and non-selective SSMs (designed
in continuous-time), stability is achieved by exponential
parametrization, resulting from zero-order-hold discretization techniques (Gu et al., 2021; Gu & Dao, 2023). Finally,
in LRNNs with diagonal-plus-low-rank transition matrices,
normalization arises naturally from the structure of general


1


**Improved state mixing in higher-order and block diagonal linear recurrent networks**





B



H-LRU


BD-LRU


state gates input
gates




|Col1|Col2|Col3|Col4|Col5|Col6|our|s|Col9|Col10|Col11|Col12|Col13|Col14|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||||
|||||||||||||||
|||||||||||||||
|||~~o~~|~~rs~~||ours|||||||||



_Figure 1._ Structure and performance of the proposed H-LRU and BD-LRU architectures. **A.** A schematic illustration of the theoretically
predicted trade-off between expressivity and efficiency of block-diagonal linear recurrent networks. **B.** Schematic illustration of the gating
mechanisms in block-diagonal form, showing both the state gates that constitute the state-transition matrix and the input gates that act on
external inputs. The structure of the gates’ selectivity is color-coded: white squares indicate fixed zero gates, black squares indicate fixed
identity gates, other colors indicate active selective gates; similar color palettes indicates row-wise normalization. **C.** Summary of the
performance of the proposed and the baseline models. The _x_ -axis indicates the number of FLOPs per recurrent step. The _y_ -axis denotes
the mean test accuracy over all considered synthetic tasks (compression, selective copying, in context recall, permutation) of the overall
best performing model configuration (hidden size up to 6k). Optimal hidden sizes vary between models, see also Fig. 6. Note that H-LRU
and BD-LRU can achieve better or matching performance than both linear and non-linear baselines while requiring fewer FLOPs per
recurrent step. Diagonal LRU presents the best results across both H-LRU m1 and BD-LRU m1, which are identical models for _m_ = 1.
**D.** Best performance for different window sizes _m_ (H-LRU) and block sizes _m_ (BD-LRU).



ized Householder transformations (Yang et al., 2024b). Although several recent studies have examined block-diagonal
architectures, they either focus on parameterizations of nonselective models (Biegun et al., 2024; Rusch & Rus, 2024;
Walker et al., 2025), analyze only the stability of the statetransition matrix norm (Fan et al., 2023), or rely on architectures where this matrix is normalized by design (Yang et al.,
2024b), without fully addressing the problem of joint normalization of selective state-transition matrix and selective
input gates, which has been previously shown critical for
sequence modeling in diagonal LRNNs (Gu & Dao, 2023;
De et al., 2024).


Building on this line of work, we explore how to improve expressivity of LRNNs through structured selective state mixing, without compromising their computational efficiency.
Our contributions are as follows:


  - We introduce two architectures with structured selective state mixing: (i) Higher-order Linear Recurrent
Units (H-LRU), which generalize first-order recurrence to _m_ -th order, which allow for mixing multiple
past states, and (ii) Block-Diagonal LRUs (BD-LRU),
which enable dense intra-block channel mixing.

  - We propose a joint gate normalization that ensures a
normalized forward pass. This enables stable scaling
with respect to block and window sizes while preserving selectivity.

  - We provide parallel-scan implementation that maintains high throughput for moderate block sizes, preserving the efficiency that motivates linear recurrences.




  - We present empirical results on synthetic sequence
modeling and language modeling tasks showing that
state mixing structure, not width alone, shapes expressivity and efficiency trade-off.


**2. Higher-order and block diagonal linear**
**recurrent networks.**


Modern linear recurrent models (e.g., LRU, Mamba), as well
as linear attention models (e.g. GLA, DeltaNet), exchange
information between tokens by means of a recurrence


**h** _t_ = **a** _t ⊙_ **h** _t−_ 1 + **b** _t ⊙_ **v** _t,_ (1)


where **h** _t ∈_ R _[N]_ is the hidden state computed at time
_t_, and **a** _t,_ **b** _t_ are input-dependent and potentially statedependent gates prescribing how current information **v** _t_ =
**Wvxt** (pointwise function of the input **x** _t_ ) gets stored in
the network state.


Through this mechanism the output of the network at time
_t_, a function of the hidden state **h** _t_, can access information
about past inputs **v** 1 _,_ **v** 2 _, . . .,_ **v** _t_ . In fact, if **h** 0 = **0**, one can
write in closed form **h** _t_ = [�] _i_ _[t]_ =1 [(][�] _[t]_ _j_ = _t−i_ **[a]** _[j]_ [)] _[ ⊙]_ **[b]** _[i][ ⊙]_ **[v]** _[i]_ [.]
However, as is well known from both modern and classical
literature, the system above suffers from vanishing gradients
with respect to the inputs (Pascanu et al., 2013; Wang &
Li, 2023; Zucchet & Orvieto, 2024). Standard approaches
to address this issue are to re-parametrize the entries of
**a** _t_ such that their absolute values are close to a value of



2


**Improved state mixing in higher-order and block diagonal linear recurrent networks**



1 (Orvieto et al., 2023), and to increase the dimensionality
of **h** _t_ (Orvieto et al., 2024). Although it can be shown that
this strategy can help memorization (Arora et al., 2023;
Okpekpe & Orvieto, 2025), it is also known that going
beyond diagonal formulations – i.e. mixing the hidden state
as **A** _t_ **h** _t−_ 1 instead of **a** _t ⊙_ **h** _t−_ 1 = diag( **a** _t_ ) **h** _t−_ 1 – can
drastically improve performance on challenging reasoning
tasks involving state-tracking (Merrill et al., 2024; Cirone
et al., 2024; Movahedi et al., 2025).


An _orthogonal_ approach to diagonal state expansion is to
instead design recursions of _higher complexity_ . An example
in recent literature comes from (Rusch & Rus, 2024), where
the authors consider system equations given by the secondorder oscillatory ordinary differential equation **h** _[′′]_ ( _t_ ) =

_−_ **¯a** ( _t_ ) _⊙_ **h** ( _t_ )+ **b** **[¯]** ( _t_ ) _⊙_ **v** ( _t_ ). After discretization [1], this leads
to a second-order difference equation of the form


**h** _t_ = **a** 1 _,t ⊙_ **h** _t−_ 1 + **a** 2 _,t ⊙_ **h** _t−_ 2 + **a** 0 _,t ⊙_ **v** _t,_ (2)


where coefficients **a** _i,t_ are a function of the discretization
method. Notably, the model 2 can already be made more expressive if we allow arbitrary selective gates **a** 1 _,t,_ **a** 2 _,t,_ **a** 0 _,t_
in contrast to the fixed parameterization induced by discretization schemes.


**Higher-order Recurrence** Inspired by Eq. 2, we generalize Eq. 1 and introduce Higher-order Linear Recurrent
Units (H-LRUs) as follows:



where **A** _[k]_ is a structured companion-like matrix which
allows richer dynamic modes (e.g. oscillatory modes).
Though eigenvalues for **A** _[k]_ _t_ [are not available in closed form][2][,]
stability for the system above can be guaranteed and is crucial for performance, as we will discuss in the next section.


**Block Diagonal Representation.** The substitution in
Eq. 3 allows us to rewrite the system equations in H-LRU
as a generalized first-order recurrence


**h** _t_ = **A** _t ×_ **h** _t−_ 1 + **a** 0 _,t ⊙_ **v** _t,_

**A** _t_ = diag( **A** [1] _t_ _[, . . .,]_ **[ A]** _[N]_ _t_ [)] _[,]_



revealing that the H-LRU architecture corresponds to a
recurrent network with a structured block diagonal statetransition matrix.


Independently, we also investigate a second kind of recurrence with complexity higher than the diagonal case, the
block diagonal linear recurrent unit (BD-LRU). In contrast
to the structured temporal state mixing implemented inside
H-LRU blocks, BD-LRU implements dense channel mixing
inside all blocks for all vectors and matrices by setting

**h** _[k]_ _t_ [=] **[ A]** _[k][ ×]_ **[ h]** _t_ _[k]_ _−_ 1 [+] **[ a]** 0 _[k]_ _,t_ _[⊙]_ **[v]** _t_ _[k]_ (BD-LRU)















**a** [1] 0 _,t_
...
**a** _[N]_ 0 _,t_











**v** _t_ [1]
...
**v** _t_ _[N]_



(4)



**h** _t−_ 1 =



**h** [1] _t−_ 1
...
**h** _[N]_ _t−_ 1










 _,_ **a** 0 _,t_ =






 _,_ **v** _t_ =






 _,_



_m_

**h** _t_ = - **a** _i,t ⊙_ **h** _t−i_ + **a** 0 _,t ⊙_ **v** _t._ (H-LRU)


_i_ =1



**A** _[k]_ _t_ [=]


**h** _[k]_ _t−_ 1 [=]










_,_












_a_ _[k]_ 1 _,_ 1 _,t_ _· · ·_ _a_ _[k]_ 1 _,m−_ 1 _,t_ _a_ _[k]_ 1 _,m,t_
_a_ _[k]_ 2 _,_ 1 _,t_ _· · ·_ _a_ _[k]_ 2 _,m−_ 1 _,t_ _a_ _[k]_ 2 _,m,t_
... ... ... ...
_a_ _[k]_ _m,_ 1 _,t_ _· · ·_ _a_ _[k]_ _m,m−_ 1 _,t_ _a_ _[k]_ _m,m,t_



_h_ _[k]_ 1 _,t−_ 1
...
_h_ _[k]_ _m,t−_ 1







This parametrizes the state evolution by an _m_ -th order difference equation. Such models are a standard tool in time series
statistics for forecasting (ARMA processes, see e.g. Hamilton (2020)) and are _canonical_ in systems theory, since they
lead to minimal realization (i.e., with the smallest memory
size) of linear dynamical systems (Glad & Ljung, 2018).


To see the connection with controllable canonical forms
for transition matrices in systems theory, it is sufficient to
denote by _h_ _[k]_ _t−_ 1 [the] _[ k]_ [-th coordinate (] _[k][ ∈{]_ [1] _[,]_ [ 2] _[, . . ., N]_ _[}]_ [) of]
**h** _t_ and by _a_ _[k]_ _i,t_ [the] _[ k]_ [-th coordinate of] **[ a]** _[i,t]_ [. Then, with] _[ ×]_
denoting the standard matrix multiplication,

**h** _[k]_ _t_ [=] **[ A]** _t_ _[k]_ _[×]_ **[ h]** _t_ _[k]_ _−_ 1 [+] **[ a]** 0 _[k]_ _,t_ _[⊙]_ **[v]** _t_ _[k][,]_















_a_ _[k]_ 1 _,_ 0 _,t_
...
_a_ _[k]_ _m,_ 0 _,t_



_v_ 1 _[k]_ _,t_
...
_vm,t_ _[k]_






 _,_ **a** _k_ 0 _,t_ [=]






 _,_ **v** _tk_ [=]










 _._







(3)






_,_




**A** _[k]_ _t_ [=]


**h** _[k]_ _t−_ 1 [=]











_a_ _[k]_ 1 _,t_ _· · ·_ _a_ _[k]_ _m−_ 1 _,t_ _a_ _[k]_ _m,t_
1 _· · ·_ 0 0
... ... ... ...
0 _· · ·_ 1 0



_h_ _[k]_ _t−_ 1
...
_h_ _[k]_ _t−m_



As for H-LRU (Eq. 4), the block size _m_ of BD-LRU corresponds to the size of a square matrix **A** _[k]_ and _k ∈_ [1 _, N_ ]
corresponds to the block index of this matrix. The hidden
size of BD-LRU is equal to the extended block diagonal
representation of the H-LRU architecture. But in contrast
to H-LRU (Eq. 4), all vectors **a** _[k]_ 0 _[,]_ **[ h]** _t_ _[k][,]_ **[ v]** _t_ _[k]_ _[∈]_ [R] _[m]_ [ and all]
matrices **A** _[k]_ _∈_ R _[m][×][m]_ in BD-LRU are dense and there is
no dependence on the several previous hidden states that is
characteristic of the H-LRU architecture. Importantly, the
structure of BD-LRU does not allow for the same eigenvalue
analysis as is possible for H-LRU. Yet, as we show in the
next section, we can guarantee its dynamical stability using
a normalization technique similar to that of H-LRU.


To endow the models with input selectivity, we introduce
input-dependent gates for both H-LRU ( _a_ _[′]_ _j,t_ [=] _[ Linear][j]_ [(] **[x]** _[t]_ [)][)]


2 _k_ _m_
Solve the equation _χ_ **A** _k_ ( _λ_ ) = det( _λI −_ **A** ) = _λ_ _−_
_a_ _[k]_ 1 _,t_ _[λ][m][−]_ [1] _[ −]_ _[a][k]_ 2 _,t_ _[λ][m][−]_ [2] _[ −· · · −]_ _[a][k]_ _m−_ 1 _,t_ _[λ][ −]_ _[a][k]_ _m,t_ [= 0][.]















_a_ _[k]_ 0 _,t_
...
0











_vt_ _[k]_
...
0






 _,_ **a** _k_ 0 _,t_ [=]






 _,_ **v** _tk_ [=]






 _,_



1Plugging in the second-order backward estimate **h** _′′_ ( _t_ )∆2 _≃_
**h** _t −_ 2 **h** _t−_ 1 + **h** _t−_ 2 (Hairer et al., 1993).



3


**Improved state mixing in higher-order and block diagonal linear recurrent networks**


_Figure 2._ Scaling of performance with window/block size on the compression task for L1
normalization with different parameterizations.
Results are shown for different window/block
sizes _m_ of the higher-order LRU (H-LRU) and
block diagonal LRU (BD-LRU). **A** . Comparison
between H-LRUs. **B** . Comparison between BDLRUs.


|A|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||



and BD-LRU ( _a_ _[′]_ _i,j,t_ [=] _[ Linear][i,j]_ [(] **[x]** _[t]_ [)][). Gate selectivity]
plays a critical role in our synthetic task experiments. An
ablation study investigating non-selective model variants
is presented in Appendix F. Fig. 1 provides a schematic
illustration of the proposed selective gating mechanisms in
block-diagonal form, showing both the state gates that form
the state-transition matrix and the input gates applied to
external inputs.


**3. Normalization**


Normalization schemes for RNNs which impose restrictions
on the eigenvalues of the state-transition matrix have proven
to be very effective as they directly address the vanishing
and exploding gradient problem (Pascanu et al., 2013). This
approach has led to the development of a variety of models
with restrictions on the norm of the state-transition matrix
(Arjovsky et al., 2016; Helfrich et al., 2018). More recently,
similar normalization techniques were applied to exponentiated gates in linear recurrent units (LRU, (Orvieto et al.,
2023)) and optimized discretization schemes in state space
models (SSM, (Gu et al., 2021)). However, as detailed
in Orvieto et al. (2023), stability in a dynamical systems
sense (i.e., requiring that the eigenvalues of the hidden-tohidden transition be less than one in absolute value) does
not necessarily guarantee a properly normalized forward
pass in this case. This can negatively affect performance, as
discussed in the next section.


To understand this phenomenon, one can consider the trivial
one-dimensional linear setting _ht_ = _aht−_ 1 + _bxt_, where
_xt_ = 1 for all _t_ . For _a ∈_ (0 _,_ 1), as _t →∞_, _ht_ converges
to the value (1 _−_ _a_ ) _[−]_ [1] _b_, which can be substantially greater
than 1 if _a_ gets close to 1, as allowed and incentivized by
recent sigmoidal parametrizations (Orvieto et al., 2023). Of
course, the forward-pass norm in this case is preserved if
input and forget gates are adapted, that is, if we consider
RNNs of the form _ht_ = _aht−_ 1 + (1 _−_ _a_ ) _xt_, i.e., _b_ = 1 _−_ _a_ .
This directly translates to the case of a diagonal network
where models such as S4 (Gu et al., 2020) and Mamba (Gu
& Dao, 2023) adopt a forget gate of the form _a_ = _e_ _[−]_ [∆],
coupled with an input gate _b_ = ∆ _≈_ (1 _−_ _a_ ) if ∆ is close



to zero. As suggested also directly from the original GRU
formulation (Cho et al., 2014) as well as recent works (Feng
et al., 2024), for the diagonal setting (coinciding with _m_ = 1
in H-LRU and BD-LRU) it is convenient to start by adapting
Eq. 1 to **h** _t_ = **a** _t_ _⊙_ **h** _t−_ 1 +(1 _−_ **a** _t_ ) _⊙_ **v** _t_ . Stability for _m ≥_ 1
is guaranteed when choosing coefficients as prescribed by
the next proposition.


**Proposition 1** _Consider either the H-LRU or the BD-LRU_
_architectures, written in matrix form as shown in Equa-_
_tions 3 and BD-LRU. If for any k ∈_ [1 _, N_ ] _, the k-th recur-_
_rent non-diagonal block_ **h** _[k]_ _t_ [=] **[ A]** _t_ _[k]_ _[×]_ **[ h]** _t_ _[k]_ _−_ 1 [+] **[ a]** 0 _[k]_ _,t_ _[⊙]_ **[v]** _t_ _[k]_ _[is]_
_such that the matrix A_ _[k]_ _t_ [:= [] **[A]** _t_ _[k][,]_ **[ a]** 0 _[k]_ _,t_ []] _[ ∈]_ [R] _[m][×]_ [(] _[m]_ [+1)] _[ has the]_
_property that_ [�] _j_ _[m]_ =1 [+1] _[|]_ [(] _[A]_ _t_ _[k]_ [)] _[i,j][| ≤]_ [1] _[ for every row][ i][ ∈]_ [[1] _[, m]_ []] _[,]_
_then the recurrence is stable from a dynamical systems per-_
_spective and the forward pass is normalized, meaning that_
_∥_ **h** _T ∥∞_ _≤_ max _t∈_ [0 _,T_ ] _∥_ **v** _t∥∞._


The proposition above suggests that to achieve a normalized
forward pass, L1-normalization should be applied to raw
selective gates. For H-LRU, it is sufficient to normalize
over all _m_ + 1 coefficients of the _m_ -th order recurrence,
while for BD-LRU, we apply a row-wise normalization over
the hidden state gates and the input gate. Let us therefore
denote as _a_ _[′]_ s the raw gates (linear functions of the input)
before normalization. We set


_f_ ( _a_ _[′]_ _j,t_ [)]
**H-LRU:** _aj,t_ = ~~�~~ ~~_m_~~
_l_ =0 _[f]_ [(] _[a]_ _l,t_ _[′]_ [);]

(5)
_f_ ( _a_ _[′]_ _i,j,t_ [)]
**BD-LRU:** _ai,j,t_ = ~~�~~ ~~_m_~~
_l_ =0 _[f]_ [(] _[a]_ _i,l,t_ _[′]_ [)] _[,]_


where _f_ ( _·_ ) is a gate parametrization function; the block
index is omitted for clarity. Note that this normalization
only affects the elements inside on-diagonal blocks and
has no impact on off-diagonal blocks (consisting of zero
matrices). Note that the introduced normalization restricts
eigenvalues of the state-transition matrix to be smaller than
the _L_ 1 norm of the corresponding row, meaning that the
eigenvalues of the state-transition matrix are limited by a



4


**Improved state mixing in higher-order and block diagonal linear recurrent networks**



value of the input gate



Models Recall Copy Compress Overall


LSTM **1.000** **1.000** 0.750 0.916
Mamba2 **1.000** 0.807 0.720 0.842
Deltanet[-1,1] **1.000** 0.892 0.782 0.892
Deltaproduct4[-1,1] **1.000** **1.000** 0.717 0.906


BD-LRU m1 (ours) 0.775 0.835 0.725 0.778
BD-LRU m2 **1.000** 0.962 0.760 0.908
BD-LRU m3 **1.000** 0.980 0.762 0.916
BD-LRU m5 **1.000** 0.985 0.782 **0.922**
BD-LRU m8 **1.000** 0.992 0.748 0.913


H-LRU m1 (ours) 0.785 0.848 0.760 0.797
H-LRU m2 0.998 0.855 0.770 0.874
H-LRU m3 **1.000** 0.855 0.772 0.876
H-LRU m5 **1.000** 0.838 0.775 0.871
H-LRU m8 **1.000** 0.810 0.768 0.859


_Table 1._ Performance on the in-context recall, selective copying
and compression tasks. The presented results are the average of
best test accuracies across four configurations of the corresponding synthetic dataset with different vocabulary sizes, sequence
lengths and number of training examples. Results are shown for
different window (H-LRU) and block sizes (BD-LRU) _m_ . Note
that overall performance of our models consistently improves with
window/block size up to approximately 3–5, after which the gains
saturate or exhibit slight degradation. All models are single-layer
configurations with a maximum overall hidden dimension of 6144.
See Appendix B for extended table.


tions allowed the models to effectively scale with window
and block size. Without normalization and with the ReLU
normalization, both H-LRU and BD-LRU improve at lower
rate with window size. With softmax or sigmoidal L1 normalizations, the improvement with window size was especially pronounced between a window/block size of 1 and
2. Our eigenvalue analysis (see Fig.3 and Appendix I) indicates that this gain corresponds to the emergence of negative
eigenvalues, consistent with the findings of (Grazzi et al.,
2024). We also observe that further improvements in performance are associated with a broader range of complex
eigenvalues, which are enabled starting from the block size
3. These results also align well with previous studies on
beneficial role of oscillatory dynamics in recurrent networks
(Rusch & Mishra, 2021; Effenberger et al., 2022; Dubinin
& Effenberger, 2024; Rusch & Rus, 2024).


We noticed that for moderate block sizes ( _m ∈_ [2 _,_ 5]), the
softmax normalization performed comparable or better than
sigmoidal normalization, making this the default choice
for all the remaining experiments. That also agrees with
previous findings that exponentiation of the gates benefits
gradient descent (Orvieto et al., 2023; Zhang et al., 2024).


**Scaling with hidden state is limited by state mixing.**
Next, we performed experiments in which we investigated
the difference between scaling the window size and the
hidden size. In these experiments we found that for both



_|λi,t| ≤_



_m_

- _|ai,l,t|_ = 1 _−|ai,_ 0 _,t|,_ (6)


_l_ =1



where _i_ is the channel index in H-LRU or row index in BDLRU. This results in a joint normalization for input and state
gates that allows selective block-diagonal LRNNs to balance
attention to hidden states and inputs in a similar way as in
first-order non-selective and selective LRUs (Orvieto et al.,
2023; De et al., 2024). This is in contrast to previous studies
on selective block-diagonal LRNNs that only addressed the
stability of the state-transition matrix (Fan et al., 2023).


Although the introduced normalization guarantees the stability of the recurrence, it has been shown that gradient-based
learning is also highly sensitive to the specific choice of
parametrization (Zucchet & Orvieto, 2024). In contrast
to the normalization used in non-selective block-diagonal
LRNNs that rely on structured parameterizations such as
discretization schemes (Rusch & Rus, 2024; Walker et al.,
2025), joint parametrization of the state-transition matrices and input gate (Biegun et al., 2024), and exponential
reparametrization (Orvieto et al., 2023), our proposed normalization is more general as it can be applied to variety
of both non-selective and selective parameterizations. This
allowed us to independently evaluate several variants of gate
parameterizations that are defined by the function _f_ in Eq. 5.
As can be seen in Fig. 2, _our normalization strategy greatly_
_improves performance_ of both H-LRUs and BD-LRUs.


**4. Experiments on token manipulation tasks**


While sequence modeling is often evaluated through largescale training, recent work demonstrates that critical capabilities can be effectively assessed via targeted synthetic benchmarks (Arora et al., 2023; Poli et al., 2024). Leveraging the
established equivalence between lossless compression and
generalization (Shannon, 1948; Deletang et al., 2023; Gu,´
2025), we first evaluate temporal information integration
using the auto-encoding compression task. Furthermore, to
assess the dynamic adaptation required for general sequence
modeling, we employ selective copying and associative recall; these tasks serve as robust indicators of the in-context
learning abilities (Poli et al., 2024; Olsson et al., 2022; Waleffe et al., 2024).


**Normalization allows scaling with window size.** In experiments on synthetic tasks, we show that our normalization strategies are crucial for performance, see Fig. 2. We
tested several variants of the function _f_ for L1 normalization
in 5: exponentiated gate _exp_ ( _·_ ) (softmax normalization),
sigmoidal gates _σ_ ( _·_ ), ReLU gates _relu_ ( _·_ ). As a baseline, we
also tested all models without normalization.


We found that both softmax and sigmoidal L1 normaliza


5


BD-LRU m1 0.560 0.340 0.210
BD-LRU m2 **1.000** 0.700 0.340
BD-LRU m3 **1.000** **1.000** 0.480
BD-LRU m4 **1.000** **1.000** 0.880
BD-LRU m5 **1.000** **1.000** **1.000**



_Figure 3._ Eigenvalue spectra of the transition matrices learned by

starting from _m_ = 2 and complex eigenvalues from _m_ = 3. Other
configurations are reported in Appendix I.


window/block size was found to be the key factor for perfor
and BD-LRUs results in models that are competitive with
LSTMs and achieve higher performance than other linear
recurrent baselines, both diagonal ones such as Mamba and
low-rank ones such as Deltanet and Deltaproduct, see Table

also had limited effect on performance, see Fig. 6. Notably,
we also found distinct scaling behaviors for the compression
and our other tasks, aligning with previous results (Deletang´
et al., 2023). In the compression (auto-encoding) task, models with smaller block size outperformed larger counterparts,
while performance on autoregressive tasks scaled positively
with block size. Therefore, the decrease in aggregate performance for larger block sizes is substantially driven by the
results on the compression task.


Our experiments show a direct trade-off between parameter
efficiency and peak performance, as governed by the block
and window sizes for BD-LRU and H-LRU, respectively.
Models with smaller block/window sizes saturate in performance at lower parameter counts, demonstrating high
efficiency. In contrast, models with larger block/window
sizes require a larger hidden dimension to match the performance of the smaller models, but can ultimately achieve a
much higher performance. This indicates that richer state
mixing increases a model’s expressive power at the expense
of parameter efficiency.


**H-LRUs are parameter efficient.** We also found that in
the compression task which does not require complex token
manipulation, H-LRU demonstrated the most parameter efficient scaling with hidden size, achieving accuracies not
accessible to Mamba and LSTM of similar sizes (in terms
of the number of trainable parameters), see Fig. 6. This
aligns well with our predictions that the inductive bias introduced by extended temporal mixing results in hidden
representations with better compression capabilities.



_Table 2._ Model performance on permutation composition tasks.
We note that BD-LRU performance improves with block size _m_,

given limited training data. See Appendix B for extended table.


**BD-LRUs are expressive across tasks.** In contrast to the
compression task, the selective copying task requires more
extensive token manipulation. We found that the perfor
than the one of H-LRUs. Furthermore, BD-LRUs were able

that is competitive with LSTMs and Deltaproduct. At the
same time, BD-LRUs achieved the best performance also in
the compression task. Overall, the introduced normalization
scheme allows BD-LRU to efficiently utilize the expressivity of their dense block diagonal structure to approximate

results on our set of synthetic tasks.


**5. Experiments on permutation tasks**


An important property of dense recurrent networks is that
one layer of such model can easily solve inherently sequential tasks such as permutation composition. In theory, linear
diagonal networks and Transformers can also solve any of
these tasks, but only if we assume an infinite depth approximation. In practice, it has been shown that they cannot
effectively approximate the evolution of recurrent state with
a bounded number of layers (Merrill et al., 2024). Further, it
was proposed that there is a parallelism-expressivity tradeoff, in which efficient parallelization comes at the expense
of decreased expressivity (Merrill & Sabharwal, 2023).


To evaluate the ability of a model to learn a permutation
structure from data, we use a synthetic dataset based on the
symmetric group _Sn_ - the group of all permutations over _n_
elements (Merrill et al., 2024). Each instance in the dataset
corresponds to a specific permutation sampled from _Sn_, and
the model is tasked with learning the mapping that defines
the permutation purely from input-output examples within
a sequence. We evaluated model performance on a series
of increasingly complex permutation learning tasks derived
from the symmetric groups _S_ 2 through _S_ 5.


**BD-LRUs efficiently learn permutations.** All tested recurrent architectures (H-LRU, BD-LRU, LSTM, Deltanet,
Deltaproduct) were able to perfectly solve the _S_ 2 task, which
represents a uniquely simple permutation group as it is also
a commutative cyclic group. However, as the group order



6


**Improved state mixing in higher-order and block diagonal linear recurrent networks**


_Figure 4._ Language modeling results on FineWeb. **A.** Best achieved perplexity for 210M parameter BD-LRU models (trained on 10B
tokens) across varying learning rates. **B.** Performance scaling of BD-LRU models on 2.5B tokens with varying hidden dimensions. Results
indicate that moderate block sizes provide a superior inductive bias. **C.** Runtime and perplexity comparison for 140M parameter models.
While H-LRUs are parameter-efficient, matching the parameter budget of a BD-LRU requires increasing the H-LRU hidden dimension by
a factor of m, making them substantially more costly to scale.



increases over _S_ 3 to _S_ 5, the non-commutative structure of
the permutation tasks increasingly posed challenges for the
models, see Table 2 and Table 4. Performance of the H-LRU
was found to decrease progressively with increasing group
size, indicating a limited capacity for modeling compositional permutations. Increasing the order of recurrence _m_
did not seem to provide any benefits for the performance.
We conclude that a strict inductive bias on the structure
of the transition matrix prevents H-LRU from solving this
task. Moreover, we found that H-LRU is unable to solve
our permutation tasks despite having access to negative and
complex eigenvalues (see Appendix I for our eigenvalue
analysis). This indicates that the presence of such eigenvalues is insufficient for these tasks and highlights that the
structure of state mixing plays a more critical role.


In contrast, BD-LRU with moderate block sizes was able to
successfully solve all permutation tasks for all group sizes,
matching the performance of LSTM and outperforming all
other recurrent architectures tested. Notably, BD-LRU with
block size 5 solved the _S_ 5 task using as few as 200K parameters, matching the parameter efficiency of more computationally demanding non-linear LSTM model. Furthermore,
we found that BD-LRUs are also sample-efficient in learning permutations, outperforming even LSTM in the regime
of limited training data. We notice that in our low training token regime Deltaproduct4 fails to learn the _S_ 5 dataset.
However, when the number of training samples approaches
the token counts used in the study (Siems et al., 2025), it
is capable of solving _S_ 5 task, showing that low-rank matrices are less sample-efficient compared to BD-LRU. Our
findings align well with our predictions that dense blocks
of BD-LRU are well-suited for implementing permutations
between hidden states. The consistent improvement with
larger block sizes on permutation tasks of increasing complexity highlights the advantage of the inductive bias of
BD-LRU architecture.



**6. Experiments on language modeling**


Our language modeling experiments with BD-LRU and
H-LRU further corroborate the findings from our synthetic task evaluations, see Fig. 4. When trained on
the FineWeb dataset, BD-LRU with moderate block sizes
achieves the lowest perplexity among parameter-matched
baselines (210M parameters trained for 10B tokens; see
Fig. 4A). Architectures with block sizes of 2, 4 and 8 outperform diagonal networks, while models with 16 block sizes,
despite being theoretically more expressive, underperform
in practice.


By varying the hidden size of BD-LRU, we obtain models
in the 140M–210M parameter range, see Fig. 4B. BD-LRU
with moderate block sizes effectively scale with parameter
numbers whereas diagonal models ( _m_ = 1) show early
saturation with increasing hidden size. Overall, these results
indicate that moderate block sizes provide a more effective
inductive bias for scaling of language models, in line with
our observations on synthetic tasks


We also conducted language-modeling experiments with
H-LRU using configurations matched in parameter count to
their BD-LRU counterparts, see Fig. 4C for 140M parameters. Consistent with our synthetic benchmarks, H-LRU exhibits stronger parameter efficiency. However, to match the
parameter budget of a BD-LRU, H-LRU requires increasing
hidden dimension by a factor of _m_, which in turn reduces
throughput and increases memory consumption by approximately the same factor, see Sec. 7 and Fig. 4C. Therefore,
although H-LRU is more parameter-efficient, it is substantially more computationally demanding to scale compared
to BD-LRU.



7


**Improved state mixing in higher-order and block diagonal linear recurrent networks**


_Figure 5._ Model throughput on the selective copying task. ( **A** ) Comparison of sequential, higher-order parallel, and autotuned higher-order
parallel implementations of BD-LRUs with 128 blocks and with a sequence length of 2048, illustrating advantage of parallel scan
implementation and the trade-off between expressivity and efficiency. BD-LRU is shown for illustration purposes only, but H-LRU
employs the same parallel scan implementation. ( **B** ) Comparison for layers with hidden size of 768 and accordingly adjusted number
of blocks. Note that trade-off between expressivity and efficiency increases over longer sequences. ( **C** ) Throughput comparison of
parameter-matched layers ( _∼_ 33M parameters). Number of blocks is adjusted to ensure consistent model sizes across architectures.
BD-LRU achieves throughput competitive with other LRNN baselines. Notably, larger block sizes demonstrate higher practical efficiency
despite increased theoretical complexity, due to superior utilization of GPU hardware operations.



**7. Implementation**


The parallel scan algorithm in LRNNs allows them to efficiently process long sequences using constant memory and
with logarithmic time complexity. Following the classic
approach (Blelloch, 1990), we consider a recurrence of the
form



for moderate block sizes with _m_ [2] _≪_ _N_ we can achieve a
significant increase in throughput in the parallel scan implementation compared to sequential implementation.


**Parallel scan implementation enables competitive**
**throughput.** In experiments with single-layer models
containing 128 blocks and trained on sequences of length
2048, when runtime is less influenced by GPU characteristics and more reflective of algorithmic complexity, we
found that increasing block size reduces throughput, revealing the predicted trade-off between expressivity and
efficiency, see Fig. 5A. For comparison, we also evaluated
models with a fixed hidden size of 768 and adjusted the
number of blocks accordingly, see 5B. We found that the expressivity–efficiency trade-off becomes more pronounced as
sequence length increases. In particular, block sizes larger
than 16 exhibit a substantial decline in throughput at longer
sequence lengths.

_,_

We also tested larger parameter-matched layers ( _∼_ 33M parameters), where number of blocks is adjusted to ensure
consistent model sizes across architectures, see Fig. 5C.
We note that our most efficient implementation relies on
compilation with maximal autotuning; thus, at such scale,
the performance differences across block sizes primarily
reflect kernel optimization in PyTorch and achieved GPU
utilization. We found that certain block sizes align more favorably with GPU architectures. In particular, we found that
moderately large block sizes ( _m_ = 16) demonstrate higher
practical efficiency despite increased theoretical complexity,
due to superior utilization of GPU hardware operations.


Overall, we observed that our parallel scan implementation
offers substantial improvements over sequential implementations, enables BD-LRUs and H-LRUs to achieve throughput
comparable to the one of linear baselines, and effectively
scales with sequence length.



**h** _i_ +1 =




**b** 0 _,_ if _i_ = 0

(7)

( **h** _i_  - _v_ **[A]** _[i]_ [)][ �] **[b]** _[i][,]_ if 0 _≤_ _i < n_ _[,]_



where **h** _i,_ **b** _i ∈_ R _[N]_ _,_ **A** _i ∈_ R _[N]_ _[×][N]_ and associative operators:


_v_ [is matrix-vector multiplication,][ �] _M_ [is matrix-matrix]

multiplication and [�] point-wise vector summation.

Defining following associative operator _•_ and making substitution to sequence of pairs,




   _M_ **[c]** _[j,A][,]_ [ (] **[c]** _[i,b]_



_HOP_ =




**c** _i_ = [ **A** _i,_ **b** _i_ ]
**c** _i •_ **c** _j ≡_ [ **c** _i,A_  


_,_
_v_ **[c]** _[j,A]_ [)][ �] **[c]** _[j,b]_ []]



(8)
reduces recurrence 7 to classic prefix sum and allows application of up and down sweeps of the Blelloch scan.



In many modern LRNNs, **A** _i_ is diagonal ( **c** _i,A_ 


In many modern LRNNs, **A** _i_ is diagonal ( **c** _i,A_ - _M_ **[c]** _[j,A][ ∼]_

_N_ ), therefore parallel scan 8 enables efficient parallel processing by reducing the time complexity from _NT_ to
_N_ log( _T_ ). However, in more general case presented in
Eq. 7, parallel scan changes the time complexity from
_N_ [2] _T_ to _N_ [3] log( _T_ ). For large dense matrices **A** _i_ and/or
short sequences, this change in complexity is not beneficial
due to the high complexity of matrix-matrix multiplication
( **c** _i,A_ - **[c]** _[j,A][ ∼]_ _[N]_ [ 3][). However, if we exploit the block]



( **c** _i,A_ - _M_ **[c]** _[j,A][ ∼]_ _[N]_ [ 3][). However, if we exploit the block]

diagonal structure of the transition matrices in H-LRU and
BD-LRU, we can reduce the time complexity of parallel
scan from _N_ [3] log( _T_ ) to _Hm_ [3] log( _T_ ), where _m_ is the block
size and _H_ is the number of blocks ( _Hm_ = _N_ ). Therefore,



8


**Improved state mixing in higher-order and block diagonal linear recurrent networks**



**Acknowledgments**


We would like to thank Wolf Singer, Sajad Movahedi and
Felix Sarnthein for the helpful discussions. Antonio Orvieto
acknowledges financial support from the Hector Foundation and the AI2050 Early Career Fellowship from Schmidt
Sciences.


**References**


Ajroldi, N. plainlm: Language model pretraining in pytorch,
2024.


Arjovsky, M., Shah, A., and Bengio, Y. Unitary evolution
recurrent neural networks. In _International Conference_
_on Machine Learning_, pp. 1120–1128. PMLR, 2016.


Arora, S., Eyuboglu, S., Timalsina, A., Johnson, I., Poli,
M., Zou, J., Rudra, A., and Re, C. Zoology: Measuring´
and improving recall in efficient language models. _arXiv_
_preprint arXiv:2312.04927_, 2023.


Biegun, K., Dolga, R., Cunningham, J., and Barber, D.
Rotrnn: Modelling long sequences with rotations. _arXiv_
_preprint arXiv:2407.07239_, 2024.


Blelloch, G. E. Prefix sums and their applications. 1990.


Chang, Y. and Bisk, Y. Language models need inductive biases to count inductively. _arXiv preprint_
_arXiv:2405.20131_, 2024.


Cho, K., Van Merrienboer, B., Gulcehre, C., Bahdanau,¨
D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. _arXiv preprint_
_arXiv:1406.1078_, 2014.


Chomsky, N. Three models for the description of language.
_IRE Transactions on information theory_, 2(3):113–124,
1956.


Cirone, N. M., Orvieto, A., Walker, B., Salvi, C., and Lyons,
T. Theoretical foundations of deep selective state-space
models. _arXiv preprint arXiv:2402.19047_, 2024.


Dao, T. and Gu, A. Transformers are ssms: Generalized
models and efficient algorithms through structured state
space duality. _arXiv preprint arXiv:2405.21060_, 2024.


De, S., Smith, S. L., Fernando, A., Botev, A., CristianMuraru, G., Gu, A., Haroun, R., Berrada, L., Chen, Y.,
Srinivasan, S., et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models.
_arXiv preprint arXiv:2402.19427_, 2024.


Deletang, G., Ruoss, A., Grau-Moya, J., Genewein, T., Wen-´
liang, L. K., Catt, E., Cundy, C., Hutter, M., Legg, S.,
Veness, J., et al. Neural networks and the chomsky hierarchy. _arXiv preprint arXiv:2207.02098_, 2022.



Deletang, G., Ruoss, A., Duquenne, P.-A., Catt, E., Ge-´
newein, T., Mattern, C., Grau-Moya, J., Wenliang, L. K.,
Aitchison, M., Orseau, L., et al. Language modeling is
compression. _arXiv preprint arXiv:2309.10668_, 2023.


Dubinin, I. and Effenberger, F. Fading memory as inductive
bias in residual recurrent networks. _Neural networks_, 173:
106179, 2024.


Effenberger, F., Carvalho, P., Dubinin, I., and Singer, W. A
biology-inspired recurrent oscillator network for computations in high-dimensional state space. _BioRxiv_, 2022.


Fan, T.-H., Chi, T.-C., and Rudnicky, A. I. Advancing regular language reasoning in linear recurrent neural networks.
_arXiv preprint arXiv:2309.07412_, 2023.


Feng, L., Tung, F., Ahmed, M. O., Bengio, Y., and Hajimirsadeghi, H. Were rnns all we needed? _arXiv preprint_
_arXiv:2410.01201_, 2024.


Glad, T. and Ljung, L. _Control theory_ . CRC press, 2018.


Grazzi, R., Siems, J., Zela, A., Franke, J. K., Hutter, F., and
Pontil, M. Unlocking state-tracking in linear rnns through
negative eigenvalues. _arXiv preprint arXiv:2411.12537_,
2024.


Gromov, A. Grokking modular arithmetic. _arXiv preprint_
_arXiv:2301.02679_, 2023.


Gu, A. On the tradeoffs of state space models and transformers, 2025. [URL https://goombalab.github.](https://goombalab.github.io/blog/2025/tradeoffs/)
[io/blog/2025/tradeoffs/.](https://goombalab.github.io/blog/2025/tradeoffs/)


Gu, A. and Dao, T. Mamba: Linear-time sequence
modeling with selective state spaces. _arXiv preprint_
_arXiv:2312.00752_, 2023.


Gu, A., Gulcehre, C., Paine, T., Hoffman, M., and Pascanu,
R. Improving the gating mechanism of recurrent neural networks. In _International Conference on Machine_
_Learning_, pp. 3800–3809. PMLR, 2020.


Gu, A., Goel, K., and Re, C. Efficiently modeling long´
sequences with structured state spaces. _arXiv preprint_
_arXiv:2111.00396_, 2021.


Hairer, E., Wanner, G., and Nørsett, S. P. _Solving ordi-_
_nary differential equations I: Nonstiff problems_ . Springer,
1993.


Hamilton, J. D. _Time series analysis_ . Princeton university
press, 2020.


Helfrich, K., Willmott, D., and Ye, Q. Orthogonal recurrent
neural networks with scaled cayley transform. In _Interna-_
_tional Conference on Machine Learning_, pp. 1969–1978.
PMLR, 2018.



9


**Improved state mixing in higher-order and block diagonal linear recurrent networks**



Hutter, M. _Universal artificial intelligence: Sequential_
_decisions based on algorithmic probability_ . Springer
Science & Business Media, 2005.


Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and
Amodei, D. Scaling laws for neural language models.
_arXiv preprint arXiv:2001.08361_, 2020.


Loshchilov, I. and Hutter, F. Sgdr: Stochastic gradient descent with warm restarts. _arXiv preprint_
_arXiv:1608.03983_, 2016.


Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.


Merrill, W. and Sabharwal, A. The parallelism tradeoff:
Limitations of log-precision transformers. _Transactions_
_of the Association for Computational Linguistics_, 11:531–
545, 2023.


Merrill, W., Petty, J., and Sabharwal, A. The illusion of state
in state-space models. _arXiv preprint arXiv:2404.08819_,
2024.


Movahedi, S., Sarnthein, F., Cirone, N. M., and Orvieto,
A. Fixed-point rnns: From diagonal to dense in a few
iterations. _arXiv preprint arXiv:2503.10799_, 2025.


Okpekpe, D. and Orvieto, A. When recalling incontext, transformers are not ssms. _arXiv preprint_
_arXiv:2508.19029_, 2025.


Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma,
N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen,
A., et al. In-context learning and induction heads. _arXiv_
_preprint arXiv:2209.11895_, 2022.


Orvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre,
C., Pascanu, R., and De, S. Resurrecting recurrent neural
networks for long sequences. In _International Conference_
_on Machine Learning_, pp. 26670–26698. PMLR, 2023.


Orvieto, A., De, S., Gulcehre, C., Pascanu, R., and Smith,
S. L. Universality of linear recurrences followed by nonlinear projections: Finite-width guarantees and benefits
of complex eigenvalues. In _International Conference on_
_Machine Learning_, pp. 38837–38863. PMLR, 2024.


Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty
of training recurrent neural networks. _International con-_
_ference on machine learning_, pp. 1310–1318, 2013.


Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., et al. Pytorch: An imperative style, high-performance
deep learning library. _Advances in neural information_
_processing systems_, 32, 2019.



Penedo, G., Kydl´ıcek, H., Lozhkov, A., Mitchell, M., Raffel,ˇ
C. A., Von Werra, L., Wolf, T., et al. The fineweb datasets:
Decanting the web for the finest text data at scale. _Ad-_
_vances in Neural Information Processing Systems_, 37:
30811–30849, 2024.


Peng, B., Zhang, R., Goldstein, D., Alcaide, E., Du, X.,
Hou, H., Lin, J., Liu, J., Lu, J., Merrill, W., et al. Rwkv-7”
goose” with expressive dynamic state evolution. _arXiv_
_preprint arXiv:2503.14456_, 2025.


Poli, M., Thomas, A. W., Nguyen, E., Ponnusamy, P., Deiseroth, B., Kersting, K., Suzuki, T., Hie, B., Ermon, S.,
Re, C., et al. Mechanistic design and scaling of hybrid´
architectures. _arXiv preprint arXiv:2403.17844_, 2024.


Rusch, T. K. and Mishra, S. Coupled Oscillatory Recurrent
Neural Network (coRNN): An accurate and (gradient)
stable architecture for learning long time dependencies.
_arXiv:2010.00951 [cs, stat]_, March 2021.


Rusch, T. K. and Rus, D. Oscillatory state-space models.
_arXiv preprint arXiv:2410.03943_, 2024.


Shannon, C. E. A mathematical theory of communication.
_The Bell system technical journal_, 27(3):379–423, 1948.


Siems, J., Carstensen, T., Zela, A., Hutter, F., Pontil, M.,
and Grazzi, R. Deltaproduct: Improving state-tracking
in linear rnns via householder products. _arXiv preprint_
_arXiv:2502.10297_, 2025.


Sun, Y., Li, X., Dalal, K., Xu, J., Vikram, A., Zhang, G.,
Dubois, Y., Chen, X., Wang, X., Koyejo, S., et al. Learning to (learn at test time): Rnns with expressive hidden
states. _arXiv preprint arXiv:2407.04620_, 2024.


von Oswald, J., Scherrer, N., Kobayashi, S., Versari, L.,
Yang, S., Schlegel, M., Maile, K., Schimpf, Y., Sieberling, O., Meulemans, A., et al. Mesanet: Sequence modeling by locally optimal test-time training. _arXiv preprint_
_arXiv:2506.05233_, 2025.


Waleffe, R., Byeon, W., Riach, D., Norick, B., Korthikanti, V., Dao, T., Gu, A., Hatamizadeh, A., Singh,
S., Narayanan, D., et al. An empirical study of mambabased language models. _arXiv preprint arXiv:2406.07887_,
2024.


Walker, B., Yang, L., Cirone, N. M., Salvi, C., and
Lyons, T. Structured linear cdes: Maximally expressive and parallel-in-time sequence models. _arXiv preprint_
_arXiv:2505.17761_, 2025.


Wang, S. and Li, Q. Stablessm: Alleviating the curse of
memory in state-space models through stable reparameterization. _arXiv preprint arXiv:2311.14495_, 2023.



10


**Improved state mixing in higher-order and block diagonal linear recurrent networks**


Yang, S., Kautz, J., and Hatamizadeh, A. Gated delta networks: Improving mamba2 with delta rule. _arXiv preprint_
_arXiv:2412.06464_, 2024a.


Yang, S., Wang, B., Zhang, Y., Shen, Y., and Kim, Y. Parallelizing linear transformers with the delta rule over sequence length. _arXiv preprint arXiv:2406.06484_, 2024b.


Zhang, M., Bhatia, K., Kumbong, H., and Re, C. The´
hedgehog & the porcupine: Expressive linear attentions
with softmax mimicry. _arXiv preprint arXiv:2402.04347_,
2024.


Zucchet, N. and Orvieto, A. Recurrent neural networks:
vanishing and exploding gradients are not the end of
the story. _Advances in Neural Information Processing_
_Systems_, 37:139402–139443, 2024.


11


**Improved state mixing in higher-order and block diagonal linear recurrent networks**


**A. Conclusion and outlook**


We introduced H-LRU and BD-LRU as structured extensions of linear recurrent models that enhance temporal and channelwise state mixing. Our results show that proper gate normalization is essential for scaling such models with window/block
size, that H-LRU excels at parameter-efficient compression, while BD-LRU is overall the best-performing architecture on
our benchmark of synthetic tasks. We also provide our parallel-scan implementation that can maintain competitive efficiency
of block diagonal architectures. Taken together, our empirical results indicate that the state-mixing structure, rather than
width alone, acts as an important driver for improved expressivity in LRNNs.


In our experiments, we observed clear task-dependent differences in how performance scales with block size. Simple tasks
such as in-context recall, S3, and Parity are effectively solved with block size 2, nearly eliminating any expressivity–efficiency
trade-off. More challenging autoregressive problems such as selective copying, S4, S5, and Regular Languages benefit
substantially from larger block sizes. In contrast, the compression auto-encoding task exhibits a distinct scaling pattern:
intermediate block sizes achieve the best results, while very large blocks degrade average performance across datasets. We
also observe the same scaling behavior in our language modeling experiments, supporting general nature of our findings.


We also find that H-LRU is particularly effective on compression, likely due to its higher-order recurrence structure, whereas
BD-LRU is highly parameter- and sample-efficient on permutation-heavy tasks, consistent with the advantages of dense
intra-block mixing. Importantly, both architectures maintain strong throughput on long sequences, making moderate-to-large
block sizes viable in practice; however, for large models, GPU utilization can become a bottleneck.


Overall, our results indicate that the optimal block or window size _m_ is inherently task-dependent. In practice, we recommend
beginning with moderate block/window sizes(with moderate hidden dimension) and adjusting upward or downward based
on task complexity, sequence length, and modeling objective, thereby navigating the expressivity–efficiency trade-off. More
broadly, the problem of selecting appropriate inductive biases remains an open research question in machine learning, and
we hope that our findings contribute an additional perspective to this ongoing direction of research.


One potential limitation is that our study explored only a subset of the possible parametrizations for the selective gates; a
broader investigation could uncover even more effective formulations. Another limitation lies in computational performance;
we observed that the throughput of our models degrades more rapidly with increasing batch sizes compared to highly
optimized baselines such as Mamba, which presents a clear direction for future engineering efforts. Evaluating the proposed
architectures on large-scale language modeling and optimizing the implementation to further improve computational
efficiency are topics left for future studies.


12


**Improved state mixing in higher-order and block diagonal linear recurrent networks**


**B. Extended tables and additional figures**

|A|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||


|B|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||



_Figure 6._ Performance of different single-layer models as a function of the hidden size in the compression task ( **A** ) and the selective
copying task ( **B** ). Results are shown for different window sizes (H-LRU) and block sizes (BD-LRU) _m_ . We compare our networks with
different configurations of Mamba (with two sizes of the convolution kernel (2,4) and several values of the state space expansion factor
(2,4,8)). For comparison to low-rank models, we also include DeltaNet and DeltaProduct with 4 Householder transforms which have
different number of heads (2,4,8).


13


**Improved state mixing in higher-order and block diagonal linear recurrent networks**


Models Recall Copy Compress Overall


LSTM **1.000** **1.000** 0.750 0.916
Mamba2 **1.000** 0.807 0.720 0.842
Deltanet[-1,1] **1.000** 0.892 0.782 0.892
Deltaproduct4[-1,1] **1.000** **1.000** 0.717 0.906


BD-LRU m1 (ours) 0.775 0.835 0.725 0.778
BD-LRU m2 **1.000** 0.962 0.760 0.908
BD-LRU m3 **1.000** 0.980 0.762 0.916
BD-LRU m4 **1.000** 0.983 **0.785** **0.922**
BD-LRU m5 **1.000** 0.985 0.782 **0.922**
BD-LRU m6 **1.000** 0.980 0.775 0.918
BD-LRU m8 **1.000** 0.992 0.748 0.913
BD-LRU m16 **1.000** 0.998 0.725 0.907


H-LRU m1 (ours) 0.785 0.848 0.760 0.797
H-LRU m2 0.998 0.855 0.770 0.874
H-LRU m3 **1.000** 0.855 0.772 0.876
H-LRU m4 **1.000** 0.845 0.775 0.873
H-LRU m5 **1.000** 0.838 0.775 0.871
H-LRU m6 **1.000** 0.818 0.775 0.864
H-LRU m8 **1.000** 0.810 0.768 0.859
H-LRU m16 **1.000** 0.680 0.705 0.795


_Table 3._ Performance on the in-context recall, selective copying and compression tasks. The presented results are the average of best test
accuracies across four configurations of the corresponding synthetic dataset with different vocabulary sizes, sequence lengths and number
of training examples. Results are shown for different window (H-LRU) and block sizes (BD-LRU) _m_ . Note that overall performance
of our models consistently improves with window/block size up to approximately 3–5, after which the gains saturate or exhibit slight
degradation. All models are single-layer configurations with a maximum overall hidden dimension of 6144.


Models _S_ 3 (10k samples) _S_ 3 (250) _S_ 4 (50k) _S_ 4 (3k) _S_ 5 (100k) Overall


LSTM **1.000** 0.320 **1.000** 0.370 **1.000** 0.738
Mamba2 0.660 0.280 0.430 0.120 0.260 0.350
Deltanet[-1,1] **1.000** 0.260 0.470 0.140 0.140 0.402
Deltaproduct4[-1,1] **1.000** 0.270 **1.000** 0.130 0.140 0.508


BD-LRU m1 (ours) 0.560 0.380 0.340 0.220 0.210 0.340
BD-LRU m2 **1.000** 0.490 0.700 0.360 0.340 0.576
BD-LRU m3 **1.000** **1.000** **1.000** 0.430 0.480 0.782
BD-LRU m4 **1.000** **1.000** **1.000** **1.000** 0.880 0.976
BD-LRU m5 **1.000** **1.000** **1.000** **1.000** **1.000** **1.000**
BD-LRU m6 **1.000** **1.000** **1.000** **1.000** **1.000** **1.000**
BD-LRU m8 **1.000** **1.000** **1.000** **1.000** **1.000** **1.000**
BD-LRU m16 **1.000** **1.000** **1.000** **1.000** **1.000** **1.000**


H-LRU m1 (ours) 0.570 0.360 0.350 0.210 0.230 0.344
H-LRU m2 0.600 0.310 0.370 0.190 0.260 0.346
H-LRU m3 0.610 0.320 0.400 0.210 0.320 0.372
H-LRU m4 0.620 0.310 0.410 0.190 0.340 0.374
H-LRU m5 0.620 0.320 0.450 0.190 0.380 0.392
H-LRU m6 0.630 0.280 0.450 0.170 0.390 0.384
H-LRU m8 0.640 0.280 0.490 0.170 0.390 0.394
H-LRU m16 0.660 0.260 0.510 0.160 0.390 0.396


_Table 4._ Model performance on permutation composition tasks for different datasets of different sizes: _S_ 3 (10k training samples), _S_ 3 (250
training samples), _S_ 4 (50k training samples), _S_ 4 (3k training samples) _S_ 5 (100k training samples). The accuracy values reflect the impact
of window size (H-LRU) and block size (BD-LRU), both denoted by _m_ . We note that BD-LRU performance improves with block size,
demonstrating strong sample efficiency by solving the tasks even given limited training data. All models are single-layer configurations
with a maximum overall hidden dimension of 6144.


14


**Improved state mixing in higher-order and block diagonal linear recurrent networks**


**C. Experiments**


**Motivation for synthetic tasks.** The sequence modeling capabilities of modern neural architectures are often evaluated
through large-scale experiments involving models with billions of parameters and trained on trillions of tokens (Kaplan
et al., 2020; Waleffe et al., 2024). However, recent studies have shown that many of these capabilities can be assessed
using smaller models trained on carefully designed synthetic datasets which target specific tasks that are crucial for general
sequence modeling (Arora et al., 2023; Poli et al., 2024).


First, the well-established equivalence between lossless compression and probabilistic modeling suggests that models that
compress well also generalize well (Shannon, 1948; Hutter, 2005). Indeed, recent work shows that there is a clear connection
between language modeling and compression (Gu, 2025), although with some difference in scaling laws (Deletang et al.,´
2023). In light of this, we include in our evaluation a task that tests the efficiency of temporal information integration, the
auto-encoding compression task from (Poli et al., 2024).


Next, general sequence modeling requires not only the ability to develop a fixed prediction algorithm, but also the capacity to
adapt dynamically to changes within the input context. Such _in-context abilities_ have been extensively studied and have been
suggested to explain the success of the Transformer architecture (Olsson et al., 2022). To benchmark this basic capability,
we choose the selective copying and associative recall tasks that have been shown to be good indicators of the in-context
abilities of sequence models (Arora et al., 2023; Poli et al., 2024), as well as indicators of downstream capabilities (Waleffe
et al., 2024).


**Synthetic token manipulation tasks.** We benchmarked our architectures using the Mechanistic Architecture Design
(MAD) framework (Poli et al., 2024), a framework for efficient model evaluation and prototyping. The MAD protocol is
motivated by the challenge of predicting how architectural choices impact performance at scale. The working hypothesis
of MAD is that an architecture’s macroscopic scaling behavior can be effectively predicted by its performance on a set of
microscopic, mechanistic tasks.


The benchmark consists of a diverse suite of sequence modeling challenges designed to test core token manipulation
capabilities. By evaluating models at a small, fixed computational scale, MAD produces a relative ranking of architectures
that has been shown to be predictive of their compute-optimal performance in large-scale language modeling (Poli et al.,
2024). This approach not only approximates scaling outcomes, but also provides valuable insights into the compositional
skills and failure modes of a given design.


In particular, we utilize three tasks from the MAD framework:


  - **Compression task** . Models are tasked to compress a random sequence of input tokens into a single aggregation token.
Then, this aggregation token is passed through an encoder MLP, the output of which is used to reconstruct the original
sequence via a decoder MLP. All models were tested using a standard encoder-decoder architecture (Embedding, Tested
Model, MLP Encoder, MLP Decoder).


  - **Selective copying task** . Models are tasked with copying tokens from one position of an input sequence to a later
position of the sequence, while ignoring irrelevant noise tokens that are randomly inserted into the sequence. This
task is designed to evaluate the ability of a model to perform selective temporal integration in the specific order of
occurrence in the sequence. All models were tested using a standard decoder-only architecture (Embedding, Tested
Model, MLP Decoder).


  - **Associative recall task** . Models are presented with an input sequence of key-value pairs and tasked with retrieving all
values from the input sequence associated with the presented keys. This task tests the ability of a model to adaptively
retrieve information depending on the established in-context associations. All models were tested using a standard
decoder-only architecture (Embedding, Tested Model, MLP Decoder).


In our experiments, each model was evaluated across four configurations: a baseline (vocabulary size: 16, sequence length:
64, training examples: 20,000) and three variations designed to probe specific failure modes. These variations all use the
same base parameters, but independently (i) increase the vocabulary size to 32, (ii) extend the sequence length to 128, or
(iii) reduce the training set to 10,000 examples to test vocabulary handling, long-range capabilities, and sample efficiency,
respectively.


15


**Improved state mixing in higher-order and block diagonal linear recurrent networks**


**Synthetic permutation tasks.** In our experiments, we employ synthetic datasets derived from the symmetric permutation
groups _Sn_, which denotes the group of all possible permutations of _n_ elements. These groups provide a natural hierarchy of
complexity: _S_ 2 contains only two permutations and is fully commutative, making it relatively simple to model. In contrast,
groups with _n ≤_ 3 (e.g., _S_ 3, _S_ 4, _S_ 5) are non-commutative, and their size grows factorially with _n_, which rapidly increases
the difficulty of learning the underlying structure. For instance, _S_ 3, with six elements, is the smallest non-commutative
group. Geometrically, _S_ 3 can be interpreted as the group of symmetries of an equilateral triangle, including both rotations
and reflections. The complexity increases substantially with _S_ 4, which contains 24 elements and corresponds to the full
symmetry group of a regular tetrahedron. _S_ 4 introduces more intricate subgroup structures and non-trivial normal subgroups.
Extending further, _S_ 5 has 120 elements and is the first symmetric group that is not solvable, representing the symmetries of
a regular pentagon in the plane.


We assess model performance on the synthetic permutation group task from (Merrill et al., 2024), which is designed to probe
state-tracking and generalization to complex structures. Using their toolbox, we generated datasets for the symmetric groups
_S_ 3, _S_ 4, and _S_ 5 with a fixed sequence length of 16. To evaluate sample efficiency, we created five distinct data configurations:
_S_ 3 (10k and 250 examples), _S_ 4 (50k and 3k examples), and _S_ 5 (100k examples). The _S_ 5 setting is particularly data-limited
compared to the multi-million-example setups used in previous studies (Siems et al., 2025). All models were tested using
a standard decoder-only architecture (Embedding, Tested Model, MLP Decoder), consistent with the MAD benchmark
protocol.


**Training details on synthetic tasks** All models were implemented in PyTorch (Paszke et al., 2019). For training, we
follow the experimental settings of the MAD framework. All models are trained with the AdamW optimizer (Loshchilov
& Hutter, 2017) with parameters _β_ 1 = 0 _._ 9 _, β_ 2 = 0 _._ 999 _, ϵ_ = 10 _[−]_ [8] and a cosine scheduler (Loshchilov & Hutter, 2016)
(minimum LR: 0 _._ 00001), with the initial learning rate selected from 0 _._ 001, 0 _._ 0005, 0 _._ 0001. The final reported metric is the
best test accuracy across all three learning rate configurations and five runs with distinct random seeds. For training we used
NVIDIA A100 and NVIDIA H100, while we used NVIDIA H100 for benchmarking the best throughput across models.


**Training details on language modeling** We conduct our experiments on the well-established FineWeb dataset(Penedo
et al., 2024) using the PlainLM training setup (Ajroldi, 2024). All models are trained with the AdamW optimizer (Loshchilov
& Hutter, 2017) with parameters _β_ 1 = 0 _._ 9 _, β_ 2 = 0 _._ 95 _, ϵ_ = 10 _[−]_ [8] and a cosine scheduler (Loshchilov & Hutter, 2016). All
models are trained on a single NVIDIA H100 GPU.


**D. Computational complexity**


This section provides a breakdown of the Floating Point Operations (FLOPs) required for hidden-to-hidden state transition
in the recurrent architectures discussed. For this breakdown, we define the number for blocks for BD-LRU and H-LRU as _H_
and _N_ denotes overall hidden dimension, _m ∗_ _H_ = _N_ as previously. The sequence length is denoted as _T_ . For Mamba2,
the state expansion factor is denoted by _S_ . In Deltanet and Deltaproduct4, _Nh_ denotes the number of heads, _C_ denotes
the number of chunks in the Deltanet implementation, _Hn_ denotes the number of Householder transformations, and _r_ = 1
denotes low rank. The calculations focus on the recurrence mechanism, omitting additional components like the input
projections or gating, as they can be precomputed in advance. A multiply-add operation is counted as 2 FLOPs.


_Table 5._ Summary of computational costs for hidden state updates.


**Architecture** **FLOPs per recurrent step** **Implementation complexity**


LSTM 8 _H_ [2] + 25 _H_ _O_ ( _TH_ [2] )
H-LRU 2 _Hm_ + 2 _H_ _O_ ( _Hm_ [3] _log_ ( _T_ ))
BD-LRU 2 _Hm_ [2] + 2 _H_ _O_ ( _Hm_ [3] _log_ ( _T_ ))
Mamba2 2 _NS_ _O_ ( _T_ ( _N_ [2] + _NS_ ))
Deltanet _Nh_ (4 _Nr_ + 4 _N_ ) _O_ ( _TCN_ + _TN_ [2] )
Deltaproduct4 _HnNh_ (4 _Nr_ + 4 _N_ ) _O_ ( _Hn_ ( _TCN_ + _TN_ [2] ))


16


**Improved state mixing in higher-order and block diagonal linear recurrent networks**


**E. Proof of Proposition 1.**


First, note that stability directly follows from the induced norm bound. We can reason blockwise: assuming [�] _j_ _[|]_ [(] _[A]_ _t_ _[k]_ [)] _[i,j][| ≤]_ [1]

implies that the eigenvalues of state-transition matrix _|λ_ _[k]_ _i,t_ _[| ≤]_ [1][. Therefore, the product of such matrices will result in]
dynamical stability.


Next, by block-diagonality, it is sufficient to show that for all _k ∈_ [1 _, m_ ], _∥_ **h** _[k]_ _T_ _[∥][∞]_ _[≤]_ [max] _[t][∈]_ [[0] _[,T]_ [ ]] _[∥]_ **[v]** _t_ _[k][∥][∞]_ [. Let] _[ h]_ _i,t_ _[k]_ [be the] _[ i]_ [-th]
coordinate of the generic _k_ -th block hidden state **h** _[k]_ _t_ [at time] _[ t]_ [.]







_h_ _[k]_ 1 _,t−_ 1
...
_h_ _[k]_ _m,t−_ 1



_h_ _[k]_ 1 _,t_
...
_h_ _[k]_ _m,t_











_a_ _[k]_ 1 _,_ 0 _,t_
...
_a_ _[k]_ _m,_ 0 _,t_











_v_ 1 _[k]_ _,t_
...
_vm,t_ _[k]_















_a_ _[k]_ 1 _,_ 1 _,t_ _· · ·_ _a_ _[k]_ 1 _,m−_ 1 _,t_ _a_ _[k]_ 1 _,m,t_
_a_ _[k]_ 2 _,_ 1 _,t_ _· · ·_ _a_ _[k]_ 2 _,m−_ 1 _,t_ _a_ _[k]_ 2 _,m,t_
... ... ... ...
_a_ _[k]_ _m,_ 1 _,t_ _· · ·_ _a_ _[k]_ _m,m−_ 1 _,t_ _a_ _[k]_ _m,m,t_










_×_







 _⊙_






 _._ (9)










 =






 +



Hence,



_h_ _[k]_ _i,t_ [=]



_m_

- _a_ _[k]_ _i,j,t_ _[h][k]_ _j,t−_ 1 [+] _[ a]_ _i,_ _[k]_ 0 _,t_ _[v]_ _i,t_ _[k]_ _[.]_ (10)

_j_ =1



It is then clear that by subadditivity of the absolute value,



_|h_ _[k]_ _i,t_ _[| ≤]_



_m_

- _|a_ _[k]_ _i,j,t_ _[| · |][h]_ _j,t_ _[k]_ _−_ 1 _[|]_ [ +] _[ |][a]_ _i,_ _[k]_ 0 _,t_ _[| · |][v]_ _i,t_ _[k]_ _[|][.]_ (11)


_j_ =1



Hence, by collecting the non-coefficient terms, we find a further upper bound












    -     _·_ max _|vi,t_ _[k]_ _[|][,]_ [ max] _j,t−_ 1 _[|]_ _._ (12)
_j∈_ [1 _,m_ ] _[|][h][k]_



_|h_ _[k]_ _i,t_ _[| ≤]_



_m_

 




- _|a_ _[k]_ _i,j,t_ _[|]_ [ +] _[ |][a]_ _i,_ _[k]_ 0 _,t_ _[|]_


_j_ =1



By hypothesis, [�] _j_ _[m]_ =1 _[|][a]_ _i,j,t_ _[k]_ _[|]_ [ +] _[ |][a][k]_ _i,_ 0 _,t_ _[|]_ [ =][ �] _j_ _[|]_ [(] _[A]_ _t_ _[k]_ [)] _[i,j][| ≤]_ [1][, and hence we conclude that]


                     -                      _|h_ _[k]_ _i,t_ _[| ≤]_ [max] _|vi,t_ _[k]_ _[|][,]_ [ max] _j,t−_ 1 _[|]_ _._ (13)
_j∈_ [1 _,m_ ] _[|][h][k]_


At this point, we can finalize the proof by induction. We want to show that _∥_ **h** _[k]_ _T_ _[∥][∞]_ _[≤]_ [max] _[t][∈]_ [[0] _[,T]_ [ ]] _[∥]_ **[v]** _t_ _[k][∥][∞]_ [. Let us start from]
_T_ = 1. Since _h_ _[k]_ _i,_ 0 [= 0][ for all] _[ i][ ∈]_ [[1] _[, m]_ []][, we have]


_h_ _[k]_ _i,_ 1 [=] _[ a]_ _i,_ _[k]_ 0 _,t_ _[v]_ _i,_ _[k]_ 1 _[,]_ (14)


hence, again because [�] _j_ _[|]_ [(] _[A]_ 0 _[k]_ [)] _[i,j][| ≤]_ [1][,] _[ |][h][k]_ _i,_ 1 _[| ≤|][v]_ _i,_ _[k]_ 1 _[|]_ [, we can conclude that] _[ ∥]_ **[h]** 1 _[k][∥][∞]_ _[≤∥]_ **[v]** 1 _[k][∥][∞]_ [. Let us then assume by]

induction that _∥_ **h** _[k]_ _T −_ 1 _[∥][∞]_ _[≤]_ [max] _[t][∈]_ [[0] _[,T][ −]_ [1]] _[∥]_ **[v]** _t_ _[k][∥][∞]_ [. Recall that by Equation 13,]


                     -                      _|h_ _[k]_ _i,t_ _[| ≤]_ [max] _|vi,t_ _[k]_ _[|][,]_ [ max] _j,t−_ 1 _[|]_ (15)
_j∈_ [1 _,m_ ] _[|][h][k]_

= max                 - _|vi,t_ _[k]_ _[|][,][ ∥]_ **[h]** _[k]_ _t−_ 1 _[∥][∞]_                 - _._ (16)


17


Hence,



**Improved state mixing in higher-order and block diagonal linear recurrent networks**


_∥_ **h** _[k]_ _t_ _[∥][∞]_ [= max] _j,t_ _[|]_ (17)
_j∈_ [1 _,m_ ] _[|][h][k]_

_≤_ max       - _|vi,t_ _[k]_ _[|][,][ ∥]_ **[h]** _[k]_ _t−_ 1 _[∥][∞]_       - (18)
_j∈_ [1 _,m_ ] [max]




   = max max _i,t_ _[|][,][ ∥]_ **[h]** _[k]_ _t−_ 1 _[∥][∞]_
_j∈_ [1 _,m_ ] _[|][v][k]_




(19)




                     -                     _≤_ max _∥_ **v** _t_ _[k][∥][∞][,]_ max _t_ _[∥][∞][∥]_ (20)
_t∈_ [0 _,T −_ 1] _[∥]_ **[v]** _[k]_


= max _t_ _[∥][∞][,]_ (21)
_t∈_ [0 _,T_ ] _[∥]_ **[v]** _[k]_


where in the second-last line we used the induction hypothesis.


**F. Selectivity ablation**


To isolate and quantify the contribution of selectivity, we conducted an ablation study. In this analysis, the input-dependent
selective gates in both the H-LRU and BD-LRU architectures were replaced with data-invariant, learnable parameters.


As hypothesized, the non-selective variants exhibited a significant performance degradation compared to their selective
counterparts on our synthetic benchmark. On tasks requiring dynamic token manipulation—such as in-context recall,
selective copying, and permutation composition—the non-selective models failed to achieve meaningful performance. For
these tasks, increasing the window or block size yielded no discernible improvement, confirming the necessity of selectivity.


However, the results on the compression task were more nuanced, see Fig. 7. We observed that our proposed _L_ 1
normalization scheme enabled the non-selective models to improve with larger block and window sizes, albeit at a lower
rate than their selective analogs.

|A|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|||||
|||||


|C|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|||||||


|D|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|||||||
|||||||



_Figure 7._ Scaling analysis of non-selective models on the compression task. **A** . Performance as a function of window size _m_ of
non-selective higher-order LRU (H-LRU) and block size _m_ of block diagonal LRU (BD-LRU). For the convolutional baseline, the
performance presented as a function of kernel size. **B** . The same results plotted against parameter count. Note that scaling with
window size of non-selective H-LRU demonstrates extreme parameter efficiency, resulting in a nearly vertical trajectory on the plot. **C** .
Comparison of scaling properties between different parameterizations for H-LRU. **D** . Comparison of scaling properties between different
parameterizations for BD-LRU.


To highlight the advantages of recurrent architectures, we used a convolution layer as a baseline. This model is limited
to explicit, local time mixing within its kernel, in contrast to the implicit and unbounded temporal integration provided
by a hidden state. Our experiments showed that H-LRU decisively outperforms the convolution on the compression task.
This demonstrates the critical role of recurrent state mixing for tasks requiring efficient long-range temporal reasoning.
Furthermore, the non-selective H-LRU with large window sizes ( _m >_ 15) demonstrated strong performance, surpassing the
LSTM and Mamba baselines and even approaching the performance of our selective models. This finding underscores the
powerful inductive bias of the higher-order recurrence for parameter-efficient compression.


In contrast, the non-selective BD-LRU performed poorly on the compression task, only marginally surpassing the convolution
baseline. Interestingly, for this non-selective variant, the sigmoidal _L_ 1 normalization outperformed softmax normalization,
highlighting a difference in how these schemes interact with selective versus fixed parameterizations.


In addition, when we analyzed H-LRU with minimal point-wise selective gates which don’t mix channel dimensions, we


18


**Improved state mixing in higher-order and block diagonal linear recurrent networks**


observed very moderate improvement in compression task. This indicates that not only selectivity itself but also density of
selectivity in gates plays important role in improving networks’ expressivity.


While the overall performance of these non-selective models is modest, their parameter efficiency can become advantageous
in resource-constrained settings. Given the strong compression results of the non-selective H-LRU, we hypothesize that
such models could be optimized for use as highly efficient embedding layers, a direction we leave for future research.


**G. Relation between expressivity of LRUs and State Space Duality**


Recently, it has been shown that there is a direct correspondence between state space models, the Transformer architecture
and structured attention matrices (Dao & Gu, 2024). Following this approach, we can reformulate the general LRU as a
general discrete time SSM
**h** _t_ = **A** _t ×_ **h** _t−_ 1 + **B** _t ×_ **v** _t_
(22)
**y** _t_ = **C** _t ×_ **h** _t._


Here, we consider the general case of SSMs, in which mixing matrices **C** _t,_ **A** _t,_ **B** _t_ are dense matrices. We note that although
state space models are commonly defined in continuous time, they have to be discretized for implementation, at which point
they conform to the discrete form described by Eq. 22. In this study, we effectively ignored the role of **C** _t_, but it can be
introduced without affecting the validity of our arguments.


Following the approach of reformulating state space models (SSMs) as attention mechanisms, the architecture given in
Eq. 22 can be expressed in block matrix representation assuming a fixed sequence length _T_ :

   **C** **B** **0** **0** _· · ·_ **0**   



**y** 1
**y** 2
**y** 3
...
**y** _T_



























**v** 1
**v** 2
**v** 3
...
**v** _T_











**C** 1 **B** 1 **0** **0** _· · ·_ **0**
**C** 2 **A** 1 **B** 1 **C** 2 **B** 2 **0** _· · ·_ **0**
**C** 3 **A** 2 **A** 1 **B** 1 **C** 3 **A** 2 **B** 2 **C** 3 **B** 3 _· · ·_ **0**
... ... ... ... ...
**C** _T_ - _Tj_ =1 **[A]** _[j]_ **[B]** [1] **C** _T_ - _Tj_ =2 **[A]** _[j]_ **[B]** [2] _· · ·_ _· · ·_ **C** _T_ **B** _T_











=




If we abstract the details of SSMs matrices, we obtain the generalized attention formulation:











**y** 1
**y** 2
**y** 3
...



~~~~



~~~~

~~~~




**A** 1 _,_ 1 **0** **0** _· · ·_ **0**
**A** 2 _,_ 1 **A** 2 _,_ 2 **0** _· · ·_ **0**

**A** 3 _,_ 1 **A** 3 _,_ 2 **A** 3 _,_ 3 ... ...
... ... ... ... ...











**v** 1
**v** 2
**v** 3
...














 =






 _._ (23)



Importantly, elements **A** _k,l_ of the block attention matrix are also matrices in this representation. According to State Space
Duality (Dao & Gu, 2024), both the attention in Transformers and diagonal SSMs result in diagonal matrices **A** _k,l_ . So, their
architecture allows for efficient parallelization as it separates temporal mixing from channel mixing.


In contrast to diagonal SSMs and LRUs, both H-LRU and BD-LRU architectures result in block-diagonal matrices **A** _k,l_,
allowing richer but limited by block channel mixing inside the generalized block attention matrix 23. Such channel mixing
allows for the state mixing patterns that are not accessible to one layer of diagonal LRU or SSMs. Although the channel
mixing in H-LRU is more expressive than the one in a diagonal LRU, it is still more restricted compared to BD-LRU (it is
equivalent to mixing only in one row of block-diagonal matrix), placing expressivity of H-LRU between diagonal LRU
and BD-LRU. Notably, if we extend SSMs with higher-order or block-diagonal structures, their expressivity would lag
behind analogous LRUs due to the restrictions on mixing patterns imposed by the chosen discretization scheme. Overall, the
generalized block attention formulation 23 reveals that diagonal, higher-order, block diagonal and dense variants of LRUs
and SSMs form a hierarchy of architectures, each providing access to increasingly complex state mixing patterns which
result in increased expressivity.


**H. Chomsky Hierarchy Tasks**


The Chomsky hierarchy formalizes increasing levels of expressiveness and computational complexity of formal languages
into several hierarchical classes (Chomsky, 1956; Deletang et al., 2022). Here, we tested several tasks from this hierarchy:´
Parity, Cycle Navigation, Modular Arithmetic with and without brackets. Parity task requires computing whether given


19


**Improved state mixing in higher-order and block diagonal linear recurrent networks**


binary string is even or not. Cycle Navigation requires computing the end position given a sequence of movements on
a cycle of length 5. Modular Arithmetic tasks require computing the result modulo 5 for given sequence of numbers in
(0 _,_ 1 _,_ 2 _,_ 3 _,_ 4) and operations in (+ _, −, ·_ ), with or without brackets.


In our experiments, we observe that similar to _S_ 3 task, Parity task can be solved by BD-LRU with access to negative
eigenvalues ( _m ≥_ 2). For Cycle Navigation task we obtain similar results as for _S_ 5 task. BD-LRU is able to solve it starting
from _m_ = 5. Therefore, the results on these two tasks from Chomsky Hierarchy support our previously found advantage of
BD-LRUs on permutations tasks.


Modular arithmetic tasks present a challenge for highly parallel Transformer architecture, often require grokking and having
pure generalization (Gromov, 2023). In contrast, it has been shown that sequential nature of state mixing in RNNs has a
strongly beneficial bias for arithmetic-like induction (Merrill & Sabharwal, 2023). However, both our linear variants and
other modern LRNNs struggle with such arithmetic tasks (Siems et al., 2025), supporting the idea that nonlinearity of state
transitions is crucial in such tasks (Chang & Bisk, 2024). In our experiments, we found that BD-LRU were able to solve
Modular Arithmetic without brackets, while the version with brackets remained challenging, similar to other RNNs.


Models cycle nav mod arith no brack mod arith w brack parity


LSTM **1.000** 0.976 0.663 **1.000**


BD-LRU m1 0.434 0.370 0.370 0.512
BD-LRU m2 0.425 0.493 0.417 **1.000**
BD-LRU m3 0.597 0.546 0.434 **1.000**
BD-LRU m4 0.608 0.459 0.435 **1.000**
BD-LRU m5 **1.000** 0.525 0.422 **1.000**
BD-LRU m6 **1.000** 0.433 0.440 **1.000**
BD-LRU m8 **1.000** 0.553 0.395 **1.000**
BD-LRU m16 **1.000** **1.000** 0.448 **1.000**


_Table 6_


20


**Improved state mixing in higher-order and block diagonal linear recurrent networks**


**I. Eigenvalue analysis**




|1.0<br>0.5<br>0 Imag<br>-0.5<br>-1.0|Col2|
|---|---|
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||


|1.0<br>0.5<br>0 Imag<br>-0.5<br>-1.0|Col2|Col3|
|---|---|---|
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||


|Col1|Col2|
|---|---|
|||
|||
|||


|Col1|Col2|
|---|---|
|||
|||
|||


|1.0<br>0.5<br>0 Imag<br>-0.5<br>-1.0|Col2|
|---|---|
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||


|1.0<br>0.5<br>0 Imag<br>-0.5<br>-1.0|Col2|Col3|
|---|---|---|
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||


|Col1|Col2|
|---|---|
|||
|||
|||


|Col1|Col2|
|---|---|
|||
|||
|||


|1.0<br>0.5<br>0 Imag<br>-0.5<br>-1.0|Col2|
|---|---|
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||


|1.0<br>0.5<br>0 Imag<br>-0.5<br>-1.0|Col2|Col3|
|---|---|---|
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||


|Col1|Col2|
|---|---|
|||
|||
|||


|Col1|Col2|
|---|---|
|||
|||
|||


|1.0<br>0.5<br>0 Imag<br>-0.5<br>-1.0|Col2|
|---|---|
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||


|1.0<br>0.5<br>0 Imag<br>-0.5<br>-1.0|Col2|Col3|
|---|---|---|
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||
|~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||


|Col1|Col2|
|---|---|
|||
|||
|||


|Col1|Col2|
|---|---|
|||
|||
|||











_(a)_







|Col1|Col2|
|---|---|
|||
|||
|||
|~~-~~0.5<br>0<br>0.5<br>Real|~~-~~0.5<br>0<br>0.5<br>Real|


_(b)_




|1.0<br>0.5<br>0 Imag<br>-0.5<br>-1.0<br>-1.0 -|Col2|
|---|---|
|~~-~~1.0<br>~~-~~<br>~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||
|~~-~~1.0<br>~~-~~<br>~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||
|~~-~~1.0<br>~~-~~<br>~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>||
|~~-~~1.0<br>~~-~~<br>~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|0.5<br>0<br>0.5<br>Real|


|1.0<br>0.5<br>0 Imag<br>-0.5<br>-1.0<br>-1.0|Col2|Col3|
|---|---|---|
|~~-~~1.0<br>~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||
|~~-~~1.0<br>~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||
|~~-~~1.0<br>~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|||
|~~-~~1.0<br>~~-~~1.0<br>~~-~~0.5<br>0<br>0.5<br>1.0<br>Imag<br>|~~-~~0.5<br>0<br>0.5<br>Real|~~-~~0.5<br>0<br>0.5<br>Real|


|Col1|Col2|
|---|---|
|||
|||
|||
|~~-~~0.5<br>0<br>0.5<br>Real|~~-~~0.5<br>0<br>0.5<br>Real|



_Figure 8._ Eigenvalues of transition matrices in H-LRU/BD-LRU on _S_ 5 dataset. (a) BD-LRU with softmax normalization. (b) H-LRU
with softmax normalization. Each subplot corresponds to a specific time step (horizontal axis) and block size (vertical axis). Note that as
block size increases, the number of available symmetries increases as well.


21


