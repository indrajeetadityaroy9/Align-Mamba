## **Understanding Input Selectivity in Mamba:** **Impact on Approximation Power, Memorization, and Associative Recall Capacity**

**Ningyuan Huang** [1 2] **Miguel Sarabia** [3] **Abhinav Moudgil** [1 4] **Pau Rodr´ıguez** [3] **Luca Zappella** [3] **Federico Danieli** [3]



**Abstract**


State-Space Models (SSMs), and particularly
Mamba, have recently emerged as a promising
alternative to Transformers. Mamba introduces
input selectivity to its SSM layer (S6) and incorporates convolution and gating into its block
definition. While these modifications do improve
Mamba’s performance over its SSM predecessors, it remains largely unclear how Mamba leverages the additional functionalities provided by
input selectivity, and how these interact with the
other operations in the Mamba architecture. In
this work, we demystify the role of input selectivity in Mamba, investigating its impact on function
approximation power, long-term memorization,
and associative recall capabilities. In particular:
(i) we prove that the S6 layer of Mamba can represent projections onto _Haar wavelets_, providing an
edge over its Diagonal SSM (S4D) predecessor
in approximating discontinuous functions commonly arising in practice; (ii) we show how the S6
layer can dynamically counteract memory decay;
(iii) we provide analytical solutions to the MQAR
associative recall task using the Mamba architecture with different mixers — Mamba, Mamba-2,
and S4D. We demonstrate the tightness of our theoretical constructions with empirical results on
concrete tasks. Our findings offer a mechanistic
understanding of Mamba and reveal opportunities
for improvement.


**1. Introduction**


State Space Models (SSMs) have recently emerged as a
promising approach for long-range sequence modeling, due
to their computational efficiency compared to Transform

1Work done while at Apple 2Flatiron Institute, New York, USA
3Apple 4Mila Research Institute, Montreal, Canada. Correspondence to: Ningyuan Huang _<_ thuang@flatironinstitute.org _>_ .


_Proceedings of the 42_ _[nd]_ _International Conference on Machine_
_Learning_, Vancouver, Canada. PMLR 267, 2025. Copyright 2025
by the author(s).



ers, and parallelizability compared to (nonlinear) Recurrent
Neural Networks (RNNs). In particular, Mamba (Gu & Dao,
2023; Dao & Gu, 2024) demonstrated state-of-the-art performance on various language modeling tasks, with smaller
model size and faster inference than Transformers. The success of Mamba has largely been attributed to the fact that
the parameters in its SSM layer (S6) are input-dependent,
leading to improved expressivity compared to its SSM predecessors (Cirone et al., 2024). The goal of this work is to
provide a more structural explanation for Mamba’s superior
performance. We do so by answering two questions: (i) how
does the S6 layer’s expressivity translate into its practical
performance? (ii) how can the S6 layer interact with the rest
of the Mamba block to solve concrete tasks?


We answer question (i) by providing a fine-grained analysis
of the S6 layer via the lens of function approximation and
long-term memory. We prove that the S6 layer can represent
projections onto _Haar wavelets_ and thus efficiently model
discontinuous signals, which is relevant for solving practical
tasks. Moreover, we show that the S6 layer still suffers
from exponential memory decay, but highlight a mechanism
which allows it to dynamically counteract such decay.


Building on the understanding of the S6 layer, we then
investigate how S6 interacts with the other components
of the Mamba block (which also encompasses a shortconvolution and a gate branch, see Tab. 2) to tackle the
Multiple-Query Associative Recall (MQAR) task (Arora
et al., 2024a). We prove that 1-layer Mamba and Mamba-2
models can both solve MQAR, even without gating; we
describe how S6 and the convolution interact to achieve this,
and how Mamba-2 leverages its independent convolutions
to get more parameter-efficient solutions. We also show that
a 1-layer Mamba can solve MQAR exactly _even without_
_input-dependence_ in its SSM: this (perhaps unexpected) result helps cementing the importance of convolution and gating in the Mamba architecture. This analysis further informs
us on how a variation to the functional form of Mamba,
and particularly to how the SSM state matrix is affected by
the input, can improve its performance on the INDUCTION
HEADS task (Bietti et al., 2024; Sanford et al., 2024a).


We complement our theoretical findings with numerical experiments on synthetic sequence modeling tasks. For the S6



1


layer, we demonstrate its approximation power on discontinuous functions, and its counteraction of memory decay,
via the KEEP _n_ -TH task — a generalization of KEEP FIRST
from (Chiang & Cholak, 2022) requiring to memorize the
_n_ -th token in a sequence. Finally, for the full Mamba model,
we confirm empirically that the model sizes prescribed theoretically by our analytical solutions to the MQAR and
INDUCTION HEADS tasks are tight in practice. Overall, our
contributions can be summarized as follows:


  - We prove that the S6 layer of Mamba can represent
projections onto _Haar wavelets_, providing an edge
over the S4D layer in approximating discontinuous
functions commonly arising in practice (Sec. 4.1).


  - We use sensitivity analysis to show the S6 layer generally suffers from exponential decay of memory, and describe how it can dynamically counteract it (Sec. 4.2).


  - We show how the Mamba architecture can exactly
solve the MQAR task using different SSM mixers,
with an explicit characterization of the required model
size, which helps explaining their performance difference (Sec. 5.1).


  - Our findings reveal opportunities to further improve
Mamba, such as by changing the way input dependence
is incorporated into the SSM state matrix (Sec. 5.2).


**2. Related Work**


**State-Space Models (SSMs)** SSMs (i.e., linear RNNs)
have recently emerged as a promising sequence-modeling
approach, with faster training than (nonlinear) RNNs, and
faster inference time than Transformers. To enable longterm memorization capability, Gu et al. (2020) designed
SSMs as polynomial approximations of signals, by prescribing non-normal HiPPO matrices as state matrices. To improve computational efficiency, Gu et al. (2022b) proposed
the S4 model: first by reparameterizing HiPPO as a sum
of normal and low-rank matrices, then by further considering a _diagonal_ simplification in the S4D model (Gu et al.,
2022a). All these SSMs are Linear Time-Invariant and thus
computationally efficient, but consequently lack the ability
to process information in a time-varying, input-dependent
manner. Mamba (Gu & Dao, 2023) overcomes this by using input-dependent SSM parameters, without sacrificing
computational efficiency, showing performance competitive
with Transformers on long-range language modeling tasks.
Mamba-2 (Dao & Gu, 2024) simplifies the state matrix to a
scalar multiple of identity and modifies the Mamba model
architecture, showing further empirical improvements.


**Expressivity of SSMs** The analyses on the expressivity
of SSMs can be divided into two main approaches: formal



language theory and approximation theory. Studies following the first approach investigate what formal languages
can SSM recognize (Merrill et al., 2024; Sarrof et al., 2024;
Grazzi et al., 2025). Studies ascribing to the second approach — including this work — characterize what function
classes can SSMs approximate: Li et al. (2022) showed that
SSMs can approximate linear functionals with exponential
memory decay; Wang et al. (2024) extended this analysis to
nonlinear RNNs, showing however that adding nonlinearity
(in the hidden state recurrence) does not fix the memory
decay issues. Orvieto et al. (2024) proved that SSMs augmented with MLPs are universal approximators of regular
functionals, but this improvement over Li et al. (2022) and
Wang et al. (2024) requires the hidden state size to grow linearly with sequence length. Cirone et al. (2024) extended the
results from Li et al. (2022) to _input-dependent_ SSMs such
as Mamba, showing their universal approximation on the
class of nonlinear functionals arising from controlled differential equations. While Cirone et al. (2024) highlighted how
Mamba can approximate a _larger_ class of functionals than
S4D, we explore the consequences of this observation by
characterizing specific function classes arising in practice.


**Associative Recall Capability** Associative Recall (AR)
describes the ability of a model to retrieve information from
its memory, based on the input context. Tasks for evaluating
associative recall capabilities include INDUCTION HEADS
(Olsson et al., 2022; Sanford et al., 2024a), _k_ -HOP INDUCTION HEADS (Sanford et al., 2024b), MULTIPLE-QUERY
ASSOCIATIVE RECALL (MQAR) (Arora et al., 2024a), and
NEEDLE-IN-THE-HAYSTACK (Kamradt, 2023).Empirically,
Mamba (Gu & Dao, 2023) and Mamba-2 (Dao & Gu, 2024)
demonstrated performance competitive with Transformers
on (a simple version of) INDUCTION HEADS and MQAR,
respectively. Theoretical understanding of how language
models perform associative recall begins to emerge: Bietti et al. (2024) constructed a 2-layer Transformer with
positional encoding that solves the INDUCTION HEADS
task; Sanford et al. (2024b) extended such construction
to a log-depth Transformer that solves the _k_ -HOP INDUC
TION HEADS task; Arora et al. (2024a) showed that a gated
convolution model can solve MQAR. However, these constructions all require the model size to scale with the input
sequence length. In this work, we show that 1-layer Mamba
models can solve INDUCTION HEADS and MQAR with
model size _independent of_ the sequence length, highlighting
the role of the convolution operation and the gate branch,
that has been previously overlooked in the literature.


**3. Preliminaries: Linear RNNs as SSMs**


The application of a Linear RNN can be interpreted as the
discrete solution of a _linear_ dynamical system, or _SSMs_,


_**h**_ ˙ ( _t_ ) = _**A**_ ( _t_ ) _**h**_ ( _t_ )+ _**B**_ ( _t_ ) _x_ ( _t_ ) _,_ _**y**_ ( _t_ ) = _**C**_ ( _t_ ) _**h**_ ( _t_ ) _._ (1)



2


Here, the input _x_ ( _t_ ) _∈_ R acts as forcing term for the _hidden_
_state_ _**h**_ _∈_ R _[N]_ through the application of the _input matrix_
_**B**_ ( _t_ ) _∈_ R _[N]_ _[×]_ [1] . The natural evolution of the hidden state
is dictated by the _state matrix_ _**A**_ ( _t_ ) _∈_ R _[N]_ _[×][N]_ . Finally, the
output _**y**_ _∈_ R _[d][y]_ is obtained by linearly transforming the
hidden state via the _output matrix_ _**C**_ ( _t_ ) _∈_ R _[d][y][×][N]_ . Collectively, we call _**A**_ ( _t_ ) _,_ _**B**_ ( _t_ ) _,_ _**C**_ ( _t_ ) the _SSM parameters_ .
For computational efficiency, modern Linear RNNs consider _diagonal_ state matrices, with negative eigenvalues
_**A**_ ( _t_ ) = **Λ** ( _t_ ) = _−_ diag([ _λ_ 1( _t_ ) _, . . ., λN_ ( _t_ )]). This simplifies the solution of (1), and ensures stability in the evolution
of the hidden state. Under this assumption, and considering
an initial state _**h**_ (0) _≡_ **0**, we can explicitly write the hidden
state solution as an integral function of the input (Dahleh
et al., 2011):


       - _t_        - _t_
_**h**_ ( _t_ ) = _e_ _s_ **[Λ]** [(] _[r]_ [)] _[ dr]_ _**B**_ ( _s_ ) _x_ ( _s_ ) _ds._ (2)

0


Generally, inputs to Linear RNNs are provided as (discrete)
sequences of values [ _**x**_ _t_ ] _[T]_ _t_ =1 [, and thus we consider a] _[ dis-]_
_cretization_ of system (1). This amounts to substituting the
_differential_ equation with an (approximating) _recurrent_ one:


(1) _≈_ _**h**_ _t_ = **Λ** _t_ _**h**_ _t−_ 1 + _**B**_ _t xt,_ _**y**_ _t_ = _**C**_ _t_ _**h**_ ( _t_ ) _,_ (3)


where **Λ** _t,_ _**B**_ _t,_ _**C**_ _t_ are the discrete counterparts of **Λ** ( _t_ ),
_**B**_ ( _t_ ), _**C**_ ( _t_ ), respectively. Notice we can recover an explicit
solution to (3) by unrolling the recurrence relation starting
from _**h**_ 0 _≡_ **0**, to obtain








- _t_

 

_r_ = _s_ +1



_**h**_ _t_ =



_t_



_s_ =1



**Λ** _r_



_**B**_ _s xs._ (4)



and Mamba (Gu & Dao, 2023), which models time-varying
(input-dependent) dynamics,


∆( _t_ ) := SoftPlus(Linear( _x_ ( _t_ ))) _,_ **Λ** ( _t_ ) := **Λ** _,_


_**B**_ ( _t_ ) _≡_ _**B**_ ( _x_ ( _t_ )) := Linear( _x_ ( _t_ )) _,_ (7)


_**C**_ ( _t_ ) _≡_ _**C**_ ( _x_ ( _t_ )) := Linear( _x_ ( _t_ )) _._


Note that both the Mamba-2 mixer and _Linear Attention_
(Katharopoulos et al., 2020) can be interpreted as specializations of Mamba (Dao & Gu, 2024), where **Λ** _t ≡_ _λt_ _**I**_ and
**Λ** _t ≡_ _**I**_, respectively.


**4. Mamba SSM Mixer Layer Analysis**


In this section, we focus on the core of the Mamba architecture: its SSM mixer layer, also referred to as S6. The goal
is to understand how input-selectivity impacts its expressivity and long-range memorization capacity. We show that
an S6 layer can represent projections onto wavelets, thus
efficiently modeling discontinuous signals. This is useful
in practice, e.g., for isolating specific tokens in a sequence.
For long-range memorization, we use sensitivity analysis
to show that, while S6 suffers from exponential decay of
memory (akin to S4D), input-selectivity allows to decrease
the rate of such memory decay by “freezing time”. We validate our theoretical insights by experimenting on the KEEP
_n_ -TH task.


**4.1. Function Approximation Power: Expressing**
**Wavelets via the S6 Layer**


We begin by analyzing the expressivity of the S6 layer and
the power of its input-dependent discretization in terms of
function approximation capabilities. We prove that an S6
layer can approximate projections onto wavelets arbitrarily well (Thm. 1), while an S4D layer can at best project
onto Fourier bases. Consequently, in approximating target
functions with discontinuities, S6 achieves a faster approximation rate than S4D (Cor. 1). The advantage of S6 over
S4D in approximating discontinuous functions translates
into their performance differences in memorization tasks.


**Linear RNNs as Time-Projection Onto Basis Functions**
The idea that Linear RNNs could perform projections onto
specific sets of basis functions is not new, and indeed served
as theoretical grounding for the HiPPO work (Gu et al.,
2020). However, to our knowledge, this interpretation has
not yet been leveraged to explain the capabilities of modern
Linear RNNs. In what follows, we take such interpretation
to analyze their expressivity. To streamline the analysis and
slim notation, we focus on 1D inputs and view them as
continuous signals, _**x**_ _s ≡_ _x_ ( _s_ ) _∈_ R, _s ∈_ [0 _, T_ ], without
loss of generality. For direct comparison with S4D, we consider a simplified version of S6 where the input-dependence
only affects the state matrix via ∆( _x_ ), and does _not_ af


The discretized SSM parameters are usually obtained following a Zeroth-Order Hold (ZOH) scheme (Toth et al.´, 2008).
Given a discrete time-step ∆( _t_ ) _∈_ R [+], ZOH prescribes


**Λ** _t_ = _e_ **[Λ]** [(] _[t]_ [)∆(] _[t]_ [)] _,_ _**C**_ _t_ = _**C**_ ( _t_ ) _,_

(5)
_**B**_ _t_ = ( **Λ** ( _t_ )∆( _t_ )) _[−]_ [1] ( _e_ **[Λ]** [(] _[t]_ [)∆(] _[t]_ [)] _−_ _I_ )( _**B**_ ( _t_ )∆( _t_ )) _._


Note that in the original Mamba formulation the authors use
Forward Euler for _**B**_ ( _t_ ) for simplicity, _**B**_ _t_ = _**B**_ ( _t_ )∆( _t_ ).


In general, there is some flexibility in the choice of the functional form that the system parameters can take. The main
requirements are that: (i) **Λ** _t_ is diagonal(-izable), so that
the product [�] _r_ _[t]_ = _s_ +1 **[Λ]** _[r]_ [in (][4][) can be computed efficiently;]
(ii) the eigenvalues of **Λ** _t_ are bounded in [ _−_ 1 _,_ 1], to ensure
stability; and that (iii) the SSM parameters do _not_ depend
on the state _**h**_ ( _t_ ), so to keep the system linear in _**h**_ ( _t_ ), and
allow to compute its solution in parallel along _t_ . The most
relevant choices analyzed in this paper are: S4D (Gu et al.,
2022a), which models linear time-invariant dynamics,


∆( _t_ ) := 1 _,_ **Λ** ( _t_ ) := **Λ** _,_ _**B**_ ( _t_ ) := _**B**_ _,_ _**C**_ ( _t_ ) := _**C**_ ; (6)



3


fect _**B**_ ( _xt_ ) (i.e., _**B**_ ( _xt_ ) = [ _B_ 1 _, . . ., BN_ ] _[⊤]_ independent of
the input _xt_ ). Substituting this into (2), and recalling that
**Λ** = _−_ diag([ _λ_ 1 _, . . ., λN_ ]), each component _n_ = 1 _, . . ., N_
of the hidden state can be evaluated separately as an inner
product between time-dependent functions:

_h_ [M] _n_ [(] _[t]_ [) =]  - _t_ _e_ _[−][λ][n]_  - _st_ [∆(] _[x][r]_ [)] _[ dr]_ _Bn_ _x_ ( _s_ ) _ds_ =  - _gn_ [M] _[, x]_  - _._ (8)

0 ~~�~~ ~~��~~ ~~�~~
=: _gn_ [M] ( _s_ ; _t,x_ )


We refer to _gn_ [M][(] _[s]_ [;] _[ t, x]_ [)][ as the] _[ Mamba basis function]_ [, with]
the notation emphasizing its general dependency on the
input signal _x_ up to time _t_ . For ease of comparison, we can
recover an analogous formula to (8) also for S4D, by letting
∆( _xt_ ) _≡_ 1 _∀t_ (see also (6)). This gives


     - _t_
_h_ [S4D] _n_ ( _t_ ) = _e_ _[−][λ][n]_ [(] _[t][−][s]_ [)] _Bn_ _x_ ( _s_ ) _ds_ =  - _gn_ [S4D] _, x_  - _._ (9)

0        - ~~��~~        =: _gn_ [S4D][(] _[s]_ [;] _[t]_ [)]


As we can see, S4D can provide only exponentials as basis
functions. The approximation properties of these functions
are limited: this is established in the literature, and ties back
to the theory of Vandermonde matrices (Gautschi & Inglese,
1987), as recently pointed out by Orvieto et al. (2024). Their
poor performance are mainly due to: (i) the stability constraint, Re( _−λn_ ) _≤_ 0, which causes an exponentially fast
decay to 0, _de-facto_ limiting the effective support of said
bases; (ii) the large degree of overlap between different
bases (obtained by varying the only free parameter _λn_ ). The
only way to curb these negative effects is by pushing the
eigenvalues to be equispaced onto the complex unit disk,
namely _e_ _[−][λ][n]_ _→_ _e_ _[i]_ [2] _N_ _[πn]_ . This is precisely the strategy rec
ommended by Orvieto et al. (2024), however it reduces the
application of the S4D layer to simply performing a Fourier
transform. In contrast, the additional flexibility provided by
the input-selectivity in Mamba allows for a much richer variety of basis functions (8) to be employed in the projection,
an example of which is shown next.


**Mamba Bases Can Represent Haar Wavelets** Here we
provide the main theoretical result in this section, namely
that the Mamba S6 layer can perform projections onto Haar
wavelets. Due to their ability to capture local aspects of a
function such as spikes and discontinuities, wavelets are
generally better suited than Fourier bases in solving certain signal processing tasks, (e.g., needles-in-the-haystack
(Kamradt, 2023), transient signals (Mallat, 2012)). Recall
the Haar wavelets are defined by dilation and translation,



12 [)][(] _[s]_ [)] _[ −]_ [1] [[] [1] 2



_ψ_ 0 _,_ 0( _s_ ) = _ψ_ ( _s_ ) := 1 [0 _,_ 1




[1] 2 _[,]_ [1]][(] _[s]_ [)]



(10)
_ψj,k_ ( _s_ ) := 2 _[j/]_ [2] _ψ_ (2 _[j]_ _s −_ _k_ ) _,_



**Theorem 1.** _Consider a Haar wavelet ψj,k_ : [0 _,_ 1] _→_ R _,_
_and the Mamba basis function (8) at t_ = 1 _, gj,k_ _[M]_ [(] _[s]_ [; 1] _[, x]_ [) =]

    - 1
_e_ _[−][λ][j,k]_ _s_ [∆] _[j,k]_ [(] _[x][r]_ [)] _[dr]_ _Bj,k. Let_ ˜ _xs_ := concat[ _xs_ ; _s_ ] _be the in-_
_put signal augmented with time positional encoding. For any_
_ϵ >_ 0 _, there exist_ 3 _Mamba basis functions gj,k_ _[M]_ [1] _[, g]_ _j,k_ _[M]_ [2] _[, g]_ _j,k_ _[M]_ [3]
_such that the approximation error_

    ��� _ψj,k_ ( _s_ ) _−_ _gj,k_ _[M]_ [1] [(] _[s]_ [; 1] _[,]_ [ ˜] _[x]_ [) +] _[ g]_ _j,k_ _[M]_ [3] [(] _[s]_ [; 1] _[,]_ [ ˜] _[x]_ [)] _[ −]_ [2] _[g]_ _j,k_ _[M]_ [2] [(] _[s]_ [; 1] _[,]_ [ ˜] _[x]_ [)] ����


_is smaller than ϵ, ∀s ∈_ [0 _,_ 1] _._


The proof relies on tweaking the input-dependent discretization ∆( _s_ ) to output ∆( _s_ ) _→∞_ or ∆( _s_ ) _→_ 0 (note that ∆
can directly depend on the time variable _s_ instead of the
input signal _xs_, due to time Positional Encoding (PE)). This
effectively allows Mamba to represent Heaviside functions
as bases: by linearly combining shifted Heaviside bases, one
can immediately recover the required Haar wavelets (see
Fig. A.1, middle-right subplots). The details of the proof are
reported in App. A.2, where we also proceed to relax the
inclusion of PE as an assumption for Thm. 1.


Theorem 1 translates into practical advantages of Mamba
over S4D, as Haar wavelets are much better than Fourier
bases for approximating _discontinuous_ functions common
in practice. This is formalized in the following corollary.


**Corollary 1.** _For a piecewise-constant function ρ_ ( _t_ ) _with_
_m ≥_ 1 _discontinuities, there exist N Mamba basis functions_
_(8) such that the L_ [2] _approximation error ∥ρ −_ [�] _[N]_ _n_ =1 _[g]_ _n_ _[M][∥]_ _L_ [2]
_is of order O_ (2 _[−]_ 3 _[N]_ _m_ ) _. On the other hand, S4D basis func-_

_tions can achieve an approximation error of O_ ( _N_ _[−]_ [1] ) _._


Corollary 1 stems from Thm. 1, and from approximation
results using Haar wavelets and Fourier bases available in
the literature (Vetterli, 2001; Eckhoff, 1993); see proof in
App. A.2. In the following, we illustrate how approximating
wavelets translates into advantages on concrete tasks.


**Task KEEP** _n_ **-TH** The goal of the KEEP _n_ -TH task is to
recover the _n_ -th element in a randomly-generated sequence,
_yt_ = _xn_ . This generalizes the KEEP FIRST task in (Chiang
& Cholak, 2022) where _n_ = 1. The solution of KEEP _n_ -TH
can be directly represented by combining the projections
of a piecewise-constant signal _x_ ( _s_ ) = [�] _i_ _[t]_ =1 _[x][i]_ **[1]** [[] _[i][−]_ [1] _[,i]_ [)][(] _[s]_ [)]
onto two Heaviside functions _H_ ( _s −_ _n_ ) _, H_ ( _s −_ ( _n −_ 1)).
As we have shown in Thm. 1, one S6 layer in Mamba can
reproduce precisely this type of projections, provided the
input is augmented with time-positional information. Thus,
we arrive at the following Corollary:


**Corollary 2.** _There exists an S6 layer that solves_ KEEP
_n_ -TH _on input augmented with time Positional Encoding._


The proof of Cor. 2 is reported in App. A.3; the results in
Tab. 1 verify empirically that Mamba with PE can perfectly



with _j ∈_ N denoting the dilation scale, and _k_ = 0 _, . . .,_ 2 _[j]_ _−_ 1
the translation. Higher-order wavelets correspond to localized and “spiky” bases; see Fig. A.1 (left) for an illustration.



4


_Table 1._ KEEP FIFTH experimental results. Average accuracy
across 3 seeds with _T_ = 50 and _|V |_ = 128. Standard error across
3 seeds is 0.00 for all models. Mamba and S4D models consist of
_embedding_, _SSM_ ( _**h**_ _∈_ R [8] _[×]_ [32] ), and _linear_ layers (without convolution and gating, see Tab. 2). Positional Encoding (PE) encodes
the position in the last element of the embedding, resulting in _|V |_
fewer parameters.


MAMBA+PE MAMBA S4D S4D+PE TRANSFORMER


Accuracy _↑_ **1.00** 0.08 0.09 0.08 **1.0**
Parameters 9.2k 9.3k 8.8k 8.7k 6.4k


solve the task (same as Transformers), whereas Mamba
without PE and S4D both fail, highlighting the advantage of
Mamba over S4D in approximating discontinuous functions
in practice. Additional ablations on model size, sequence
length, and the role of PE are reported in App. D.2.


**4.2. Long-Range Modelling: Sensitivity Analysis**


In this section, we examine the long-range memorization
capacity of SSM layers, by performing a sensitivity analysis
of the layer output with respect to changes to the input, as
the sequence length increases. To this end, we analyze the
derivative of the SSM hidden state at time _t_ with respect
to the past input at time _j_, _|_ _∂x_ _[∂h]_ _j_ _[t]_ _[|]_ [. We argue that preserving]

sensitivity (i.e., a non-zero derivative) is _necessary_ for memorization: if the past input has no impact on the current state,
one cannot hope for any information about it to be retained.
With Lem. 1, we show how generally the sensitivity of an
S6 layer decays exponentially fast, similarly to S4D. Thanks
to input selectivity, however, the S6 layer can adjust the rate
of this decay, thus dynamically tweaking the amount of information to retain. In Lem. 2 we illustrate this mechanism,
which proves to be useful for solving the task in Sec. 5.2.


For simplicity, we consider 1D inputs _xt ∈_ R. Given a
generic, input-dependent recurrence relationship as in (4),
we show in App. B that the sensitivity of the state with
respect to its inputs at the _j_ -th instant _xj ∈_ R is given by



_where_ ˜ _c_ (∆ _, λn, Bn, x≤j_ ) _depends on the input subsequence_
_x≤j, independent of the sequence length t._


Lemma 1 shows that both S4D and S6 have exponential
decay of sensitivity when the sequence length _t_ increases.
For S4D, the only mitigation strategy is to set _λn →_ 0
(and thus _e_ _[λ][n]_ _→_ 1). While S6 can implement the same
strategy, it can also counteract the decay by adapting the
input-dependent discretization, as formalized in Lem. 2.


**Lemma 2.** _Consider the discrete-time S6 in (7) where_
_**B**_ _t_ = [ _B_ 1( _xt_ ) _, . . ., Bn_ ( _xt_ )] _[⊤]_ _∈_ R _[N]_ _. Suppose there exists_
_a constant c ≥_ 0 _such that_



lim
_t→∞_ _[λ][n]_



_t_

- ∆( _xr_ ) _≤_ _c._ (12)


_r_ =1



_Then the sensitivity of the n-th component of the state at_
_time t with respect to any input xj is lower bounded by_



_≥_ _e−c_ _∂_ _Bn_ ( _xs_ ) ∆( _xs_ ) _xs_
���� ���� _∂xj_



_._ (13)
����



lim
_t→∞_



_∂h_ _[M]_ _t_
���� _∂xj_



_∂_ _**h**_ _t_ _∂_
=
_∂xj_ _∂xj_



�� _t_ �� _t_ _s_ =1 _r_ = _s_ +1 **[Λ]** _[r]_




  _**B**_ _sxs_


  _**B**_ _sxs_ _._



�� _t_ �� _∂_
= _r_ = _j_ +1 **[Λ]** _[r]_ _∂xj_ [(] _**[B]**_ _[j][x][j]_ [)]

+ _[∂]_ _∂x_ **[Λ]** _j_ _[j]_ - _js−_ =11 �� _jr−_ =1 _s_ +1 **[Λ]** _[r]_ 


(11)



This implies that, to retain sensitivity for longer sequences,
Mamba must necessarily push _λ_ ∆( _xt_ ) _→_ 0 (or equivalently,
_e_ _[−][λ]_ [∆(] _[x][t]_ [)] _→_ 1). We illustrate this with the KEEP _n_ -TH task
shown in Fig. 1, where we report the distribution of the
learned parameters _e_ _[−][λ]_ [∆(] _**[x]**_ _[t]_ [)] as we increase the sequence
length. The shift towards 1 appears clear, validating the
condition discussed in Lem. 2.


**Remark 1.** _While in this section we mainly focus on the_
_properties of the_ original _Mamba mixer layer (Gu & Dao,_
_2023), we note that the results proven in Thm. 1, Lem. 1 and_
_Lem. 2 hold analogously for the Mamba-2 mixer (Dao & Gu,_
_2024). We remind that the Mamba-2 mixer layer can be seen_
_as a simplification of Mamba’s, whereby the state matrix_
_is parameterized by a single scalar,_ **Λ** = _λ_ _**I**_ _, rather than_
_its full diagonal. Nonetheless, we do not rely on Mamba’s_
_additional flexibility for our derivations; choosing a suitable_
_scalar λ suffices (see details in App. A.2)._


**5. Full Mamba Architecture Analysis**


In this section, we describe the full Mamba architecture and
study how its SSM mixer coordinates with the other components in the model to efficiently solve associative-recall
tasks. A full Mamba architecture includes an embedding
layer, a number of Mamba mixer blocks, and an output layer.
The mixer block is further composed of a short convolution,
an SSM, and a gate — we refer to Tab. 2 for details in the
differences between Mamba and Mamba-2, but point out
that Mamba-2 leverages _three independent_ short convolutions (rather than a _single common_ one) to compute its SSM
parameters, as outlined in (14b).



**Lemma 1.** _Consider the hidden states arising from the_
_S4D and S6 SSMs defined in (6) and (7). The sensitivity of_
_the n-th component of their states at time t with respect to_
_the input at time j ≪_ _t is given by, respectively,_

_∂h_ _[S4D]_ _t_ = ˜ _c_ ( _λn, Bn, x≤j_ ) _e−λn_ ( _t−_ ( _j_ +1)) _,_
���� _∂xj_ ����




_[S4D]_ _t_ = ˜ _c_ ( _λn, Bn, x≤j_ ) _e−λn_ ( _t−_ ( _j_ +1)) _,_
_∂xj_ ����

_∂h_ _[M]_ _t_ = ˜ _c_ (∆ _, λn, Bn, x≤j_ ) _e−λn_ - _tr_ = _j_
���� _∂xj_ ����



= ˜ _c_ (∆ _, λn, Bn, x≤j_ ) _e−λn_ - _tr_ = _j_ +1 [∆(] _[x][r]_ [)] _,_
����



5


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||
||||||||||||||||||||


Sequence length _T_




|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
||||||||||



_Figure 1._ Distribution of _e_ _[−][λ]_ [∆] _[t]_ computed on test inputs by models trained to successfully solve KEEP _n_ -TH tasks for various sequence
lengths _T_ (same setup as Tab. 1). The left histogram confirms that the Mamba model must push _e_ _[−][λ]_ [∆] _[t]_ _→_ 1 as _T_ increases, as implied
by Lem. 2, to retain information throughout the sequence. Also the S4D must behave similarly. Meanwhile, the right histogram shows
that, compared to S4D, Mamba has additional flexibility in forgetting irrelevant information ( _e_ _[−][λ]_ [∆] _[t]_ is mostly 0 before timestep _n_ ) and
memorizing information selectively ( _e_ _[−][λ]_ [∆] _[t]_ is mostly 1 from timestep _n_ onwards), while S4D is forced to memorize indiscriminately.


_Table 2._ Comparison of Mamba (including S4D as a special case) and Mamba-2 (single-head) mixers. We denote with _⊙_ the Hadamard
(elementwise) product, _⊗_ the Kronecker (outer) product, conv( _**x**_ )[ _t_ ] the _t_ -th output of the convolution, and _σ_ the pointwise nonlinearity.


MAMBA MAMBA-2


**Λ** _∈_ R _[d][×][N]_ _,_ _**x**_ _t ∈_ R _[d]_ _,_ _**h**_ _t ∈_ R _[d][×][N]_ **Λ** = _λ ∈_ R _,_ _**x**_ _t ∈_ R _[d]_ _,_ _**h**_ _t ∈_ R _[d][×][N]_ (14a)



_**x**_ ˆ _t_ = _σ_ (conv( _**x**_ )[ _t_ ]) _∈_ R _[d]_ _**x**_ ˆ _t_ = _σ_ (conv _u_ (Linear( _**x**_ ))[ _t_ ]) _∈_ R _[d]_ (14b)
∆ _t_ = SoftPlus(Linear(ˆ _**x**_ _t_ )) _∈_ R _[d]_ (∆ [S4D] _t_ = **1** ) ∆ _t_ = SoftPlus(Linear( _**x**_ _t_ )) _∈_ R (14c)

_**B**_ _t_ = Linear(ˆ _**x**_ _t_ ) _∈_ R _[N]_ ( _**B**_ _t_ [S4D] = _**B**_ ) _**B**_ _t_ = _σ_ (conv _B_ (Linear( _**x**_ ))[ _t_ ]) _∈_ R _[N]_ (14d)

_**C**_ _t_ = Linear(ˆ _**x**_ _t_ ) _∈_ R _[N]_ ( _**C**_ _t_ [S4D] = _**C**_ ) _**C**_ _t_ = _σ_ (conv _C_ (Linear( _**x**_ ))[ _t_ ]) _∈_ R _[N]_ (14e)


_**h**_ _t_ = _e_ **[Λ]** _[⊙]_ [(∆] _[t][⊗]_ **[1]** _[N]_ [ )] _⊙_ _**h**_ _t−_ 1 + (∆ _t ⊙_ _**x**_ ˆ _t_ ) _⊗_ _**B**_ _t_ _**h**_ _t_ = _e_ _[λ]_ [∆] _[t]_ _**h**_ _t−_ 1 + (∆ _t_ ˆ _**x**_ _t_ ) _⊗_ _**B**_ _t_ (14f)


_**y**_ _t_ = _**h**_ _t_ _**C**_ _t ∈_ R _[d]_ _,_ _**y**_ ˜ _t_ = _g_ ( _**x**_ _t_ ) _⊙_ _**y**_ _t_ := _σ_ (Linear( _**x**_ _t_ )) _⊙_ _**y**_ _t ∈_ R _[d]_ (14g)



While Mamba and Mamba-2 achieve performance competitive with Transformers and outperform their SSM predecessors in solving MQAR and INDUCTION HEADS, the details
of how this solution can be assembled by the architecture
remain elusive, with only lower bounds on the SSM mixer
size available in the literature (Arora et al., 2024b; Sanford
et al., 2024b), lacking the consideration of other components of Mamba such as convolution and gating. Here we
close this gap by providing analytical constructions for a
1-layer Mamba model that can exactly solve these tasks for
any input. Perhaps counterintuitively, as we prove in Thm. 4,
the components of a single Mamba mixer block are already
powerful enough to solve MQAR exactly, even just using
an S4D mixer layer (replacing the S6). With Thm. 2 and
Thm. 3 we further show how, thanks to the input selectivity
of S6, Mamba and Mamba-2 can leverage leaner mechanisms to solve MQAR. Particularly, the S6 layer can use _**B**_ _t_
and _**C**_ _t_ to efficiently structure information within its hidden
state, and retrieve it when required. Finally, we show how
the ability to structure the hidden state (used for MQAR)
can be combined with the capacity to dynamically adjust
the rate of memory decay (investigated for KEEP _n_ -TH) to
exactly solve INDUCTION HEADS with a variant of S6. We
name this variant Mamba-∆ _[⊤]_, and discuss it in Sec. 5.2.



We remark that the constructions described in this section
are just _possible_ solutions that the Mamba architectures
can implement, and we do not exclude the existence of
alternative ones. Nonetheless, in Fig. 2 we verify empirically
that our solutions are tight in terms of model size.


**5.1.** 1 **-Layer Mamba Can Solve MQAR**


The MQAR task (Arora et al., 2024a) prescribes input and
output sequences as follows


_**x**_ = [ _k_ 1 _, v_ 1 _, . . ., kκ, vκ,_ _|_ _. . ., ki_ 1 _, . . ., kiκ, . . ._ ] _,_

~~�~~    - ~~�~~    -    - ~~��~~    _κ_ key-value pairs shuffled keys, interwoven with noise


_**y**_ = [ _×, ×, . . ., ×, ×,_ _|_ _. . ., vi_ 1 _, . . ., vij_ _, . . ._ ] _._


The keys _ki_ are randomly chosen from a key set of size _κ_,
whereas the values _vi_ and the noise are randomly taken from
a vocabulary of size _|V |_ . The goal is to correctly predict the
value associated with the corresponding key at the query
positions, while other non-query positions (denoted with _×_ )
are ignored.


In this section we prove that the MQAR task can be exactly solved by three architectures: vanilla Mamba (Thm. 2),
Mamba-2 (Thm. 3), and Mamba with an S4D mixer
(Thm. 4), which we refer to as Mamba-S4D. Next we pro


6


_Table 3._ Overview of exact solutions to the MQAR task that can be implemented by the Mamba model with S4D-mixer in Thm. 4 (top),
Mamba-mixer in Thm. 2 (middle), and Mamba-2-mixer in Thm. 3 (bottom). While S4D-mixer lacks input selectivity in its SSM layer, it
can solve MQAR via the gated-convolution mechanism on a larger embedding space (top). By contrast, both Mamba and Mamba-2 can
solve MQAR with the same selective SSM layer construction _without gating_, and differ on the choice of convolutions (middle, bottom).


S4D ! Embedding ! Convolution ! SSM ! Gate















































































































MAMBA ! Embedding ! Convolution ! SSM










































































































































































|B( −)<br>⊙ 3··… ⊙ 1<br>Ω··· Ω<br>···· ·<br>···· ·|Col2|B( −)<br>··… ⊙<br>···<br>···<br>···|B( −)<br>·3·… ⊙<br>Ω···<br>·Ω··<br>····|Col5|
|---|---|---|---|---|
|⊙|⊙|⊙|⊙|⊙|
|·<br>·<br>·<br>·<br>·<br>·<br>·<br>·|·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>1<br>·<br>+|·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>_h_<br>·<br>·<br>·<br>1<br>·<br>·<br>·<br>·<br>+|·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>_h_<br>·<br>·<br>·<br>1<br>·<br>·<br>·<br>·<br>+|·<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br><br>·<br>·<br>·<br>1<br>·<br>·<br>1<br>·<br>+|


|̂x9|Col2|
|---|---|
|3<br>·<br>·<br>…<br><br>⊙<br><br>·<br>·<br>Ω<br><br>·<br>Ω<br><br>·<br>·<br>·<br><br>·<br>·<br>·<br>·<br><br>_B_( −)|3<br>·<br>·<br>…<br><br>⊙<br><br>·<br>·<br>Ω<br><br>·<br>Ω<br><br>·<br>·<br>·<br><br>·<br>·<br>·<br>·<br><br>_B_( −)|
|·<br>·<br>·<br>1<br>·<br>·<br>1<br>·<br>+|·<br>·<br>·<br>·<br>·<br>·<br>·<br>·|


|x9̂ h9<br>C( −)<br>Ω···<br>·Ω··<br>····|Col2|
|---|---|
|·<br>·<br>1<br>·<br>·|·<br>·<br>·<br>·<br>1<br>·<br>·<br>·<br>·<br>·<br>·<br>·<br>3<br>·<br>…<br>−<br>_y_|





















































































































































































































tracting key-value pairs and removing non-informative ones
(e.g., value-key, value-value). The SSM layer (S6) organizes
the hidden state _matrix_ (14f) via _**B**_ _t_ (14d) such that each
column corresponds to a specific key, and holds the value
embedding associated with said key (hence the state size
_N_ = _κ_ ). At query time, _**C**_ _t_ (14e) uses the query(=key)
embedding to retrieve the desired value from the correct
column in the hidden state matrix.


**Theorem 3.** _There exists a_ 1 _-layer Mamba-2 model with-_
_out gating that solves_ MQAR _with κ pairs using embedding_
_size d_ = _O_ (log _κ_ + log _|V |_ ) _, and state size N_ = log _κ._



_augmented Transformers can solve_ MQAR _. Indeed, in light_
_of the convolution layer in Tab. 2, we can interpret Mamba-2_
_as a convolution-augmented subquadratic Transformer._


**Remark 3.** _With the results from Thm. 2 and 3, we can_
_infer that the extra convolutions included in the Mamba-_
_2 layer over vanilla Mamba in general allow for a more_
_parameter-efficient solution of MQAR, by noting that d_ =
log _κ_ + log _|V | < κ_ + log _|V |, and N_ = log _κ < κ._


**Theorem 4.** _There exists a_ 1 _-layer Mamba model with an_
_S4D mixer that solves_ MQAR _with κ pairs, using embed-_
_ding size d_ = _O_ ( _κ_ log _|V |_ ) _, and state size N_ = 1 _._



7


MAMBA AND MAMBA-∆ _[⊤]_


Vocabulary size ( _|V |_ )





S4D MAMBA MAMBA-2









Number of keys ( _κ_ )





_Figure 2._ Trained models accuracy on MQAR task (best of 7 seeds), varying _κ_ and _d_ . For
S4D _N_ = 4, for Mamba _N_ = 2 _κ_, and for Mamba-2 _N_ = 8 ln _κ_ . We use _T_ = 100 and
_|V |_ = 128 for all runs. The theoretical bounds on model size for assembling the solutions
proposed in Thm. 2 to 4 (black lines) separate reasonably well models that can achieve
100% accuracy (above black lines) from those that do not (below). In terms of model size
efficiency, Mamba-2 is better than Mamba, which in turn is better than S4D.



_Figure 3._ Trained models accuracy on INDUC
TION HEADS task (best of 5 seeds), varying
_|V |_ and _d_, _N_ = 4 _|V |_ . Mamba-∆ _[⊤]_ ’s performance (outlined) is equal or better than
Mamba’s (filled) and only hits 100% above
the theoretical bound from Lem. 3 (black).



_Proof sketch._ We first present the construction using _d_ =
_O_ ( _κ|V |_ ), and again apply JL Lemma to reduce to _d_ =
_O_ ( _κ_ log _|V |_ ). To overcome the limitation given by the lack
of input selectivity in the SSM layer, we organize the hidden
state along the embedding dimension only, and partition it
so that each chunk holds the value associated with a specific
key. At query time, the retrieval of the desired key-value
chunk is done by leveraging the gating layer.


**Remark 4.** _Our construction of the S4D-mixer relies both_
_on gated convolution and the SSM recurrence. Arora et al._
_(2024a) provided a construction to solve_ MQAR _based on_
_gated convolution only. In contrast, our construction relies_
_on size-_ 2 _kernels only, thanks to the SSM recurrence._


To validate our theoretical constructions, we train Mamba,
Mamba-2, and Mamba-S4D models with varying embedding size _d_, on MQAR tasks with different number of keyvalue pairs _κ_ . The goal is to check how tight in practice are
the theoretical bounds on model dimension derived above,
and whether indeed trained models must respect them to
solve the tasks. Results are reported in Fig. 2, where dashed
curves denote our theoretical bounds and markers indicate
empirical results. Notice our bounds in Thm. 2 to 4 are close
to the empirical threshold between models sizes that can
recover exact solutions or not, illustrating the tightness of
our theoretical results. See additional details in App. D.3.


**5.2.** 1 **-Layer Mamba-** ∆ _[⊤]_ **Can Solve INDUCTION HEADS**


The INDUCTION HEADS task was first introduced by Olsson et al. (2022) to study Associative Recall capabilities
in Transformers. Here we use the formulation from Sanford et al. (2024a): given an input sequence of tokens

[ _x_ 1 _, . . ., xt_ ] from a finite vocabulary _xt ∈_ _V_, the goal
is to report, for each _xt_, the token coming immediately
after the latest previous occurrence in the input of token



_xi_ . That is, the output _yt_ must be _yt_ = _xj_ ( _t_ )+1 where
_j_ ( _t_ ) = max _{j_ : _j < t, xj_ = _xt}_ (or a “blank” token,
_yi_ = _×_, if _xt_ appears for the first time).


Note the INDUCTION HEADS task is similar to MQAR,
but with two significant differences. (i) There is no logical
distinction between keys and values, so the model needs to
identify the role of each token, but this can be handled by
the short convolution, as we will show. More importantly,
(ii) we need to retain information about only the _latest_ previous occurrence of a token, so the model should dynamically
forget and remember information pertaining different tokens. Since the latter memorization ability was investigated
in Sec. 4.2, it is natural to leverage those insights in our
solution. To do so efficiently, we introduce a slight variation
to the S6 layer: the Mamba-∆ _[⊤]_ SSM mixer, prescribing the
following hidden state evolution


_**h**_ _t_ = _e_ **[Λ]** _[⊙]_ [(] **[1]** _[d][⊗]_ [∆(ˆ] _**[x]**_ _[t]_ [))] _⊙_ _**h**_ _t−_ 1 + ˆ _**x**_ _t ⊗_ _**B**_ _t._ (15)


Comparing this to (14f), the only difference lies in the action
of ∆(ˆ _**x**_ _t_ ), which now varies along the _state_ dimension _N_,
rather than the _embedding_ dimension _d_ . While Gu & Dao
(2023) hypothesized a similar performance for both versions,
our findings reveal that the dependence along state dimension is better suited to solving the INDUCTION HEADS task:


**Lemma 3.** _There exists a_ 1 _-layer Mamba model with_
_the Mamba-_ ∆ _[⊤]_ _SSM mixer (15) that solves_ INDUCTION
HEADS _with vocabulary V using embedding size d_ = 2 _|V |_
_and state size N_ = _|V |._


_Proof sketch._ The proof follows closely the MQAR construction, in that we leverage the matrix structure in the
hidden state such that its columns are indexed by the keys
and store the associated values. In the INDUCTION HEADS
task, though, each token _xi_ acts as key in the ( _xi, xi_ +1) pair,
and as value in the ( _xi−_ 1 _, xi_ ) pair. To handle this distinction,



8


we simply duplicate the embedding, and let the convolution
layer correctly combine tokens information pairwise, so
that after convolution each token encapsulates information
both regarding a value and its preceding key. Moreover, we
use ∆( _xt_ ) to selectively erase outdated information, or to
retain currently valid information, depending on the input
observed. Pushing ∆( _xt_ ) _→∞_ flushes a previously memorized value, while ∆( _xt_ ) _→_ 0 preserves it. We refer to
App. C.2 for the detailed proof.


**Remark 5.** _To solve_ INDUCTION HEADS _, Bietti et al._
_(2024); Sanford et al. (2024b) constructed_ 2 _-layer Trans-_
_formers relying on PE and with size scaling as sequence_
_length. On the other hand, we propose a_ 1 _-layer Mamba_
_composing a convolution and a variant SSM layer. Notably,_
_this allows us to drop the PE and thus have the model size_
_depend only on |V |, and_ not _the sequence length, improving_
_upon the constructions for Transformers._


To demonstrate the efficiency of our Mamba-∆ _[⊤]_ variant
on the INDUCTION HEADS task, we compare it against the
Mamba baseline, and report results in Fig. 3. For all model
sizes considered, Mamba-∆ _[⊤]_ performs equally or better,
demonstrating that selectivity along the state dimension (in
the state matrix) improves Mamba’s ability to solve the
INDUCTION HEADS task.


**6. Conclusion and Future Work**


In this work, we demystify the role of input selectivity in
Mamba, showing its impact on approximation power, longterm memory, and associative recall capabilities. We prove
that the S6 layer can efficiently represent discontinuous signals and adaptively mitigate sensitivity decay. We also uncover the role of other architectural components in Mamba,
particularly convolution and gating. We present a mechanistic explanation of how Mamba solves memorization and
associative recall tasks, with tight theoretical model size
bounds matching empirical results. Our findings reveal opportunities to further improve Mamba, such as an alternative
way to inject input dependence within the SSM state matrix.


Our current theory does not consider the aspects of optimization and generalization, both of which are interesting future
directions to explore. Moreover, our analysis focuses on simple associative recall tasks; extending it to more complicated
tasks such as _k_ -HOP INDUCTION HEADS (Sanford et al.,
2024b), SEQUENTIAL FUNCTION COMPOSITION (Chen
et al., 2024), and POINTER VALUE RETRIEVAL (Zhang
et al., 2021) would be a natural next step. Overall, our work
and proposed improvements add to the growing understanding of SSMs and could accelerate their development.



**Impact Statement**


The goal of this paper is to improve the understanding of
the Mamba architecture, specifically through analyzing the
role of input selectivity. Our findings contribute to the advancement of State Space Models, which may in turn further
democratize access to Large Language Models, sharpening
both the existing positive and negative aspects of LLMs. No
additional societal impact is expected from this work.


**Acknowledgements**


We thank Pierre Ablin, Samy Bengio, Adam Golinski, Aryo´
Lotfi, Jason Ramapuram, Jonathan Sheaffer (in alphabetical
order) for their helpful feedback and critical discussions
throughout the process of writing this paper. We would also
like to thank Alberto Bietti for pointers to the associative
recall tasks.


**References**


Arora, S., Eyuboglu, S., Timalsina, A., Johnson, I., Poli,
M., Zou, J., Rudra, A., and Re, C. Zoology: Measuring
and Improving Recall in Efficient Language Models. In
_International Conference on Learning Representations_,
2024a.


Arora, S., Eyuboglu, S., Zhang, M., Timalsina, A., Alberti,
S., Zou, J., Rudra, A., and Re, C. Simple Linear Attention
Language Models Balance the Recall-Throughput Tradeoff. In _International Conference on Machine Learning_,
volume 235, pp. 1763–1840, 2024b.


Bietti, A., Cabannes, V., Bouchacourt, D., Jegou, H., and
Bottou, L. Birth of a Transformer: A Memory Viewpoint.
_Advances in Neural Information Processing Systems_, 36,
2024.


Chen, L., Peng, B., and Wu, H. Theoretical Limitations of
Multi-Layer Transformer. arXiv preprint, 2024. URL
[https://arxiv.org/abs/2412.02975.](https://arxiv.org/abs/2412.02975)


Chiang, D. and Cholak, P. Overcoming a Theoretical Limitation of Self-Attention. In _Annual Meeting of the As-_
_sociation for Computational Linguistics_, volume 1, pp.
7654–7664, 2022. doi: 10.18653/v1/2022.acl-long.527.


Cirone, N. M., Orvieto, A., Walker, B., Salvi, C., and Lyons,
T. Theoretical Foundations of Deep Selective State-Space
Models. In _Advances in Neural Information Processing_
_Systems_, volume 37, 2024.


Dahleh, M., Dahleh, M. A., and Verghese, G. Lectures on
Dynamic Systems and Control. MIT OpenCourseWare,
[2011. URL https://ocw.mit.edu/courses/](https://ocw.mit.edu/courses/6-241j-dynamic-systems-and-control-spring-2011/996025f6db0d90b00f11c44fc49b85f9_MIT6_241JS11_textbook.pdf)
[6-241j-dynamic-systems-and-control-s](https://ocw.mit.edu/courses/6-241j-dynamic-systems-and-control-spring-2011/996025f6db0d90b00f11c44fc49b85f9_MIT6_241JS11_textbook.pdf)



[9](https://ocw.mit.edu/courses/6-241j-dynamic-systems-and-control-spring-2011/996025f6db0d90b00f11c44fc49b85f9_MIT6_241JS11_textbook.pdf)


[pring-2011/996025f6db0d90b00f11c44fc4](https://ocw.mit.edu/courses/6-241j-dynamic-systems-and-control-spring-2011/996025f6db0d90b00f11c44fc49b85f9_MIT6_241JS11_textbook.pdf)
[9b85f9_MIT6_241JS11_textbook.pdf.](https://ocw.mit.edu/courses/6-241j-dynamic-systems-and-control-spring-2011/996025f6db0d90b00f11c44fc49b85f9_MIT6_241JS11_textbook.pdf)


Dao, T. and Gu, A. Transformers are SSMs: Generalized
Models and Efficient Algorithms Through Structured
State Space Duality. In _International Conference on Ma-_
_chine Learning_, volume 235, pp. 10041–10071, 2024.


Eckhoff, K. S. Accurate and Efficient Reconstruction of
Discontinuous Functions from Truncated Series Expansions. _Mathematics of Computation_, 61(204):745–763,
1993. doi: 10.1090/S0025-5718-1993-1195430-1.


Gautschi, W. and Inglese, G. Lower Bounds for the Condition Number of Vandermonde Matrices. _Numerische_
_Mathematik_, 52:241–250, 1987. doi: 10.1007/BF013988
78.


Grazzi, R., Siems, J., Franke, J. K., Zela, A., Hutter, F., and
Pontil, M. Unlocking State-Tracking in Linear RNNs
Through Negative Eigenvalues. In _International Confer-_
_ence on Learning Representations_, 2025.


Gu, A. and Dao, T. Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint, 2023.
[URL https://arxiv.org/abs/2312.00752.](https://arxiv.org/abs/2312.00752)


Gu, A., Dao, T., Ermon, S., Rudra, A., and Re, C. HiPPO:´
Recurrent Memory with Optimal Polynomial Projections.
_Advances in Neural Information Processing Systems_, 33:
1474–1487, 2020.


Gu, A., Goel, K., Gupta, A., and Re, C. On the Parameteri-´
zation and Initialization of Diagonal State Space Models.
_Advances in Neural Information Processing Systems_, 35:
35971–35983, 2022a.


Gu, A., Goel, K., and Re, C. Efficiently Modeling Long´
Sequences with Structured State Spaces. In _International_
_Conference on Learning Representations_, 2022b.


Kamradt, G. Needle In A Haystack - Pressure Testing LLMs.
[Github, 2023. URL https://github.com/gkamr](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
[adt/LLMTest_NeedleInAHaystack. [Accessed](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
2025-05-29].


Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In _International Conference_
_on Machine Learning_, pp. 5156–5165, 2020.


Kingma, D. P. and Ba, J. Adam: A Method for Stochastic
[Optimization. arXiv preprint, 2014. URL https://ar](https://arxiv.org/abs/1412.6980)
[xiv.org/abs/1412.6980.](https://arxiv.org/abs/1412.6980)


Li, M., Zhang, X., Huang, Y., and Oymak, S. On the Power
of Convolution Augmented Transformer. In _Conference_
_on Artifical Intelligence_, number 17, pp. 18393–18402,
2025. doi: 10.1609/aaai.v39i17.34024.



Li, Z., Han, J., Weinan, E., and Li, Q. Approximation and
Optimization Theory for Linear Continuous-Time Recurrent Neural Networks. _Journal of Machine Learning_
_Research_, 23(42):1–85, 2022.


Mallat, S. Group Invariant Scattering. _Communications on_
_Pure and Applied Mathematics_, 65(10):1331–1398, 2012.
doi: 10.1002/cpa.21413.


Merrill, W., Petty, J., and Sabharwal, A. The Illusion of
State in State-Space Models. In _International Conference_
_on Machine Learning_, 2024.


Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma,
N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen,
A., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds,
Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J.,
Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark,
J., Kaplan, J., McCandlish, S., and Olah, C. In-context
Learning and Induction Heads. _Transformer Circuits_
_Thread_ [, 2022. URL https://transformer-cir](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)
[cuits.pub/2022/in-context-learning-a](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)
[nd-induction-heads/index.html.](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)


Orvieto, A., De, S., Gulcehre, C., Pascanu, R., and Smith,
S. L. Universality of Linear Recurrences Followed by
Non-linear Projections: Finite-Width Guarantees and Benefits of Complex Eigenvalues. In _International Confer-_
_ence on Machine Learning_, 2024.


Sanford, C., Hsu, D., and Telgarsky, M. One-Layer Transformers Fail to Solve the Induction Heads Task. arXiv
[preprint, 2024a. URL https://arxiv.org/abs/](https://arxiv.org/abs/2408.14332)
[2408.14332.](https://arxiv.org/abs/2408.14332)


Sanford, C., Hsu, D., and Telgarsky, M. Transformers, Parallel Computation, and Logarithmic Depth. In _International_
_Conference on Machine Learning_, 2024b.


Sarrof, Y., Veitsman, Y., and Hahn, M. The Expressive
Capacity of State Space Models: A Formal Language
Perspective. _Advances in Neural Information Processing_
_Systems_, 37:41202–41241, 2024.


Toth, R., Felici, F., Heuberger, P., and Van den Hof, P. Cru-´
cial Aspects of Zero-Order Hold LPV State-Space System Discretization. _IFAC Proceedings Volumes_, 41(2):
4952–4957, 2008.


Vershynin, R. _High-Dimensional Probability: An Intro-_
_duction with Applications in Data Science_ . 2018. doi:
10.1017/9781108231596.


Vetterli, M. Wavelets, Approximation, and Compression.
_IEEE Signal Processing Magazine_, 18(5):59–73, 2001.
doi: 10.1109/79.952805.



10


Wang, S., Li, Z., and Li, Q. Inverse Approximation Theory
for Nonlinear Recurrent Neural Networks. In _Interna-_
_tional Conference on Learning Representations_, 2024.


Zhang, C., Raghu, M., Kleinberg, J., and Bengio, S. Pointer
Value Retrieval: A New Benchmark for Understanding
the Limits of Neural Network Generalization. arXiv
[preprint, 2021. URL https://arxiv.org/abs/](https://arxiv.org/abs/2107.12580)
[2107.12580.](https://arxiv.org/abs/2107.12580)



11


**A. Approximation Power of Mamba**


**A.1. Notation**


We typically use bold upper case _**A**_ _,_ _**B**_ _,_ _**C**_ to denote matrices and bold lower case _**x**_ _,_ _**y**_ to denote vectors or sequence. In
Sec. 3 and Sec. 4, the hidden state at time _t_ is denoted as _**h**_ ( _t_ ) (in the continuous setting) or _**h**_ _t_ (in the discrete setting). In
Sec. 5, with a slight abuse of notation, the hidden state _**h**_ _t ∈_ R _[d][×][N]_ denotes a matrix. We use _**A**_ _, λ_ to denote the discretized
versions of _**A**_ _, λ_ . We use R _,_ N to denote the reals and the natural number. The identity matrix is denoted as _**I**_, where the
all-ones vector is denoted as **1** . We let diag( _**v**_ ) be the diagonal matrix with diagonal filled with the vector _**v**_ . We denote
SoftPlus _,_ ReLU _,_ SiLU as the corresponding pointwise nonlinearity _σ_, and Linear as the linear layer. We let 1 be the
indicator function, _H_ ( _s_ ) be the heaviside function. We denote with _⊙_ the Hadamard (elementwise) product and _⊗_ the
Kronecker (outer) product. We use _d_ for embedding size, _N_ for state size, and _t_ or _T_ for sequence length.


**A.2. Mamba Approximates Haar Wavelets**


In the following, we recall and outline the complete proof for Thm. 1.
**Theorem 1.** _Consider a Haar wavelet ψj,k_ : [0 _,_ 1] _→_ R _, and the Mamba basis function (8) at t_ = 1 _, gj,k_ _[M]_ [(] _[s]_ [; 1] _[, x]_ [) =]

    - 1
_e_ _[−][λ][j,k]_ _s_ [∆] _[j,k]_ [(] _[x][r]_ [)] _[dr]_ _Bj,k. Let_ ˜ _xs_ := concat[ _xs_ ; _s_ ] _be the input signal augmented with time positional encoding. For any_
_ϵ >_ 0 _, there exist_ 3 _Mamba basis functions gj,k_ _[M]_ [1] _[, g]_ _j,k_ _[M]_ [2] _[, g]_ _j,k_ _[M]_ [3] _[such that the approximation error]_

               ��� _ψj,k_ ( _s_ ) _−_ _gj,k_ _[M]_ [1] [(] _[s]_ [; 1] _[,]_ [ ˜] _[x]_ [) +] _[ g]_ _j,k_ _[M]_ [3] [(] _[s]_ [; 1] _[,]_ [ ˜] _[x]_ [)] _[ −]_ [2] _[g]_ _j,k_ _[M]_ [2] [(] _[s]_ [; 1] _[,]_ [ ˜] _[x]_ [)] ����


_is smaller than ϵ, ∀s ∈_ [0 _,_ 1] _._


_Proof._ The Haar wavelet at scale _j_ with translation _k_ is defined on the interval _s ∈_ [0 _,_ 1] as



2 _[j/]_ [2] _s ∈_ [0 _,_ 2 _[−]_ [(] _[j]_ [+1)] )

_−_ 2 _[j/]_ [2] _s ∈_ [2 _[−]_ [(] _[j]_ [+1)] _,_ 2 _[−][j]_ ) _,_ (16)

0 otherwise



_ψj,_ 0( _s_ ) =











2 _[j/]_ [2] _s ∈_ [2 _[−][j]_ _k,_ 2 _[−][j]_ _k_ + 2 _[−]_ [(] _[j]_ [+1)] )

_−_ 2 _[j/]_ [2] _s ∈_ [2 _[−][j]_ _k_ + 2 _[−]_ [(] _[j]_ [+1)] _,_ 2 _[−][j]_ ( _k_ + 1)) _._ (17)

0 otherwise



_ψj,k_ ( _s_ ) = _ψj,_ 0( _s −_ 2 _[−][j]_ _k_ ) =











See also Fig. A.1 for an illustration. Notice that each wavelet can be represented as a linear combination of three shifted
Heaviside functions, namely:


_ψj,k_ ( _s_ ) = 2 2 _j_         - _H_         - _s −_ 2 _[−][j]_ _k_         - _−_ 2 _H_         - _s −_ �2 _[−][j]_ _k_ + 2 _[−]_ [(] _[j]_ [+1)][��] + _H_         - _s −_ 2 _[−][j]_ ( _k_ + 1)� [�] _,_ (18)


where _H_ ( _s_ ) = 1 _s>_ 0 denotes the Heaviside function. The goal is then to show that Mamba can indeed reproduce a shifted
Heaviside as one of its basis functions _gj,k_ [M] [, by opportunely choosing its free parameters] _[ λ][j,k][,]_ [ ∆] _[j,k]_ [ and] _[ B][j,k]_ [ determining]
_gj,k_ [M] [.]


_Figure A.1._ (Left) Example of Haar Wavelets; (Middle-Right) Shape of the 3 Mamba basis _gj,k_ [M][1] _[, g]_ _j,k_ [M][2] _[, g]_ _j,k_ [M][3] [, whose linear combination can]
arbitrarily approximate the Haar wavelet _ψj,k_ .


12


_j_
To this end, we consider a 1D input. We set _λj,k_ = 1 and _Bj,k_ = 2 2, which reduces the Mamba basis function to


_j_                                          - 1
_gj,k_ [M] [(] _[s]_ [; 1] _[, x]_ [) = 2] 2 _e_ _[−]_ _s_ [∆(] _[x]_ [(] _[r]_ [))] _[ dr]_ _._ (19)


We then leave it to the last free parameter ∆( _x_ ( _r_ )) to perform most of the heavy-lifting. By extending the input signal _x_ to
include the absolute time _t_ as an input, we can directly write ∆( _x_ ( _t_ )) = ∆( _t_ ) (this requirement will be relaxed in Prop. 1).
Notice that we can use the Mamba basis function to recover a (scaled) Heaviside centered at any given instant _t_ [ˆ] _k_, provided
∆( _t_ ) behaves as follows:




0 if _s <_ _t_ [ˆ] _k_ _j_
_j_ _→_ 2 2 _H_ ( _s −_ _t_ ˆ _k_ ) _._ (20)
2 2 else



∆( _s_ ) _→_ ∆ [ˆ] _k_ ( _s_ ) =




_∞_ if _s <_ _t_ [ˆ] _k_
= _⇒_ _gj,k_ [M] [(] _[s]_ [; 1] _[, x]_ [) =]
0 else



Remember from (7) that ∆( _s_ ) _≡_ ∆( _s_ ; _b_ ∆ _, w_ ∆) := SoftPlus ( _w_ ∆ _s_ + _b_ ∆). Note that


lim lim (21)
_x→−∞_ [Softplus(] _[x]_ [) = 0] _[,]_ _x→∞_ [Softplus(] _[x]_ [) =] _[ ∞][.]_


Then, by choosing _b_ ∆ = _−w_ ∆ _t_ [ˆ] _k_, and pushing _w_ ∆ _→−∞_, we can get arbitrarily close to approximating ∆ [ˆ] _k_ ( _s_ ). Concretely,
we have
_gj,k_ [M] [(] _[s]_ [; 1] _[, x]_ [) := 2] 2 _j e_ _[−]_              - _s_ 1 [∆][(] _[r]_ [;] _[b]_ [∆][=] _[−][w]_ [∆] _[t]_ [ˆ] _[k][,w]_ [∆][)] _[ dr]_ _−−−−−−→w_ ∆ _→−∞_ 2 2 _j H_              - _s −_ _t_ [ˆ] _k_              - _._ (22)

By substituting _t_ [ˆ] _k ∈{_ 2 _[−][j]_ _k,_ 2 _[−][j]_ _k_ + 2 _[−]_ [(] _[j]_ [+1)] _,_ 2 _[−][j]_ ( _k_ + 1) _}_, the resulting mamba basis functions _gj,k_ [M][1] _[, g]_ _j,k_ [M][2] _[, g]_ _j,k_ [M][3] [can approxi-]
mate arbitrarily well _H_ ( _s −_ 2 _[−][j]_ _k_ ) _, H_ ( _s −_ (2 _[−][j]_ _k_ + 2 _[−]_ [(] _[j]_ [+1)] )) _, H_ ( _s −_ 2 _[−][j]_ ( _k_ + 1)), respectively, which are precisely the
shifted Heaviside functions appearing in (18). This allows us to finally write that _∀ϵ_, _∃w_ ∆ such that

                  ��� _ψj,k_ ( _t_ ) _−_ _gj,k_ [M][1] [+] _[ g]_ _j,k_ [M][3] _[−]_ [2] _[g]_ _j,k_ [M][2] ���� _< ϵ._ (23)


Additionally, to remove the requirement of explicitly augmenting the Mamba input with time positional encoding, we show
that Mamba can autonomously recover said time position by considering a constant input _xt ≡_ 1 _∀t_ . This further relaxes the
assumption in Thm. 1 to using an additional Mamba layer receiving all-ones as input.


**Proposition 1.** _A_ 1 _-layer Mamba can recover absolute time, given an all-ones input._



_Proof._ The proof follows by considering a Mamba layer (8) with _B ≡_ 1 _, λ ≡_ 0. Substituting this into (8), and providing an
all-one input _xt ≡_ 1 _∀t_ gives




   - _t_
_h_ [M] ( _t_ ) =



_x_ ( _s_ ) _ds_ = _t._ (24)
0



This applies analogously to the discrete view, _h_ [M] _t_ [=][ �] _s_ _[t]_ =1 _[x]_ [(] _[s]_ [) =] _[ t.]_


Before proving Cor. 1, we recall the function approximation problem. Given a function _ρ ∈_ _L_ [2] ([0 _,_ 1]) (as a function of time
_t ∈_ [0 _,_ 1]), and a set of bases parameterized by the SSM _{gn}_ (e.g. Mamba or S4D), the goal is to project _ρ_ into the best
size- _N_ basis _GN_ = _{g_ 1 _, . . ., gN_ _}_ that minimizes the approximation error,



arg min _∥ρ −_ proj _GN_ ( _ρ_ ) _∥L_ 2 = arg min
_GN_ _GN_


where _N_ denotes the hidden state size and




- 1


0




- _ρ_ ( _t_ ) _−_ proj _GN_ ( _ρ_ )( _t_ )�2 _dt,_ (25)



proj _GN_ ( _ρ_ ) :=                     - _⟨ρ, gn⟩_ _gn._ (26)

_gn∈GN_


We note that the _N_ basis _GN_ is optimally chosen for the target function _ρ_ (instead of using a fixed set of _N_ bases independent
of _ρ_ ). The inner products _⟨ρ, gn⟩_ = �0 _t_ _[ρ]_ [(] _[t]_ [)] _[g][n]_ [(] _[t]_ [)] _[dt]_ [ for the various] _[ n]_ [ determine the coefficients of the projection of the target]
function _ρ_ onto the basis _GN_ .


We are now ready to prove Cor. 1.


13


**Corollary 1.** _For a piecewise-constant function ρ_ ( _t_ ) _with m ≥_ 1 _discontinuities, there exist N Mamba basis functions (8)_
_such that the L_ [2] _approximation error ∥ρ −_ [�] _[N]_ _n_ =1 _[g]_ _n_ _[M][∥]_ _L_ [2] _[ is of order][ O]_ [(2] _[−]_ 3 _[N]_ _m_ ) _. On the other hand, S4D basis functions can_

_achieve an approximation error of O_ ( _N_ _[−]_ [1] ) _._


_Proof._ As discussed in Sec. 4.1, a S4D basis function (9) can represent any Fourier basis function _fn_ = _e_ _[i]_ [2] _[πn]_ . When
approximating a target function with discontinuities, the Fourier coefficients decay very slowly (Eckhoff, 1993),


lim (27)
_n→∞_ _[|⟨][ρ, f][n][⟩|]_ [ =] _[ O]_ [(] _[n][−]_ [1][)] _[.]_


This implies that using _N_ Fourier bases (and thus _N_ S4D basis functions) for approximating discontinuous functions yields
an approximation error _O_ ( _N_ _[−]_ [1] ), proving the second part of Cor. 1.


By Thm. 1, _N_ Mamba basis functions can approximate arbitrarily close any _N/_ 3 number of Haar wavelets. A set of _N/_ 3
Haar wavelets can achieve an approximation error of _O_ (2 _[−][N/]_ [3] _[m]_ ) (Vetterli, 2001) when targeting a piecewise-constant
function with _m ≥_ 1 discontinuities. We provide a concrete argument for completeness. Suppose the function _ρ_ is piecewise
constant with _m_ = 1 discontinuity. Then using an _optimal_ set of _N_ Haar wavelets _HN ⊂{ψj,k}j∈_ N _,_ 0 _≤k≤_ 2 _j_ _−_ 1, we can
achieve an approximation error of




 - _⟨ρ, ψj,k⟩_ [2] (28)


_ψj,k∈HN_



2 _[j]_ _−_ 1





- _⟨ρ, ψj,k⟩_ [2] _−_ 

_k_ =0 _ψj,k_



min
_HN_ _[∥][ρ][ −]_ [proj] _[H][N]_ [ (] _[ρ]_ [)] _[∥][L]_ [2][ =]


=


=



_∞_



_j_ =0



_∞_

- _⟨ρ, ψj,k_ ( _ρ_ ) _⟩_ [2] (30)


_j_ = _N_



_∞_

- _⟨ρ, ψj,k_ ( _ρ_ ) _⟩_ [2] _−_


_j_ =0



_N_ _−_ 1

- _⟨ρ, ψj,k_ ( _ρ_ ) _⟩_ [2] (29)


_j_ =0



= _O_ (2 _[−][N]_ ) _._ (31)


The second equality holds by noting that _⟨ρ, ψj,k⟩_ = �01 _[ρ]_ [(] _[t]_ [)] _[ψ][j,k]_ [(] _[t]_ [)] _[dt]_ [ = 0][ if] _[ ρ]_ [ is constant in the interval][ [2] _[−][j]_ [(] _[k][ −]_ [1)] _[,]_ [ 2] _[−][j][k]_ []][.]
Thus, for a piecewise-constant _ρ_ with one discontinuity, only one wavelet at each scale _j_ has _⟨ρ, ψj,k_ ( _x_ ) _⟩̸_ = 0. Given that
we can choose adaptively the best _N_ wavelets, we pick the ones with nonzero coefficients across all _J_ = _N_ scales. Then
the approximation error is bounded the coefficient the highest scale, which has magnitude _O_ (2 _[−][N/]_ [2] ) and squared error
_O_ (2 _[−][N]_ ). Similar analysis shows that for piecewise-constant functions with _m_ discontinuities, the optimal adaptive wavelet
basis with _N_ wavelet functions achieves an approximation error of _O_ (2 _[−][N/m]_ ).


**Remark 6.** _While the assumption of a piecewise-constant target function seems restrictive in Cor. 1, we note that this_
_can be relaxed to a continuous target function ρ by additionally assuming the input signals x are piecewise-constant. The_
_equivalence is due to the intermediate value theorem. Note that piecewise-constant input signals are ubiquitous in language_
_modelling tasks, where xt takes values in a finite-size vocabulary._


**A.3. Mamba can solve KEEP** _n_ **-TH**


Here we provide a direct application of the result from App. A.2, to explain the ability of Mamba to exactly solve the KEEP
_n_ -TH task when equipped with Positional Encoding.


**Corollary 2.** _There exists an S6 layer that solves_ KEEP _n_ -TH _on input augmented with time Positional Encoding._


_Proof._ To memorize the _n_ -th position of a piecewise-constant signal _x_ ( _s_ ) = [�] _i_ _[t]_ =1 _[x][i]_ [1] [[] _[i][−]_ [1] _[,i]_ [)][(] _[s]_ [)][, it suffices to represent]
two Heaviside functions _h_ 1 = _H_ ( _s −_ _n_ ) _, h_ 2 = _H_ ( _s −_ ( _n −_ 1)) (recall _H_ ( _s −_ _n_ ) := 1 _s>n_ ), by noting that _h_ 1 _−_ _h_ 2 is one
at the interval [ _n −_ 1 _, n_ ) and zero elsewhere, we have _⟨x, h_ 1 _−_ _h_ 2 _⟩_ = _xn_, as desired.


14


As shown in (22), the Mamba basis function can represent any Heaviside function. Specifically, we can let _w_ ∆ _→−∞_, and



∆1([ _xt_ ; _t_ ]) = SoftPlus( _w_ ∆ _t −_ _w_ ∆ _n_ ) =




_∞_ _t < n_
(32a)
0 _t ≥_ _n_ _[,]_



∆2([ _xt_ ; _t_ ]) = SoftPlus( _w_ ∆ _t −_ _w_ ∆( _n −_ 1)) =




_∞_ _t < n −_ 1
(32b)
0 _t ≥_ _n −_ 1 _[.]_



Suppose _λ ≡_ 1 _, B_ = 1 in the Mamba basis function (8). Using the above ∆1 _,_ ∆2, we can obtain the Mamba basis functions
representing the desired Heavisides,


                         - _t_ _w_ ∆ _→−∞_
_g_ 1 [M][(] _[s]_ [;] _[ t, x]_ [) =] _[ e][−]_ _s_ [∆][1][(] _[x]_ [(] _[r]_ [))] _[ dr]_ _−−−−−−→_ _H_ ( _s −_ _n_ ) _._ (33a)


                         - _t_ _w_ ∆ _→−∞_
_g_ 2 [M][(] _[s]_ [;] _[ t, x]_ [) =] _[ e][−]_ _s_ [∆][2][(] _[x]_ [(] _[r]_ [))] _[ dr]_ _−−−−−−→_ _H_ ( _s −_ ( _n −_ 1)) _._ (33b)


15


**B. Sensitivity of Mamba**


In this section, we prove Lem. 1 and 2. These follow directly from the general sensitivity formula (11), which we derive in
more details below: Given a generic, input-dependent recurrence relationship as in (4), we have that the sensitivity of the
state at time _t_ with respect to its input sequence at the _j_ -th instant _xj ∈_ R is given by







**Λ** _r_


 






_∂_ _**h**_ _t_ _∂_
=
_∂xj_ _∂xj_




- _t_

 

_s_ =1




- _t_

 

_r_ = _s_ +1




- _t_

 

_r_ = _s_ +1


- _t_

 

_r_ = _s_ +1







   
- _**B**_ _sxs_ 



_∂_
_∂xj_






=


=


=


=



_t_



_s_ =1


_t_



_s_ =1



 _t_

 

_r_ = _j_ +1


 _t_

 

_r_ = _j_ +1



**Λ** _r_


**Λ** _r_















 _**[B]**_ _[s][x][s][δ][s<j]_ [ +]



_**B**_ _sxs_


_**B**_ _sxs_ +



**Λ** _r_




_∂_
_∂xj_



**Λ** _r_




- _t_

 

_r_ = _s_ +1






(34)



_∂_ **Λ** _j_
 _∂xj_









_r_ = _s_ +1 _,_
_r_ = _j_



_t_




**Λ** _r_


**Λ** _r_



_**B**_ _sxs_ 










_∂_
_∂xj_














**Λ** _r_









_r_ = _s_ +1 _,_
_r_ = _j_



 _[∂]_

_∂xj_



_t_








_**B**_ _jxj_ - +



_j−_ 1



_s_ =1



_∂_ **Λ** _j_

_∂xj_




 



_∂_
_∂xj_





 _**[B]**_ _[s][x][s]_


 


_j−_ 1 - _j−_ 1

- 

_s_ =1 _r_ = _s_ +1







_**B**_ _jxj_ - + _[∂]_ **[Λ]** _[j]_

_∂xj_



**Λ** _r_



_**B**_ _sxs_



_._



Since **Λ** _r_ = _−_ diag([ _λ_ 1( _r_ ) _, . . ., λn_ ( _r_ )]) is diagonal and _**B**_ _j_ = [ _B_ 1( _j_ ) _, . . ., Bn_ ( _j_ )], we obtain from (34) the sensitivity of
the _n_ -th component of the hidden state _**h**_ _t,n ≡_ _ht_ with respect to the input _xj_ as








- _j−_ 1

 

_r_ = _s_ +1



_j−_ 1



_s_ =1




  
_λn_ ( _r_ )







_∂ht_
=
_∂xj_



 _t_

 

_r_ = _j_ +1




_∂_
( _Bn_ ( _j_ ) _xj_ ) + _[∂λ][n]_ [(] _[j]_ [)]
_∂xj_ _∂xj_



_λn_ ( _r_ )



_Bn_ ( _s_ ) _xs_



_._ (35)



With these ingredients, we are now ready to prove the main results in this section.

**Lemma 1.** _Consider the hidden states arising from the S4D and S6 SSMs defined in (6) and (7). The sensitivity of the n-th_
_component of their states at time t with respect to the input at time j ≪_ _t is given by, respectively,_
_∂h_ _[S4D]_ _t_ = ˜ _c_ ( _λn, Bn, x≤j_ ) _e−λn_ ( _t−_ ( _j_ +1)) _,_
���� _∂xj_ ����




_[S4D]_ _t_ = ˜ _c_ ( _λn, Bn, x≤j_ ) _e−λn_ ( _t−_ ( _j_ +1)) _,_
_∂xj_ ����

_∂h_ _[M]_ _t_ = ˜ _c_ (∆ _, λn, Bn, x≤j_ ) _e−λn_ - _tr_ = _j_
���� _∂xj_ ����



= ˜ _c_ (∆ _, λn, Bn, x≤j_ ) _e−λn_ - _tr_ = _j_ +1 [∆(] _[x][r]_ [)] _,_
����



_where_ ˜ _c_ (∆ _, λn, Bn, x≤j_ ) _depends on the input subsequence x≤j, independent of the sequence length t._


_Proof._ Recall the Mamba discretization (5) chooses input-dependent SSM parameters as

_λn_ ( _r_ ) = _e_ _[−][λ][n]_ [∆(] _[x][r]_ [)] _,_ _Bn_ ( _j_ ) = _Bn_ ( _xj_ )∆( _xj_ ) _._ (36)


Substituting (36) to (35), we see that the first factor becomes



_t_

- _λn_ ( _r_ ) = _e_ _[−][λ][n]_ - _tr_ = _j_ +1 [∆(] _[x][r]_ [)] _,_ (37)


_r_ = _j_ +1



while the second factor becomes


_∂_
( _Bn_ ( _j_ ) _xj_ ) + _[∂λ][n]_ [(] _[r]_ [)]
_∂xj_ _∂xj_



_j−_ 1



_s_ =1




- _j−_ 1

 

_r_ = _s_ +1




  
_λn_ ( _r_ )



_Bn_ ( _s_ ) _xs_



= _[∂]_




_[∂]_ ( _Bn_ ( _xj_ ) ∆( _xj_ ) _xj_ ) _−_ _λe_ _[−][λ]_ [∆(] _[x][j]_ [)] _[∂]_ [∆(] _[x][j]_ [)]

_∂xj_ _∂xj_



_∂xj_



_j−_ 1

- _e_ _[−][λ][n]_ - _jr−_ =1 _s_ +1 [∆(] _[x][r]_ [)] _Bn_ ( _xs_ ) ∆( _xs_ ) _xs._ (38)


_s_ =1



16


We are interested in the behavior of the sensitivity for _j_ fixed and _t →∞_ (i.e., the sensitivity of the current state with respect
to early input in long-range sequences). Notice that the second factor does not scale with _t_ (since _j_ is fixed), and can be
bound in terms of the parameters defining the transformations _**B**_ ( _x_ ) and ∆( _x_ ), as well as _λn_ and _x_, as such:


_|_ (38) _| ≤_ _c_ ˜( _Bn,_ ∆ _, λn, x≤j_ ) _._ (39)


On the other hand, the first factor in (37) shows in general _exponential_ dependence on _t_ . Putting both together, we have

_∂h_ [M] _t_ _≤_ _c_ ˜(∆ _, λn, Bn, x≤j_ ) _e−λn_              - _tr_ = _j_ +1 [∆(] _[x][r]_ [)] _._ (40)
���� _∂xj_ ����


The proof for S4D is immediate by taking ∆( _xt_ ) = 1 _, Bn_ ( _xt_ ) = _Bn_ for all _t_, resulting in

_∂h_ [S4D] _t_ _≤_ _c_ ˜( _λn, Bn, x≤j_ ) _e−λn_ ( _t−_ ( _j_ +1)) _._ (41)
���� _∂xj_ ����


**Lemma 2.** _Consider the discrete-time S6 in (7) where_ _**B**_ _t_ = [ _B_ 1( _xt_ ) _, . . ., Bn_ ( _xt_ )] _[⊤]_ _∈_ R _[N]_ _. Suppose there exists a_
_constant c ≥_ 0 _such that_



lim
_t→∞_ _[λ][n]_



_t_

- ∆( _xr_ ) _≤_ _c._ (12)


_r_ =1



_Then the sensitivity of the n-th component of the state at time t with respect to any input xj is lower bounded by_



_≥_ _e−c_ _∂_ _Bn_ ( _xs_ ) ∆( _xs_ ) _xs_
���� ���� _∂xj_



_._ (13)
����



lim
_t→∞_



_∂h_ _[M]_ _t_
���� _∂xj_



_Proof._ Recall for the scalar input sequence, each component of the Mamba hidden state is given by



_h_ [M] _t_ [=]



_t_

- _e_ _[−][λ][n]_ - _tr_ = _s_ +1 [∆(] _[x][r]_ [)] _Bn_ ( _xs_ ) ∆( _xs_ ) _xs._ (42)


_s_ =1



Then the condition in (12) implies



_t_ lim _→∞_ _[h]_ _t_ [M] _[≥]_



_t_

- _e_ _[−][c]_ _Bn_ ( _xs_ ) ∆( _xs_ ) _xs._ (43)


_s_ =1



Straightforward computation of lim _t→∞_ ��� _∂x∂hj_ M _t_



completes the proof.
���



For comparison, we provide a similar sensitivity analysis for the softmax Attention layer as well.


_Proof of Sensitivity of Softmax Attention._



_e_ _[x][t]_ _**[W]**_ _[ ⊤]_ _Q_ _**[W]**_ _[K]_ _[x][s]_

~~�~~ _t_ _Q_ _**[W]**_ _[K]_ _[x][r]_ _**[W]**_ _[V][ x][s]_
_r_ =1 _[e][x][t]_ _**[W]**_ _[ ⊤]_







_∂yt_ [Attn] = _∂_
_∂xj_ _∂xj_




- _t_

 

_s_ =1



_e_ _[x][t]_ _**[W]**_ _[ ⊤]_ _Q_ _**[W]**_ _[K]_ _[x][j]_ ( _xt_ _**W**_ _Q_ _[⊤]_ _**[W]**_ _[K]_ _**[W]**_ _[V]_ _[x][j]_ [+] _**[ W]**_ _[V]_ [)] _e_ _[x][t]_ _**[W]**_ _[ ⊤]_ _Q_ _**[W]**_ _[K]_ _[x][j]_ _xt_ _**W**_ _Q_ _[⊤]_ _**[W]**_ _[K]_
= _−_

~~�~~ _tr_ =1 _[e][x][t]_ _**[W]**_ _[ ⊤]_ _Q_ _**[W]**_ _[K]_ _[x][r]_ ~~��~~ _tr_ =1 _[e][x][t]_ _**[W]**_ _[ ⊤]_ _Q_ _**[W]**_ _[K]_ _[x][r]_ ~~[�]~~ [2]



_t_

- _e_ _[x][t]_ _**[W]**_ _[ ⊤]_ _Q_ _**[W]**_ _[K]_ _[x][s]_ _**W**_ _V xs_


_s_ =1



(44)



_e_ _[x][t]_ _**[W]**_ _[ ⊤]_ _Q_ _**[W]**_ _[K]_ _[x][j]_
=

~~�~~ _t_ _Q_ _**[W]**_ _[K]_ _[x][r]_
_r_ =1 _[e][x][t]_ _**[W]**_ _[ ⊤]_

_e_ _[x][t]_ _**[W]**_ _[ ⊤]_ _Q_ _**[W]**_ _[K]_ _[x][j]_
=

~~�~~ _t_ _Q_ _**[W]**_ _[K]_ _[x][r]_
_r_ =1 _[e][x][t]_ _**[W]**_ _[ ⊤]_





_xt_ _**W**_ _Q_ _[⊤]_ _**[W]**_ _[K]_ _**[W]**_ _[V]_ _[x][j]_ [+] _**[ W]**_ _[V]_ _[−]_ _[x][t]_ _**[W]**_ _Q_ _[ ⊤]_ _**[W]**_ _[K]_




- _ts_ =1 _[e][x][t]_ _**[W]**_ _[ ⊤]_ _Q_ _**[W]**_ _[K]_ _[x][s]_ _**W**_ _V xs_

~~�~~ _t_ _Q_ _**[W]**_ _[K]_ _[x][r]_
_r_ =1 _[e][x][t]_ _**[W]**_ _[ ⊤]_









_**W**_ _V_ + _xt_ _**W**_ _Q_ _[⊤]_ _**[W]**_ _[K]_




- - _ts_ =1 _[e][x][t]_ _**[W]**_ _[ ⊤]_ _Q_ _**[W]**_ _[K]_ _[x][s]_ _**W**_ _V xs_

_**W**_ _V xj −_ ~~�~~ _t_ _Q_ _**[W]**_ _[K]_ _[x][r]_
_r_ =1 _[e][x][t]_ _**[W]**_ _[ ⊤]_



��



_._



17


Even in this case, the second term can be bound by a constant _C_ in terms of _∥_ _**W**_ _Q∥, ∥_ _**W**_ _K∥, ∥_ _**W**_ _V ∥_ and _∥_ _**x**_ _∥_ (notice that
the rightmost term, where a fraction of two sums over _t_ appear, behaves as a weighted average of _**W**_ _V xs_, and hence does
_not_ scale with _t_ ). Only the denominator at the first factor remains as a function of _t_, providing

_∂h_ [Attn] _t_ _≤_ _C_ 1 �min _e_ _[x][t]_ _**[W]**_ _[ ⊤]_ _Q_ _**[W]**_ _[K]_ _[x][r]_ [�] _[−]_ [1] _._ (45)
���� _∂xj_ ���� _t_ _r_



1
_≤_ _C_
���� _t_



_t_




min _e_ _[x][t]_ _**[W]**_ _[ ⊤]_ _Q_ _**[W]**_ _[K]_ _[x][r]_ [�] _[−]_ [1] _._ (45)
_r_



18


**C. Proofs of Section 5**


**C.1. Proofs of Mamba Solving the MQAR Task**


**The MQAR Task** As a reminder, the MQAR task receives an input in the form



_**x**_ = [ _k_ 1 _v_ 1 _. . . ki vi_ _. . . kκ_ _vκ_ _| v× . . . v× ki_ 1 _v× . . . v× kij v× . . . v× kiκ v× . . ._ ] _∈_ R _[T]_ _,_ (46)

~~�~~ _κ_ key-value pairs ~~��~~ ~~�~~   - shuffled queries (=keys), interwoven with noise ~~�~~   - ~~�~~



where the keys _ki_ are randomly taken from a key set of size _κ_, and the values _vi_ and the various noise tokens _v×_ are
randomly taken from a vocabulary of size _|V |_ . The goal is to output a sequence that, at the location of each query, reports
the value matching the corresponding key, that is


_**y**_ = [ _× . . . × | × . . . × vi_ 1 _× . . . × vij_ _× . . . × viκ_ _× . . . ×_ ] _,_ (47)


while the other components of the output (denoted with _×_ ) are ignored.


For the rest of the proofs, we often make use of the Johnson-Lindenstrauss (JL) Lemma to reduce the embedding dimensionality. For completeness, we state and prove JL lemma below.


**Lemma 4** (JL Lemma) **.** _Given a set of standard bases {_ _**e**_ 1 _, . . .,_ _**e**_ _d} and ϵ ∈_ (0 _,_ 0 _._ 5) _, there exists a random projection_

_matrix M_ : R _[d]_ _→_ R _[p]_ _where p_ = _O_ ( _ϵ_ _[−]_ [2] log _d_ ) _, Mi,j_ _i.i.d.∼_ ~~_√_~~ 1 ~~_p_~~ Unif _{±_ 1 _} such that for all pairs_ ( _i, j_ ) _,_


_|⟨M_ _**e**_ _i, M_ _**e**_ _j⟩| ≤_ _ϵ._ (48)

_Proof._ Let _[√]_ ~~_pM_~~ _i,j_ := _Zi,j_, where _Zi,j_ is the symmetric Bernoulli variable. For any _i, j ∈_ [ _d_ ], we have



_⟨M_ _**e**_ _i, M_ _**e**_ _j⟩_ = _⟨M_ [: _,i_ ] _, M_ [: _,j_ ] _⟩_ = _p_ [1]



_p_





- _Zl,iZl,j ≡_ [1]

_p_

_l_ =1



_p_



_p_

- _Zl,_


_l_ =1



where the last equality follows from the fact that the product of two independent Bernoulli variables is Bernoulli. In other
words, the dot product of the two projected vectors is a sum of _k_ i.i.d. symmetric Bernoullis. By Hoeffding’s inequality (e.g.
Vershynin (2018, Theorem 2.2.2)),



_p_




_Zl ≥_ _ϵ_

_l_ =1







_≤_ exp ( _−_ _[pϵ]_ [2] (49)

2 [)] _[.]_



P




1

_p_



Thus







_p_




_Zl ≥_ _ϵ_

_l_ =1



_≤_ 2 exp ( _−_ _[pϵ]_ [2] (50)

2 [)] _[.]_



P [ _|⟨M_ _**e**_ _i, M_ _**e**_ _j⟩| ≥_ _ϵ_ ] = 2P




1

_p_



Therefore, except with probability less than 2 exp ( _−_ _[pϵ]_ [2]



2 [)][, it holds that] _[ |⟨][M]_ _**[e]**_ _[i][, M]_ _**[e]**_ _[j][⟩| ≤]_ _[ϵ]_ [. Let] _[ p]_ [ =] _ϵ_ 4 [2] [ln] _[d]_ _δ_



Therefore, except with probability less than 2 exp ( _−_ 2 [)][, it holds that] _[ |⟨][M]_ _**[e]**_ _[i][, M]_ _**[e]**_ _[j][⟩| ≤]_ _[ϵ]_ [. Let] _[ p]_ [ =] _ϵ_ [2] [ln] _δ_ [for some]

_δ ∈_ (0 _,_ 1). By a union bound, this holds for all - _d_ 2� pairs of ( _**e**_ _i,_ _**e**_ _j_ ) except with probability




2 exp ( _−_ _[pϵ]_ [2]




_[d]_
2 [) = exp(] _[−]_ [2 ln] _δ_



(51)
_δ_ [) =] _[ δ]_ [2] _[.]_




- _d_

2




[2]

2 [)] _[ < d]_ [2][ exp (] _[−]_ _[pϵ]_ 2 [2]



Since there is a probability grater than 1 _−_ _δ_ [2] that _|⟨M_ _**e**_ _i, M_ _**e**_ _j⟩| ≤_ _ϵ_ holds for all _i, j_, this guarantees the existence of such
_M_ by the probabilistic method.


**Theorem 2.** _There exists a_ 1 _-layer Mamba model without gating that solves_ MQAR _with κ pairs using embedding size_
_d_ = _O_ ( _κ_ + log _|V |_ ) _, and state size N_ = _κ._


_Proof._ The proof is divided into two steps: We first construct a 1-layer Mamba model without gating that solves MQAR
using standard basis vectors _d_ = _O_ ( _κ_ + _|V |_ ); based on such construction, we then apply the JL Lemma (Lem. 4) together
with a shifting trick to complete the proof for _d_ = _O_ ( _κ_ + log _|V |_ ).


19


**Step 1: Construction With** _d_ = ( _κ_ + _O_ ( _|V |_ ) **.** We consider one-hot encoding of keys and values. The main idea is to (i) use
the size-2 convolution to combine key-value pairs and filter out other uninformative pairs (particularly, preserve key-value
pairs and discard value-value pairs); (ii) use appropriate input-dependent SSM matrices to store and retrieve the key-value
information in the hidden state; (iii) use the output layer to project to the value embedding subspace. Crucially, we observe
that the hidden state of Mamba at time _t_ is a _d × N_ matrix (14f). Thus, we leverage this matrix structure such that each
column of the state corresponds to a given key ( _N_ = _κ_ ), and holds the value associated with it, that is:


_**h**_ = [ _**v**_ _j_ 1 _|_ _**v**_ _j_ 2 _|_ _. . ._ _|_ _**v**_ _jκ_ ] _∈_ R _[d][×][N]_ _._ (52)


We first describe how our proposed solution operates, and then prove that indeed our solution solves the MQAR task exactly.


  - **Embedding** The task of the embedding layer is to clearly distinguish values and keys. We achieve this by letting it map
to orthogonal directions: let the embedding dimension be _d_ = _|V |_ + _κ_, and _**e**_ _i ∈_ R _[d]_ denote the standard basis vector.
We impose
_ki �→_ _**k**_ _i_ := _k ·_ _**e**_ _i,_ and _vi �→_ _**v**_ _i_ := _v ·_ _**e**_ _κ_ + _i_ (53)


for some parameters _k, v >_ 0.


  - **Convolution** We use a size-2 convolution to combine information of a key-value pair, as it is essential to the task
solution. We remind that for a sequence ( _. . .,_ _**x**_ _t−_ 1 _,_ _**x**_ _t, . . ._ ), the action of the convolution layer with size-2 kernel with
left padding _**x**_ 0 is given by


_**x**_ ˆ _t_ = conv( _**x**_ _t−_ 1 _,_ _**x**_ _t_ ) := _σ_ ( _**c**_ 0 _⊙_ _**x**_ _t−_ 1 + _**c**_ 1 _⊙_ _**x**_ _t −_ _**b**_ ) _,_ _**x**_ 0 := **0** _,_ (54)


for certain parameters _**c**_ 0 _,_ _**c**_ 1 _,_ _**b**_ _∈_ R _[d]_ and a nonlinearity [1] _σ_ = ReLU. For our construction, it suffices to pick
_**c**_ 0 = _c_ 0 **1** _,_ _**c**_ 1 = _c_ 1 **1**, and _**b**_ = _b_ **1**, with _c_ 0 _, c_ 1 _, b >_ 0. We want such nonlinear convolution to preserve information
from the ( _**k**_ _i,_ _**v**_ _j_ ) pairs, so we impose _c_ 0 _k > b_ and _c_ 1 _v > b_ . We also need to prevent the value in ( _**v**_ _i,_ _**k**_ _j_ ) from getting
associated to the wrong key, but we want to preserve key information to be able to extract it at retrieval time, so we ask
_c_ 0 _v ≤_ _b_ and _c_ 1 _k > b_ . The ( _**k**_ _i,_ _**k**_ _j_ ) pair represents an edge-case, and we will show later how to deal with this. Finally,
we want to ignore contributions from ( _**v**_ _i,_ _**v**_ _j_ ) pairs (as they refer to “noise” tokens), but we will delegate this task to
input-selectivity in the SSM layer. To summarize, we need to satisfy


_c_ 0 _k > b,_ _c_ 1 _v > b,_ _c_ 1 _k > b,_ _c_ 0 _v ≤_ _b._ (55)


A feasible parameter combination satisfying (55) is given by:


_v_ = 1 _,_ _k_ = 2 _,_ _c_ 0 = 1 _,_ _c_ 1 = 2 _,_ and _b_ = 1 _._ (56)


Let us show how such nonlinear convolution acts on the four different types of input pairs, under the assumptions (55):


conv(( _**k**_ _i,_ _**v**_ _j_ )) = ReLU( _c_ 0 _**k**_ _i_ + _c_ 1 _**v**_ _j −_ _**b**_ ) = ( _c_ 0 _k −_ _b_ ) _**e**_ _i_ + ( _c_ 1 _v −_ _b_ ) _**e**_ _κ_ + _j_ (57a)

conv(( _**v**_ _i,_ _**k**_ _j_ )) = ReLU( _c_ 0 _**v**_ _i_ + _c_ 1 _**k**_ _j −_ _**b**_ ) = ( _c_ 1 _k −_ _b_ ) _**e**_ _j_ (57b)

conv(( _**k**_ _i,_ _**k**_ _j_ )) = ReLU( _c_ 0 _**k**_ _i_ + _c_ 1 _**k**_ _j −_ _**b**_ ) = ( _c_ 0 _k −_ _b_ ) _**e**_ _i_ + ( _c_ 1 _k −_ _b_ ) _**e**_ _j_ (57c)

conv(( _**v**_ _i,_ _**v**_ _j_ )) = ReLU( _c_ 0 _**v**_ _i_ + _c_ 1 _**v**_ _j −_ _**b**_ ) = ( _c_ 1 _v −_ _b_ ) _**e**_ _κ_ + _j._ (57d)


  - **Mamba SSM** The SSM layer organizes a hidden state _matrix_ where its columns are indexed by the keys and store
information of the associated values. To this end, we let


**Λ** = **0** _,_ ∆ _t_ = **1** _,_ _**B**_ ( _**x**_ ) = _**C**_ ( _**x**_ ) = [ _Iκ|_ **0** _|V |_ ] _**x**_ _≡_ _Wk_ _**x**_ _,_ (58)


where _Wk_ projects the input to the key embedding subspace. Consequently, the SSM layer covers three roles at once:


1In the original Mamba definition, we have _σ ≡_ SiLU; for ease of illustration we consider instead _σ ≡_ ReLU, but notice that similar
considerations still hold in this case, in light of the fact that SiLU( _x_ ) _→_ ReLU( _x_ ) for _x →±∞_ : it suffices then to opportunely scale the
inputs.


20


1. Ensure that the right key get associated to the right column in the hidden state. This role is covered by the input
matrix _**B**_ ( _**x**_ ) = _WB_ _**x**_ . By picking _WB_ = _Wk_ = [ _Iκ|_ **0** _|V |_ ], we can see that only keys in the convolved input pair
are used for populating the hidden state. This choice of the input matrix also ensures that information from ( _**v**_ _i,_ _**v**_ _j_ )
pairs does not affect the hidden state.

2. Propagate information down the sequence without corrupting it (i.e., memorization). The state matrix _A_ ( _**x**_ ) can
take care of this, provided we fix it to the all-one matrix. This can be achieved by prescribing **Λ** _≡_ **0** in (14a).
Notice that with this choice, the SSM layer simply performs a cumulative sum _**h**_ _t_ = [�] _s_ _[t]_ =1 _**[x]**_ [ˆ] _[s][ ⊗]_ _**[B]**_ _[s]_ [.]
3. Retrieve the correct column when needed. This task is taken care of by the output matrix _C_ ( _**x**_ ) = _WC_ _**x**_ . As with
the input matrix, it suffices to have _WC_ = _Wk_ = [ _Iκ|_ **0** _|V |_ ]: also in this case, when an input containing a key is
encountered, the _C_ ( _**x**_ ) matrix will retrieve information only from the column corresponding to that specific key.




- **Output** The final output layer simply needs to correctly classify _**v**_ _i_ as the most likely value from the retrieved vector
_**y**_ _t_ . To this end, it suffices to consider



_**v**_ 1 _[⊤]_
...
_**v**_ _|_ _[⊤]_ _V |_






 _,_ (59)



_Wo_ =


as _**v**_ _i_ will return the maximum scalar product with _**y**_ _t_ .










Let us illustrate with a step-by-step example how the above construction yields the required result. Consider a generic input
sequence _**x**_ for the MQAR task. Under the embedding layer prescribed in (53), the (embeded) input will be in the form


_**x**_ =    - _**k**_ _i_ 1 _**v**_ _j_ ( _i_ 1) _**k**_ _i_ 2 _**v**_ _j_ ( _i_ 2) _. . ._ _**k**_ _iκ_ _**v**_ _j_ ( _iκ_ ) _**v**_ _×_ _. . ._ _**k**_ _k_ 1 _. . ._ _**k**_ _kκ_ _. . ._ [�] _∈_ R _[d][×][T]_ _,_ (60)


so that the _im_ -th key is associated to the _j_ ( _im_ )-th value. The order in which the keys appear at query time is also random
and denoted by the _km_ subscript. Moreover, keys at query time are randomly interwoven with value tokens, denoted as _**v**_ _×_ .
After convolution, this input gets mapped to






 _,_


(61)



_**x**_ ˆ =



 _c_ 1 _,k_ _**e**_ _i_ 1 _c_ 0 _,k_ _**e**_ _i_ 1 _c_ 1 _,k_ _**e**_ _i_ 2 _c_ 0 _,k_ _**e**_ _i_ 2 _. . ._ _c_ 1 _,k_ _**e**_ _iκ_ _c_ 0 _,k_ _**e**_ _iκ_ **0** _. . ._ _c_ 1 _,k_ _**e**_ _k_ 1 _. . ._ _c_ 1 _,k_ _**e**_ _kκ_ _. . ._

 + + + + + + + + +
**0** _c_ 1 _,v_ _**e**_ _j_ ( _i_ 1) **0** _c_ 1 _,v_ _**e**_ _j_ ( _i_ 2) _. . ._ **0** _c_ 1 _,v_ _**e**_ _j_ ( _iκ_ ) **0** _. . ._ **0** _. . ._ **0** _. . ._



where we denote _c_ 0 _,k_ = _c_ 0 _k −_ _b_, _c_ 1 _,k_ = _c_ 1 _k −_ _b_, _c_ 1 _,v_ = _c_ 1 _v −_ _b_ to slim notation, and separate the components of ˆ _**x**_ into
key-related (top) and value-related (bottom) for illustrative purposes. Notice how, for the initial part of the input, distinct
keys (the various _**e**_ _im_ at the top) are only associated with their respective values (the various _**e**_ _j_ ( _im_ ) at the bottom), or with **0** .
Before moving on to the SSM layer, let us remind its action (14f):



_**C**_ _t_




- _t−_ 1

 







  -   _**y**_ _t_ = _e_ **[Λ]** _[⊙]_ [(∆] _[t][⊗]_ **[1]** _[N]_ [)] _⊙_ _**h**_ _t−_ 1 + (∆ _t ⊙_ _**x**_ ˆ _t_ ) _⊗_ _**B**_ _t_ _**C**_ _t_ =



_t_



_s_ =1




- _e_ **[Λ]** _[⊙]_ [(∆] _[m][⊗]_ **[1]** _[N]_ [)] _⊙_ (ˆ _**x**_ _s ·_ _**B**_ _s_ _[⊤]_ [)]

_m_ = _s_



(62)



======== **Λ** _≡_ **0** _,_ ∆ _m≡⇒_ **1** _**y**_ _t_ = - _**h**_ _t−_ 1 + ˆ _**x**_ _t ·_ _**B**_ _s_ _[⊤]_ - _**C**_ _t_ =



_t_



_s_ =1




- _**x**_ ˆ _s ·_ _**B**_ _s⊤_ - _**C**_ _t._



Notice how the hidden state naturally admits a matrix structure _**h**_ _t ∈_ R _[d][×][N]_ due to the outer product _**x**_ _s ·_ _**B**_ _s_ _[⊤]_ [, whose]
columns are updated by _**x**_ _t_, where _**B**_ _s_ determines which columns are affected. If _**B**_ _t ∝_ _**e**_ _i_ (i.e., only nonzero at component
_i_ ), then _**x**_ _t_ will contribute to only the _i_ -th column in _**h**_ _t_ . Similarly, taking _**C**_ _t ∝_ _**e**_ _i_ ensures that only the _i_ -th column from
the hidden state is retrieved. This is precisely what we achieve with the choice outlined above. In light of this, the hidden
state after the initial part of the input (where the key-value pairs are listed) admits the following form at time 2 _κ_



_**h**_ 2 _κ_ =



 _k_ ˆ _**e**_ 1 _k_ ˆ _**e**_ 2 _. . ._ _k_ ˆ _**e**_ _κ_

 + + +
_v_ ˆ _**e**_ _j_ (1) _v_ ˆ _**e**_ _j_ (2) _. . ._ _v_ ˆ _**e**_ _j_ ( _κ_ )





 _,_ with _k_ ˆ = _c_ [2] 1 _,k_ [+] _[ c]_ 0 [2] _,k_ _[,]_ _v_ ˆ = _c_ 1 _,vc_ 0 _,k,_ (63)



and remains unchanged until the first key _**k**_ _k_ 1 is encountered at query time, as _**B**_ ( _**x**_ ) = _**B**_ (( _**v**_ _i,_ _**v**_ _i_ )) = **0** for all the tokens
in between. For each key _**k**_ _i_ encountered at query time, _**C**_ ( _**x**_ ) then takes care of extracting the corresponding _i_ -th column of


21


_**h**_, which holds a vector proportional to the embedding of its associated value, _**e**_ _j_ ( _i_ ). Finally, the output matrix computes
scalar products of all the values embedding with the extracted vector, which will then be maximum for _**e**_ _j_ ( _i_ ), thus accurately
solving the MQAR task.


There is an edge case to this construction, which occurs when, at retrieval time, two keys appear adjacent to each other in a
key-key pair ( _**k**_ _i,_ _**k**_ _l_ ). In this case, _**k**_ _l_ represents the actual query, while _**k**_ _i_ acts as noise: the convolution will in fact contain
information from two keys, implying that _C_ ( _**x**_ ) will have two nonzero components at that point. However, with the correct
parameter choice, we can have the query information dominate the noise, and still recover the desired solution. We have by
(57c)
_**C**_ (conv( _**k**_ _i,_ _**k**_ _l_ )) = ( _c_ 0 _k −_ _b_ ) _**e**_ _i_ + ( _c_ 1 _k −_ _b_ ) _**e**_ _l._ (64)


When tested against the hidden state at that instant, then, the output matrix will return a linear combination of the values
associated to the two keys:


_**C**_ (conv( _**k**_ _i,_ _**k**_ _l_ )) _**h**_ _t_ = ( _c_ 0 _k −_ _b_ )      - _k_ ˆ _**e**_ _i_ + ˆ _v_ _**e**_ _j_ ( _i_ )� + ( _c_ 1 _k −_ _b_ )      - _k_ ˆ _**e**_ _l_ + ˆ _v_ _**e**_ _j_ ( _l_ )� _._ (65)


To ensure the key-value pair of _**k**_ _l_ dominates that of _**k**_ _i_, we need to impose


_c_ 0 _k −_ _b ≪_ _c_ 1 _k −_ _b_ _⇐_ = _c_ 0 _≪_ _c_ 1 _,_ (66)


which is already satisfied by the parameter choice (56).


**Step 2: Dimensionality Reduction to** _d_ = _O_ ( _κ_ + log _|V |_ ) **.** Having proved the construction with _d_ = _O_ ( _κ_ + _|V |_ ), we now
apply JL Lemma to reduce the embedding dimension and suitably adjust the Mamba architecture weights to ensure the
desired output. Concretely:


  - **Embedding** We use the same key embeddings as above _{_ _**e**_ 1 _, . . .,_ _**e**_ _κ}_ while reducing the value embedding dimensionality to satisfy almost-orthogonality. By JL Lemma (c.f. Lem. 4), given the value embeddings _{_ _**e**_ _j}_ _[|]_ _j_ _[V]_ =1+ _[ |]_ [+] _[κ]_ _κ_ [and]
_ϵ ∈_ (0 _,_ 0 _._ 5), there exists _Mv_ : R _[|][V][ |]_ _→_ R _[p]_ where _p_ = _O_ (log _|V |_ ) and each of its entry is in _{−_ ~~_√_~~ [1] ~~_p_~~ _,_ ~~_√_~~ 1 ~~_p_~~ _}_ such that

_⟨Mv_ _**e**_ _j_ 1 _, Mv_ _**e**_ _j_ 2 _⟩≤_ _ϵ_ for any _j_ 1 _̸_ = _j_ 2. Let _M_ := _**I**_ _κ ⊕_ _Mv ∈_ R [(] _[κ]_ [+] _[p]_ [)] _[×]_ [(] _[κ]_ [+] _[|][V][ |]_ [)] be the direct sum of the identity matrix
preserving the one-hot key embeddings and the JL matrix projecting the value embeddings. Let the normalized value
embedding be ¯ _**v**_ _j_ := _M_ _**e**_ _j_ Let the _shifted_ value embedding be



_**v**_ _j_ := _M_ _**e**_ _j_ + _β_ [ **0** _κ|_ **1** _p_ ] _[⊤]_ =



_p_


( _Mi_ + _κ,j_ + _β_ ) _**e**_ _i_ + _κ ∈_ R _[κ]_ [+] _[p]_ _,_ (67)

_i_ =1



where each component of _**v**_ _j_ is in the range [ _−_ ~~_√_~~ [1] ~~_p_~~ + _β,_ ~~_√_~~ 1 ~~_p_~~ + _β_ ] _≡_ [ _v_ min _, v_ max]. Note that by letting _β > −_ ~~_√_~~ [1] ~~_p_~~, we
can ensure all components of _**v**_ _j_ are nonnegative.


- **Convolution** We use a size-2 convolution as (54), to retain the value from ( _**k**_ _i,_ _**v**_ _j_ ) pairs and discard the value from
( _**v**_ _i,_ _**k**_ _j_ ) pairs. Ideally we want


conv( _**k**_ _i,_ _**v**_ _j_ ) := ReLU( _c_ 0 _**k**_ _i_ + _c_ 1 _**v**_ _j −_ _**b**_ ) _∝_ ( _c_ 0 _−_ _b_ ) _**k**_ _i_ + ( _c_ 1 _−_ _b_ ) _**v**_ _j_ (68a)

conv( _**v**_ _i,_ _**k**_ _j_ ) := ReLU( _c_ 0 _**v**_ _i_ + _c_ 1 _**k**_ _j −_ _**b**_ ) _⊥_ span( _{_ _**v**_ 1 _, . . .,_ _**v**_ _|V |}_ ) _._ (68b)


Given the shifted value embeddings, we must impose the following constraints to achieve (68):


_c_ 0 _k > b,_ _c_ 1 _v_ min _> b,_ _c_ 0 _v_ max _≤_ _b._ (69)


A feasible parameter combination satisfying (69) is given by


_β_ = 2 ( = _⇒_ [ _v_ min _, v_ max] _⊆_ [1 _,_ 3] since _p ≥_ 1) _,_ _k_ = 10 _,_ _c_ 0 = 1 _,_ _c_ 1 = 10 _,_ and _b_ = 3 _._ (70)


Consequently, we have the desired convolution outputs



_p_

- _**e**_ _i_ + _κ_


_i_ =1



conv( _**k**_ _i,_ _**v**_ _j_ ) = ( _c_ 0 _k −_ _b_ ) _**e**_ _i_ +



_p_

- ( _c_ 1( _Mi_ + _κ,j_ + _β_ ) _−_ _b_ ) _**e**_ _i_ + _κ_ = ( _c_ 0 _k −_ _b_ ) _**e**_ _i_ + _c_ 1 _**v**_ ¯ _j_ + ( _c_ 1 _β −_ _b_ )


_i_ =1



(71a)




~~�~~ ~~��~~ ~~�~~
:= _**s**_

conv( _**v**_ _i,_ _**k**_ _j_ ) = ( _c_ 1 _k −_ _b_ ) _**e**_ _j._ (71b)


22


Note that the edge case for conv( _**k**_ _i,_ _**k**_ _j_ ) is taken care of since _c_ 0 _≪_ _c_ 1, while the ( _**v**_ _i,_ _**v**_ _j_ ) pairs will be ignored in the
SSM layer, following the same argument as in step 1.


  - **Mamba SSM** The SSM layer is the same as in step 1 (58), propagating information through the hidden state (c.f. (63)).
At the query time for key _**k**_ _i, i_ = 1 _, . . ., κ_, the output _**y**_ _t_ = �( _c_ 0 _k −_ _b_ ) [2] + ( _c_ 1 _k −_ _b_ ) [2][�] _**e**_ _i_ + ( _c_ 0 _k −_ _b_ )   - _c_ 1 _**v**_ ¯ _j_ ( _i_ ) + _**s**_   
contains the (scaled) normalized value embedding shifted by a constant vector _**s**_ .


  - **Output** The output layer undoes the shift and classifies based on the normalized value embeddings. Recall _Mv ∈_
R _[|][V][ |×][p]_ is the JL matrix from the value embedding projection. We set the output linear layer with the weight matrix
_Wo_ = [ **0** _|Mv_ _[⊤]_ []] _[ ∈]_ [R] _[|][V][ |×]_ [(] _[κ]_ [+] _[p]_ [)][ and the bias vector] _**[ b]**_ _[o]_ [=] _[ −]_ [(] _[c]_ [0] _[k][ −]_ _[b]_ [)] _[W][o]_ _**[s]**_ [. The final output is given by]

_Wo_ _**y**_ _t_ + _**b**_ _o_ = ( _c_ 0 _k −_ _b_ ) _Wo_       - _c_ 1 _**v**_ ¯ _j_ ( _i_ ) + _**s**_       - _−_ ( _c_ 0 _k −_ _b_ ) _Wo_ _**s**_ = ( _c_ 0 _k −_ _b_ ) _c_ 1 _Mv_ _[⊤][M][v]_ _**[e]**_ _j_ ( _i_ ) _[,]_ (72)


which yields the maximum at component _j_ ( _i_ ) since _⟨Mv_ _**e**_ _j_ ( _i_ ) _, Mv_ _**e**_ _l⟩≤_ _ϵ_ for any _l ̸_ = _j_ ( _i_ ) by JL Lemma. This
completes the proof.


**Theorem 3.** _There exists a_ 1 _-layer Mamba-2 model without gating that solves_ MQAR _with κ pairs using embedding size_
_d_ = _O_ (log _κ_ + log _|V |_ ) _, and state size N_ = log _κ._


_Proof._ The idea is to execute the Mamba solution outlined in the proof of Thm. 2, but in a _leaner_ manner due to the
additional degrees of freedom in Mamba-2: particularly, we leverage the fact that the convolutions for the value (14b),
key (14d), and query (14e) can be chosen independently (instead of using the same convolution in Mamba). The proof is
divided into two steps: We first present the construction using standard basis vectors with _d_ = _κ_ + _|V |_ ; We then reduce the
embedding dimension by applying JL lemma.


**Step 1: Construction With** _d_ = _O_ ( _κ_ + _|V |_ ) **.**


  - **Embedding - Same as Mamba** The role of the embedding layer is to clearly distinguish values and keys. We achieve
this by letting it map to independent directions: let the embedding dimension be _d_ = _|V |_ + _κ_, and _**e**_ _i ∈_ R _[d]_ denote the
standard basis vector. We let
_ki �→_ _**k**_ _i_ := _**e**_ _i,_ and _vi �→_ _**v**_ _i_ := _**e**_ _κ_ + _i._ (73)


  - **Convolution** Differently from Mamba that uses the same convolution kernel to compute the SSM input _**u**_ and
parameters _**B**_ _,_ _**C**_, Mamba-2 has the additional flexibility of using three independent convolutions for computing
_**u**_ _,_ _**B**_ _,_ _**C**_ (see details in Tab. 2). We now exploit this flexibility by setting:



_**x**_ ˆ _[B]_ _t_ [= conv] _[B]_ [(] _**[x]**_ _[t][−]_ [1] _[,]_ _**[ x]**_ _[t]_ [) =] _[ σ]_ [(] _**[c]**_ [0] _[⊙]_ _**[x]**_ _[t][−]_ [1] [+] _**[ c]**_ [1] _[⊙]_ _**[x]**_ _[t]_ _[−]_ _**[b]**_ [) :=] _**[ x]**_ _[t][−]_ [1] _[,]_
_**x**_ ˆ _t_ = conv _u_ ( _**x**_ _t−_ 1 _,_ _**x**_ _t_ ) := _**x**_ _t,_

_**x**_ ˆ _[C]_ _t_ [= conv] _[C]_ [(] _**[x]**_ _[t][−]_ [1] _[,]_ _**[ x]**_ _[t]_ [) :=] _**[ x]**_ _[t][.]_



(74)



Consequently, the output from conv _B shifts_ the input sequence to the right by one position, whereas the outputs from
conv _u,_ conv _C_ are the same as the input.


- **Mamba-2 SSM - Same as Mamba** The role of the SSM layer is to associate key-value pairs, propagate information
through the state, and retrieve the correct value given a query(=key). Unlike Thm. 2, the convolved input (74) contains
only key or value information but never mixes them. A simple choice is to set _**B**_ _,_ _**C**_ as the identity matrices, but this
requires the state size be the same as the embedding size _N_ = _d_ . To further reduce the state size to _N_ = _κ_, we use the
same choice as Mamba (58) by letting


_**λ**_ = **0** _,_ _**B**_ ( _**x**_ _t_ ) = _**C**_ ( _**x**_ _t_ ) = [ _**I**_ _κ|_ **0** _|V |_ ] _**x**_ _t ≡_ _Wk_ _**x**_ _t,_ (75)


- **Output - Same as Mamba** Even in this case, as a classifier it suffices to pick



_**v**_ 1 _[⊤]_
...
_**v**_ _|_ _[⊤]_ _V |_






 _._ (76)



_Wo_ =










23


With the definitions above, we can simplify the outcome of the Mamba-2 layer application. This in fact reduces to



_t_



_s_ =1




- _**x**_ _s_ ( _Wk_ _**x**_ _s−_ 1) _[⊤]_ [�] _Wk_ _**x**_ _t_ =



_t_

- _**x**_ _s_ ( _**x**_ _[⊤]_ _s−_ 1 _[W][ ⊤]_ _k_ _[W][k]_ _**[x]**_ _[t]_ [)] (77)

_s_ =1



_**y**_ _t_ =



_t_



_s_ =1




- _**x**_ ˆ _s_ _**B**_ (ˆ _**x**_ _Bs_ [)] _[⊤]_ [�] _**C**_ (ˆ _**x**_ _[C]_ _s_ [) =]



where _**x**_ 0 = **0** is the zero padding vector. Note that the output _**y**_ _t_ for _t ≥_ 2 _κ_ contains the desired key-value association,
_**x**_ _s_ ( _Wk_ _**x**_ _s−_ 1) _[⊤]_ = _**v**_ _j_ ( _i_ ) _**k**_ _i_ _[⊤]_ [for] _[ s]_ [ = 2] _[,]_ [ 4] _[, . . .,]_ [ 2] _[κ]_ [.]


At query time where _**x**_ _t_ = _**k**_ _i_, thanks to our construction of the Embedding layer, _⟨_ _**k**_ _i,_ _**x**_ _s−_ 1 _⟩_ = 0 for all _s_, _except_ when
_**x**_ _s−_ 1 = _**k**_ _i_ : in that case we have instead _⟨_ _**k**_ _i,_ _**x**_ _s−_ 1 _⟩_ = 1. Whatever the index _s −_ 1 at which this occurs, the associated
value _**x**_ _s_ is precisely the one we are seeking, i.e., the value embedding _**v**_ _j_ ( _i_ ) immediately following the key matching the
corresponding query _**k**_ _i_ . Thus, _**y**_ _t_ = _**v**_ _j_ ( _i_ ). Now, applying the output matrix amounts to computing scalar products between
_**v**_ _j_ ( _i_ ) and all possible value vectors, which will return 1 only at the desired value per the orthogonal embedding construction.


**Step 2: Dimensionality Reduction to** _d_ = _O_ (log _κ_ + log _|V |_ ) **.** We now reduce the embedding dimension _d_ = _|V |_ + _κ_ to
_d_ = _O_ (log _|V |_ + log _κ_ ). To this end, we apply JL Lemma (c.f. Lem. 4) to construct nearly orthogonal embedding vectors,
while keeping the convolution, SSM, and output layers the same as step 1. By JL lemma, fixed _ϵ ∈_ (0 _,_ 0 _._ 5), there exists a
matrix _M ∈_ R _[d][×][p]_ for _p_ = _O_ (log _d_ ) such that _|⟨M_ _**e**_ _i, M_ _**e**_ _j⟩| ≤_ _ϵ_ for all _i, j ∈_ [ _d_ ]. We apply JL lemma separately for the
key embedding subspace (with a JL matrix _Mk ∈_ R [log] _[ κ][×][κ]_ ) and the value embedding subspace (with another JL matrix
_Mv ∈_ R [log] _[ |][V][ |×|][V][ |]_ ). We collect the final JL matrix via a direct sum, _M_ = _Mk ⊕_ _Mv_ . Let _**k**_ _i_ := _M_ _**e**_ _i,_ _**v**_ _j_ := _M_ _**e**_ _κ_ + _j_ .
Then for _i ̸_ = _j_, we have _⟨_ _**k**_ _i,_ _**k**_ _j⟩≤_ _ϵ, ⟨_ _**v**_ _i,_ _**v**_ _j⟩≤_ _ϵ_ . On the other hand, _⟨_ _**k**_ _j,_ _**k**_ _j⟩_ = [�] _i_ _[p]_ =1 _[M]_ [ 2] _i,j_ _[≈]_ [1][. Similarly, we see that]
_⟨_ _**v**_ _j,_ _**v**_ _j⟩≈_ 1. It remains to show such low-dimensional embeddings, followed by convolutions, SSM, and output layer,
yields the desired solution. The action of the convolution layer on the low-dimensional embeddings is the same as in step 1,
shifting the attention-keys by one position to the right while keeping the attention-queries and attention-values unchanged.
Also the action of the SSM layer is the same as step 1 (77), where the hidden state is a sum of key-value association matrices
_**v**_ _j_ ( _i_ ) _**k**_ _i_ _[⊤]_ [, except with embedding dimension] _[ O]_ [(log] _[ |][V][ |]_ [ + log] _[ κ]_ [)][. At query time, upon encountering] _**[ k]**_ _[i]_ [, the retrieved output]
is _**y**_ _t ∝_ _**v**_ _j_ ( _i_ ) + _ϵ_ ( [�] _l_ = _j_ ( _i_ ) _**[v]**_ _[l]_ [)][ for] _[ ϵ][ ≪]_ [1][, in light of the fact that the low-dimensional embeddings are nearly orthogonal by]

construction. Then, applying the output matrix _Wo_ _**y**_ _t_ yields the maximum component at _j_ ( _i_ ), as desired.


**Remark 7.** _The above construction with N_ = log _κ works for generic inputs where the values are drawn randomly from_
_the vocabulary with sufficient size. Such construction may fail in the adversarial case where most values are the same and_
_the number of keys κ is large. Concretely, suppose the input is_ [ _k_ 1 _, v_ 1 _, k_ 2 _, v, . . ., kκ, v_ ] _, such that all the values at time_
_t_ = 4 _,_ 6 _, . . .,_ 2 _κ are the same token v, and v_ 1 _̸_ = _v. Then at the retrieval time for the query token k_ 1 _, the signal value is_ _**v**_ 1 _,_
_whereas the noisy values from other keys contribute to ϵ_ ( _κ −_ 1) _**v**_ _. Then if ϵ_ ( _κ −_ 1) _>_ 1 _, the model might fail to retrieve the_
_desired value. This can be counteracted by decreasing ϵ; as per JL Lemma, though, this might come at the cost of scaling_
_the embedding dimension — without however impacting its logarithmic behavior (remember p_ = _O_ ( _ϵ_ _[−]_ [2] log _d_ ) _)._


**Theorem 4.** _There exists a_ 1 _-layer Mamba model with an S4D mixer that solves_ MQAR _with κ pairs, using embedding_
_size d_ = _O_ ( _κ_ log _|V |_ ) _, and state size N_ = 1 _._


_Proof._ The proof is divided into two steps: we first construct a 1-layer Mamba model using S4D as SSM layer, that solves
MQAR using _d_ = _O_ ( _κ|V |_ ); we then apply JL Lemma with a shifting trick to complete the proof for _d_ = _O_ ( _κ_ log _|V |_ ).


**Step 1: Construction With** _d_ = _O_ ( _κ|V |_ ) **.** On a high level, the idea is to organize the hidden state of the SSM in chunks,
each collecting a vector representing the value associated to a specific key, similarly to the proof in Thm. 2. However, unlike
the Mamba layer, the S4D layer does not have access to input-dependent matrices _B_ ( _**x**_ ) _, C_ ( _**x**_ ), implying that the _N_ columns
of the S4D state are the same up to scaling, and hence cannot encapsulate additional information regarding the input. We
then decide to work with a single-column as hidden state, and instead partition it along the embedding dimension _d_ . Ideally,
we want the hidden state before retrieval to be a long column vector,


_**h**_ _t_ = [ _**v**_ 1 _[⊤]_ _|_ _**v**_ 2 _[⊤]_ _|_ _. . ._ _|_ _**v**_ _κ_ _[⊤]_ ] _[⊤]_ _∈_ R _[κ][|][V][ |]_ _._ (78)


This can be achieved by specifying each component of the full Mamba architecture as follows.


  - **Embedding** The goal of the embedding layer is to organize keys and values in a form that is suitable to assemble a


24


hidden state as in (78). More in detail, we let a key _ki_ and a value _vi_ be mapped to, respectively



_ki �→_ _**k**_ _i_ = _k ·_ [0 _, . . .,_ 0 _|_ _. . ._ _|_ 1 _, . . .,_ 1

~~�~~                          - ~~�~~                          _|V |i_ : _|V |_ ( _i_ +1)



_|_ _. . ._ _|_ 0 _, . . .,_ 0 _| . . ._ ] _[⊤]_ (79)



_vi �→_ _**v**_ _i_ = _v ·_ [0 _, . . .,_ 1 _, . . .,_ 0 _|_ _. . ._ _|_ 0 _, . . .,_ 1 _, . . .,_ 0] _[⊤]_ (80)
���� ����
_i_ _κ|V |_ + _i_

= _v ·_ [ _**e**_ _[⊤]_ _i_ _|_ _. . ._ _|_ _**e**_ _[⊤]_ _i_ ] _[⊤]_ _,_ (81)


for some fixed parameters _k >_ 0 _, v >_ 0. Notice that, in light of this, any combination _**k**_ _i_ + _**v**_ _j_ can form a dictionary: it
has a maximum value at a component uniquely defined by the pair _i, j_ .


- **Convolution** The (short) convolution layer is responsible for filtering out irrelevant information from the sequence, and
retaining only the one pertaining ( _**k**_ _i,_ _**v**_ _j_ ) pairs. To this end, we limit ourselves to a convolution with kernel size 2: this
acts on any pair ( _**x**_ _t,_ _**x**_ _t_ +1) of the input sequence by mapping it to


( _**x**_ _t,_ _**x**_ _t_ +1) _�→_ _σ_ ( _**c**_ 0 _⊙_ _**x**_ _t_ + _**c**_ 1 _⊙_ _**x**_ _t_ +1 _−_ _**b**_ ) _._ (82)


By carefully picking the parameters _k, v,_ _**c**_ 0 _,_ _**c**_ 1 _,_ _**b**_, we can ensure that only a ( _**k**_ _i,_ _**v**_ _j_ ) pair “survives” the operation,
and everything else gets mapped to the null vector. This for example can be achieved by setting _k_ = 10 _, v_ = 1 _,_ _**c**_ 0 =
10 _·_ **1** _,_ _**c**_ 1 = **1** _,_ _**b**_ = _k_ _**c**_ 0. With this choice, we see that the convolution maps


( _**k**_ _i,_ _**v**_ _j_ ) _�→_ [0 _, . . .,_ 0 _|_ _. . ._ _|_ 0 _, . . .,_ 1 _, . . .,_ 0 _|_ _. . ._ _|_ 0 _, . . .,_ 0] _[⊤]_ (83)
����
_|V |i_ + _j_

= [ **0** _[⊤]_ _|_ _. . ._ _|_ _**e**_ _[⊤]_ _j_ _|_ _. . ._ _|_ **0** _[⊤]_ ] _[⊤]_ (84)

( _**k**_ _i,_ _**k**_ _j_ ) _�→_ [0 _, . . .,_ 0] _[⊤]_ (85)

( _**v**_ _i,_ _**k**_ _j_ ) _�→_ [0 _, . . .,_ 0] _[⊤]_ (86)

( _**v**_ _i,_ _**v**_ _j_ ) _�→_ [0 _, . . .,_ 0] _[⊤]_ _,_ (87)


- **S4D SSM** The SSM layer simply needs to accumulate and propagate the combined values down the sequence. We
remind that from (5) and (6), the output of S4D is given by



_**h**_ [S4D] _t_ =




- _t_ _**x**_ _s ·_ - **Λ** _t−_ ( _s_ +1) _**B**_ _s_ - _⊤_ _._ (88)


_s_ =1



We make use of a “trivial” SSM where **Λ** = _**B**_ = 1, resulting in a hidden state which, after collecting the initial
( _**k**_ _i,_ _**v**_ _j_ ( _i_ )) pairs, is constant in the form:


_**h**_ _t_ = [0 _, . . .,_ 1 _, . . .,_ 0 _|_ 0 _, . . .,_ 1 _, . . .,_ 0 _|_ _. . ._ _|_ 0 _, . . .,_ 1 _, . . .,_ 0] _[⊤]_
���� ���� ����
_j_ 1 _|V |_ + _j_ 2 _κ|V |_ + _jκ_ (89)

= [ _**e**_ _[⊤]_ _j_ 1 _|_ _**e**_ _[⊤]_ _j_ 2 _|_ _. . ._ _|_ _**e**_ _[⊤]_ _jκ_ ] _[⊤]_ _._


- **Gate** The role of the gating mechanism (14g) is to retrieve the part of the hidden state which refer to the requested key.
The gate branch acts on a linear transformation of the original sequence: by picking this transformation as the identity.
we ensure that, when a key is encountered, only the corresponding value is retrieved from the hidden state, in fact:


_**y**_ ˜ _t_ = _**k**_ _i ⊙_ _**h**_ _t_ = [0 _, . . .,_ 0 _| . . . |_ 0 _, . . .,_ 1 _, . . .,_ 0 _|_ 0 _, . . .,_ 0 _| . . ._ ] _._ (90)
����
_|V |i_ + _j_ ( _i_ )


- **Output** The final output layer simply needs to test the retrieved vector _**y**_ _t_ against all values _**v**_ _i_ : only the correct one
will return a scalar product different from 0. It suffices to pick






 =


25



_**v**_ 1 _[⊤]_
...
_**v**_ _|_ _[⊤]_ _V |_










_**e**_ _[⊤]_ 1 _. . ._ _**e**_ _[⊤]_ 1
... ... ...
_**e**_ _[⊤]_ _|V |_ _. . ._ _**e**_ _[⊤]_ _|V |_






 _._ (91)



_Wo_ [S4D] =









**Step 2: Dimensionality Reduction to** _d_ = _O_ ( _κ_ log( _|V |_ + 1)) **.** Having proved the construction with _d_ = _O_ ( _|V |_ ), we now
apply JL Lemma to reduce the embedding dimension and suitably adjust the Mamba architecture weights to ensure the
desired output. Concretely:


  - **Embedding** We use JL lemma to identify _d_ = _|V |_ + 1 almost-orthogonal vectors within a _p ∼_ _O_ (log( _|V |_ + 1))dimensional space. Namely, there exists a random matrix _M_ : R _[d]_ _→_ R _[p]_ such that any two (different) vectors in the set
_{M_ _**e**_ 1 _, . . ., M_ _**e**_ _|V |, M_ _**e**_ _|V |_ +1 _}_ are almost-orthogonal, in the sense that _|⟨M_ _**e**_ _i, M_ _**e**_ _j⟩| < ϵ_ for some small _ϵ ∈_ (0 _,_ 0 _._ 5).
Without loss of generality, we ensure that the last of these vectors is parallel to the all-one vector **1** _p_ : notice this is
always possible via an opportune rotation, which does not affect the scalar product of the recovered vectors (and hence,
their almost-orthogonality). We let the embedding layer perform the following map



_ki �→_ _**k**_ _i_ = _k ·_ [0 _, . . .,_ 0 _|_ _. . ._ _|_ 1 _, . . .,_ 1

~~�~~ ~~��~~                        _pi_ : _p_ ( _i_ +1)



_|_ _. . ._ _|_ 0 _, . . .,_ 0 _| . . ._ ] _[⊤]_ _∈_ R _[κp]_ _,_ _i_ = 1 _. . . κ_ (92)



_vi �→_ _**v**_ _i_ = [( _M_ _**e**_ _i_ + _β_ **1** ) _[⊤]_ _|_ _. . ._ _|_ ( _M_ _**e**_ _i_ + _β_ **1** ) _[⊤]_ ] _[⊤]_ _∈_ R _[κp]_ _,_ _i_ = 1 _. . . |V |,_ (93)


where _M_ : R _[d]_ _→_ R _[p]_, _Mi,j ∈{−_ ~~_√_~~ [1] ~~_p_~~ _,_ ~~_√_~~ 1 ~~_p_~~ _}_ is the (rotated) projection matrix recovered with JL Lemma, and _β >_ ~~_√_~~ 1 ~~_p_~~,

so that each component of the value embedding _**v**_ _i_ is nonnegative, and falls in the range of [ _β −_ ~~_√_~~ 1 ~~_p_~~ _, β_ + ~~_√_~~ 1 ~~_p_~~ ].


- **Convolution** We use a size-2 convolution as in step 1, requiring it to retain only the ( _**k**_ _i,_ _**v**_ _j_ ) pair information while
sending other pairs ( _**v**_ _i,_ _**k**_ _j_ ) _,_ ( _**v**_ _i,_ _**v**_ _j_ ) _,_ ( _**k**_ _i,_ _**k**_ _j_ ) to zero. To this end, we let


_β_ = 1 _,_ _k_ = 10 _,_ _c_ 0 = 10 _,_ _c_ 1 = 1 _,_ _b_ = _kc_ 0 _._ (94)


Note that by the choice of _β_ and _p ≥_ 1, the range of components in _**v**_ _i_ is limited to [0 _,_ 2]. Consequently, we have


conv( _**k**_ _i,_ _**v**_ _j_ ) = ReLU( _c_ 0 _**k**_ _i_ + _c_ 1 _**v**_ _j −_ _b_ ) = [ **0** _[⊤]_ _|_ _. . ._ _|_ ( _M_ _**e**_ _j_ + _β_ **1** ) _[⊤]_ _|_ _. . ._ _|_ **0** _[⊤]_ ] _[⊤]_ ; (95)

conv( _**v**_ _i,_ _**k**_ _j_ ) = conv( _**k**_ _i,_ _**k**_ _j_ ) = conv( _**v**_ _i,_ _**v**_ _j_ ) = 0 _._ (96)


- **S4D SSM** The SSM layer proceeds similarly as the construction in step 1, yielding the state at _t >_ 2 _κ_ as


_**h**_ _t_ = [( _M_ _**e**_ _j_ 1 + _β_ **1** ) _[⊤]_ _|_ ( _M_ _**e**_ _j_ 2 + _β_ **1** ) _[⊤]_ _|_ _. . ._ _|_ ( _M_ _**e**_ _jκ_ + _β_ **1** ) _[⊤]_ ] _[⊤]_ _._ (97)


- **Gate and Output** Also the gating layer and the output layer proceed similarly as the construction in step 1. After
gating, when a key _ki_ is encountered, we have



_**y**_ ˆ _t_ = _**h**_ _t ⊙_ _**k**_ _i_ = _·_ [0 _, . . .,_ 0 _|_ _. . ._ _|_ ( _M_ _**e**_ _ji_ + _β_ **1** ) _[⊤]_


~~�~~ ~~��~~ ~~�~~
_pi_ : _p_ ( _i_ +1)


And finally, after applying the output matrix, we obtain



_|_ _. . ._ _|_ 0 _, . . .,_ 0 _| . . ._ ] _[⊤]_ _._ (98)



**0**
...
( _M_ _**e**_ _ji_ + _β_ **1** )
...
**0**



_ϵ_ + _ϵβ_
...
1 + _ϵβ_
...
_ϵ_ + _ϵβ_















( _M_ _**e**_ 1) _[⊤]_ _. . ._ ( _M_ _**e**_ 1) _[⊤]_
... ... ...
( _M_ _**e**_ _|V |_ ) _[⊤]_ _. . ._ ( _M_ _**e**_ _|V |_ ) _[⊤]_
















_Wo_ [S4D] _**y**_ ˆ _t_ =










_≈_









_,_ (99)



in light of the fact that both ( _M_ _**e**_ _i_ ) _[⊤]_ _· M_ _**e**_ _j ≈_ _ϵ_ if _i ̸_ = _j_ otherwise ( _M_ _**e**_ _i_ ) _[⊤]_ _· M_ _**e**_ _i ≈_ 1 per JL construction, and
( _M_ _**e**_ _i_ ) _[⊤]_ _·_ **1** _≤_ _ϵ_ per the assumption that _M_ _**e**_ _|V |_ +1 is parallel to **1** _p_ . This allows us to recover the correct value,
completing the proof.


26


**C.2. Proofs of Mamba Solving the INDUCTION HEADS Task**


**The INDUCTION HEADS Task** As a reminder, for the INDUCTION HEADS task, the input is a sequence of tokens

[ _x_ 1 _, . . ., xt_ ] from a finite vocabulary _V_ ; The output is a sequence of tokens [ _y_ 1 _, . . ., yt_ ] from the augmented vocabulary
_V ∪{×}_, where _yi_ equals the input token right after the latest previous occurrence of the input token _xi_, i.e., _yi_ = _xj_ ( _i_ )+1
where _j_ ( _i_ ) = max _{j_ : _j < i, xj_ = _xi}_ ; otherwise _yi_ = _×_ . An example input and output drawn from the vocabulary
_V_ = _{_ 1 _,_ 2 _,_ 3 _,_ 4 _}_ with sequence length 8 is shown below:


_t_ = [ 1 _,_ 2 _,_ 3 _,_ 4 _,_ 5 _,_ 6 _,_ 7 _,_ 8]
_x_ = [ 2 _,_ 1 _,_ 3 _,_ 2 _,_ 4 _,_ 3 _,_ 2 _,_ 4] _._
_y_ = [ _×,_ _×,_ _×,_ 1 _,_ _×,_ 2 _,_ 4 _,_ 3]


Note that the input token 2 appears three times, at instants _t_ = 1 _,_ 4 _,_ 7. Thus, at _t_ = 7, the _latest_ previous occurrence of
_j_ (7) = 4, which yields the output _y_ 7 = _xj_ (7)+1 = _x_ 4+1 = 4.

**Lemma 3.** _There exists a_ 1 _-layer Mamba model with the Mamba-_ ∆ _[⊤]_ _SSM mixer (15) that solves_ INDUCTION HEADS
_with vocabulary V using embedding size d_ = 2 _|V | and state size N_ = _|V |._


_Proof._ We follow a procedure similar to the MQAR construction, in that we leverage the matrix structure in the hidden
state such that its columns are indexed by the key token and store the associated value token. However, differently from
the MQAR task, in the INDUCTION HEADS task there is no distinction between the key and value set, but rather all
tokens are drawn from the same vocabulary _V_ - i.e., each token _xi_ acts as key in the ( _xi, xi_ +1) pair, and as value in the
( _xi−_ 1 _, xi_ ) pair. To resolve this, we use a doubling-embedding trick in Mamba, together with suitable choices of convolution.
Moreover, the INDUCTION HEADS task requires finding the _latest_ previous occurrence; we will achieve this by leveraging
the input-dependent state matrix.


The main idea is to _double_ the embedding size in Mamba, which enables the convolution layer to perform _concatenations_ of
the adjacent embedding pairs. Concretely: we let the state size _N_ = _|V |_, and the embedding size _d_ = 2 _|V |_ . We design the
architecture as follows.


  - **Embedding** We use 2 _|V |_ -dimensional standard basis vectors to embed the vocabulary _V_ = _{_ 1 _,_ 2 _, . . ., |V |}_, i.e.,



_vi �→_ - _**e**_ _vi_
_**e**_ _vi_




_∈_ R _[d]_ _≡_ R [2] _[|][V][ |]_ _._ (100)




- **Convolution** We use size-2 convolution (with left-padding **0** ) combining the pair ( _**x**_ _i−_ 1 _,_ _**x**_ _i_ ) by summing the first
_|V |_ -dimensions of _**x**_ _i−_ 1 with the last _|V |_ -dimensions of _**x**_ _i_, effectively _concatenating_ ( _**e**_ _xi−_ 1 _,_ _**e**_ _xi_ ). We let




                         - **1**
_**x**_ ˆ _i ≡_ conv( _**x**_ _i−_ 1 _,_ _**x**_ _i_ ) = _**c**_ 0 _⊙_ _**x**_ _i−_ 1 + _**c**_ 1 _⊙_ _**x**_ _i,_ where _**c**_ 0 =
**0**




- - **0**
_,_ _**c**_ 1 =
**1**




_._ (101)



Note that we describe the proof for _linear_ convolution here to simplify notation, but it holds also for nonlinear
convolution (14b), noting that each embedding vector _**x**_ _i_ and the convolution weights _**c**_ 0 _,_ _**c**_ 1 are nonnegative, effectively
reducing the nonlinearity _σ_ = ReLU (or SiLU) to be the identity map. The same reasoning applies to the design of
_**B**_ _,_ _**C**_, as we discuss next.


- **Mamba-** ∆ _[⊤]_ **SSM** Since the convolved output ˆ _**x**_ _i_ contains _xi−_ 1 in its first _|V |_ dimensions and _xi_ in its last _|V |_
dimensions, we choose the state matrix **Λ** and the input matrix _**B**_ to depend on the first _|V |_ dimensions of ˆ _**x**_ (i.e.,
extracting the key), and the output matrix _**C**_ to depend on the last _|V |_ dimensions (i.e., extracting the query). To this
end, we let **Λ** = _−_ **1** _∈_ R _[d][×][N]_ _, w_ ∆ _≫_ 0, and


∆(ˆ _**x**_ _i_ ) := SoftPlus(Linear(ˆ _**x**_ _i_ )) _,_ where Linear(ˆ _**x**_ _i_ ) := _w_ ∆[ _**I**_ _|V | |_ **0** _|V |_ ]ˆ _**x**_ _i ∈_ R _[N]_ _,_ (102a)

**Λ** (ˆ _**x**_ _i_ ) := _e_ **[Λ]** _[⊙]_ [(] **[1]** _[d][⊗]_ [∆(ˆ] _**[x]**_ _[i]_ [))] = _e_ _[−]_ **[1]** _[⊗]_ [∆(ˆ] _**[x]**_ _[i]_ [)] _∈_ R _[d][×][N]_ _≡_ R _[|][V][ |×|][V][ |]_ _,_ (102b)

_**B**_ (ˆ _**x**_ _i_ ) := Linear(ˆ _**x**_ _i_ ) = [ _**I**_ _|V | |_ **0** _|V |_ ] ˆ _**x**_ _i ∈_ R _[N]_ _,_ (102c)

_**C**_ (ˆ _**x**_ _i_ ) := Linear(ˆ _**x**_ _i_ ) = [ **0** _|V | |_ _**I**_ _|V |_ ] ˆ _**x**_ _i ∈_ R _[N]_ _._ (102d)


27


  - **Output** _Wo_ = [ **0** _|V | | I|V |_ ] _∈_ R _[|][V][ |×][d]_


We now show the correctness of such construction. Consider the generic input and output sequences:


_t_ = [ 1 _,_ 2 _,_ 3 _,_ 4 _,_ 5 _,_ 6 _,_ 7 _,_ 8 _, . . ._ ]
_x_ = [ _v_ 2 _,_ _v_ 1 _,_ _v_ 3 _,_ _v_ 2 _,_ _v_ 4 _,_ _v_ 3 _,_ _v_ 2 _,_ _v_ 4 _, . . ._ ] (103)
_y_ = [ _×,_ _×,_ _×,_ _v_ 1 _,_ _×,_ _v_ 2 _,_ _v_ 4 _,_ _v_ 3 _, . . ._ ] _._


After the embedding and convolution layers, the SSM input is a sequence of _d_ -dimensional vectors for _d_ = 2 _|V |_,



_**x**_ ˆ =        - **0** _**e**_ _v_ 2 _**e**_ _v_ 1 _**e**_ _v_ 3 _**e**_ _v_ 2 _**e**_ _v_ 4 _**e**_ _v_ 3 _**e**_ _v_ 2 _. . ._
_**e**_ _v_ 2 _**e**_ _v_ 1 _**e**_ _v_ 3 _**e**_ _v_ 2 _**e**_ _v_ 4 _**e**_ _v_ 3 _**e**_ _v_ 2 _**e**_ _v_ 4 _. . ._


where ˆ _**x**_ _i_ stores the ( _xi−_ 1 _, xi_ ) pair.




_∈_ R [2] _[|][V][ |×][t]_ _,_ (104)



The action of the SSM layer organizes the hidden state matrix of size _d × N ≡_ 2 _|V | × |V |_ by the input matrix _**B**_ (ˆ _**x**_ _i_ ) taking
outer-product with ˆ _**x**_ _i_, followed by retrieving the desired column via the output matrix _**C**_ (ˆ _**x**_ _i_ ). Now by the choice of SSM
parameters, we have

        - _**B**_ (ˆ _**x**_ )        -        - **0** _**e**_ _v_ _**e**_ _v_ _**e**_ _v_ _**e**_ _v_ _**e**_ _v_ _**e**_ _v_ _**e**_ _v_ _. . ._        



_∈_ R [2] _[|][V][ |×][t]_ _,_ (105)



_**C**_ (ˆ _**x**_ )




- = - **0** _**e**_ _v_ 2 _**e**_ _v_ 1 _**e**_ _v_ 3 _**e**_ _v_ 2 _**e**_ _v_ 4 _**e**_ _v_ 3 _**e**_ _v_ 2 _. . ._
_**e**_ _v_ 2 _**e**_ _v_ 1 _**e**_ _v_ 3 _**e**_ _v_ 2 _**e**_ _v_ 4 _**e**_ _v_ 3 _**e**_ _v_ 2 _**e**_ _v_ 4 _. . ._



where we stack them together to visualize that _**B**_ (ˆ _**x**_ ) amounts to shifting _**C**_ (ˆ _**x**_ ) to the right by one position, due to the
design of conv _B_ .


We now verify the desired behavior in the SSM layer. Suppose temporarily the state matrix is **Λ** = **1** _∈_ R _[d][×][N]_ . Then the
hidden state at time _s ≤_ _t_ would be a cumulative sum,



_s_



_i_ =2




_**e**_ _[⊤]_ _xi−_ 1 _[.]_ (106)



_**h**_ _s_ =



_s_

- _**x**_ ˆ _i_ _**B**_ (ˆ _**x**_ _i_ ) _[⊤]_ =


_i_ =1




- _**e**_ _xi−_ 1
_**e**_ _xi_



Thus, the _j_ -th column of the hidden state matrix would store the sum of all ˆ _**x**_ _i_ where the key _**B**_ (ˆ _**x**_ _i_ ) = _**x**_ _i−_ 1 = _**e**_ _j_ . Yet
the INDUCTION HEADS task requires storing the _latest_ associated value only (not _all_ associated values). To this end, we
leverage the input-dependence of the state matrix, and particularly of ∆( _xt_ ). Recall the Mamba-∆ _[⊤]_ layer is given by


_**h**_ _s_ = _e_ **[Λ]** _[⊙]_ [(] **[1]** _[⊗]_ [∆(ˆ] _**[x]**_ _[s]_ [))] _⊙_ _**h**_ _s−_ 1 + ˆ _**x**_ _s_ _**B**_ (ˆ _**x**_ _s_ ) _[⊤]_ _,_ (107)

_**y**_ _s_ = _**h**_ _s_ _**C**_ (ˆ _**x**_ _s_ ) _._ (108)


We design ∆(ˆ _**x**_ _t_ ) _∈_ R _[N]_ _≡_ R _[|][V][ |]_ such that when the input contains the key information, the corresponding key column in
the state is _erased_ (while the other columns remain the same). Without loss of generality, suppose _**B**_ (ˆ _**x**_ _s_ ) = _**e**_ _j_ . By the
definition of _**B**_ (102c), this implies that [ _**I**_ _|V | |_ **0** _|V |_ ]ˆ _**x**_ _s_ = _**e**_ _j_ . By the choice of _w_ ∆ _≫_ 0 and the definition of ∆ (102a), we
have
∆(ˆ _**x**_ _i_ ) = SoftPlus( _w_ ∆[ _**I**_ _d |_ **0** _d_ ]ˆ _**x**_ _i_ ) = _w_ ∆ _**e**_ _j._ (109)


Therefore (102b) yields the state matrix as


**Λ** _s_ = _e_ _[−]_ **[1]** _[⊗]_ [∆(ˆ] _**[x]**_ _[s]_ [)] = _e_ _[−]_ **[1]** _[⊗]_ [(] _[w]_ [∆] _**[e]**_ _[j]_ [)] = **1** _⊗_ exp [0 _, . . ., −w_ ∆ _, . . .,_ 0] _[⊤]_ _[w]_ [∆] = _[→∞]_ **1** _⊗_ ( **1** _−_ _**e**_ _j_ ) _∈_ R _[d][×][N]_ _,_ (110)
~~����~~
_j_


which has an all-zeros _j_ -th column and all-ones columns elsewhere. Consequently, the _j_ -th column of the hidden state
_**h**_ _s_ [: _, j_ ] is erased by the action _e_ _[−]_ **[1]** _[⊗]_ [∆(ˆ] _**[x]**_ _[s]_ [)] _⊙_ _**h**_ _s−_ 1, and then updated with the current input containing the latest value by
the action ˆ _**x**_ _s ⊗_ _**B**_ (ˆ _**x**_ _s_ ), as desired. We remark that such erasure operation is akin to the construction of the S6 layer for
solving the KEEP _n_ -TH task in Cor. 2, in which we have Mamba approximate a Heaviside by tweaking ∆( _**x**_ _t_ ), so to erase
information from all tokens before a given one. We also see that such _selective_ erasure works consistently well for long
sequences when _t →∞_, since it preserves all other columns (except the _j_ -th one) by setting ∆( _**u**_ _s_ )[ _l_ ] = 0 for _l ̸_ = _j_, and
thereby satisfying the condition in Lem. 2 to avoid sensitivity decay.


28


Finally, the SSM output is given by _**y**_ _s_ = _**h**_ _s_ _**C**_ _s_ = _**h**_ _s_ _**e**_ _xs_, which retrieves the _xs_ -th column of the state that stores the



token immediately after the _latest_ previous occurrence of _xs_, i.e. _**y**_ _s_ = - _**e**_ _xj_ ( _s_ )
_**e**_ _xj_ ( _s_ )+1

_Wo_ _**y**_ _s_ = _**e**_ _xj_ ( _s_ )+1 to obtain the target value, which completes the proof.


29




. We then apply the output matrix


**D. Additional Experiment Details**


**D.1. Training Details**


For all experiments, unless otherwise noted, we train with the Adam optimizer (Kingma & Ba, 2014), for 600 epochs
using an initial learning rate _η_ = 0 _._ 03 and cosine annealing down to _η_ = 1 _×_ 10 _[−]_ [6] . The training set is composed of 10 [5]

randomly generated samples with a fixed seed and the batch size is 16, which results in up to 3 _._ 75 _·_ 10 [6] gradient updates.
We perform early stopping if the validation loss reaches below 10 _[−]_ [6] or if six hours have elapsed since the beginning of
training. The validation set and test sets have 10 [3] and 10 [5] samples respectively and are generated with the same function but
using different seeds. Reported accuracies are always obtained from the test set after the last epoch of training.


**D.2. Task KEEP** _n_ **-TH**


In this section, we provide additional details and ablation of the KEEP _n_ -TH task used for Sec. 4, with experimental set-up
and partial results reported in Tab. 1.


**Model** All the results in Tab. A.1 - Tab. A.4 use 1-layer models. The MAMBA and S4D models (with or without PE) are
simplified architectures, which consists of embedding layer, SSM layer, and output linear layer, _without convolution nor_
_gating_ from the Mamba mixer block. The simplification is intended to investigate the role of the SSM layer alone (i.e. S6
versus S4D), without confounding factors from other components in the mixer block.


_i.i.d_
**Experiment Set-up** For each input sequence _**x**_ = ( _x_ 1 _, . . ., xT_ ), we draw _xi_ _∼_ Unif( _{_ 1 _, . . ., |V |}_ ) randomly from a
vocabulary with size _|V |_ = 128. The target output is the _n_ -th token in the input sequence, i.e. _yt_ = _xn_ for _n < t ≤_ _T_ . The
predicted output at time _t_ = _n, . . ., T_ are taken to compute (cross-entropy) loss for training, and accuracy for evaluation.


**Discussion** When equipped with PE, Mamba manages to achieve 100% accuracy on KEEP _n_ -TH, regardless of sequence
length. This is thanks to its ability to dynamically adjust (via ∆( _xt_ )) for how long the hidden state retains memory of the
target token (in position _n_ = 5). On the other hand, S4D is lacking such ability, and already fails at the task for _T_ = 20
(Tab. A.1 and A.2). Then again Transformers do not need to retain memory, as they can look back to the whole sequence at
each step, in light of their attention mechanism, and have no issue solving the task for any _T_ . When removing PE from
Mamba, however (Tab. A.3 and A.4), the model loses its way to discriminate the specific token that must be retrieved, and
performance drops to that of S4D, as expected.


Table A.1: Ablation on KEEP _n_ -TH: MAMBA+PE, S4D+PE, TRANSFORMERS with varying sequence length _T_ = 10 _,_ 20,
embedding dimension _d_, and state size _N_ .


T=10 T=20
acc. # epch. # prm. acc. # epch. # prm.



MAMBA
(+PE)


S4D
(+PE)



d=8, N=8 1.00 (0.00) 107 2.2k 1.00 (0.00) 37 2.2k
d=8, N=32 1.00 (0.00) 241 2.8k 1.00 (0.00) 39 2.8k
d=8, N=64 1.00 (0.00) 161 3.5k 1.00 (0.00) 47 3.5k
d=32, N=8 1.00 (0.00) 56 9.2k 1.00 (0.00) 15 9.2k
d=32, N=32 1.00 (0.00) 19 11.5k 1.00 (0.00) 14 11.5k
d=32, N=64 1.00 (0.00) 88 14.5k 1.00 (0.00) 19 14.5k
d=64, N=8 0.99 (0.00) 553 18.7k 0.99 (0.01) 208 18.7k
d=64, N=32 1.00 (0.00) 429 23.3k 1.00 (0.00) 12 23.3k
d=64, N=64 1.00 (0.00) 345 29.4k 1.00 (0.00) 12 29.4k

d=8, N=8 0.99 (0.00) 600 2.1k 0.43 (0.04) 600 2.1k
d=8, N=32 0.99 (0.00) 600 2.5k 0.46 (0.05) 600 2.5k
d=8, N=64 0.99 (0.00) 600 3.0k 0.43 (0.03) 600 3.0k
d=32, N=8 0.94 (0.00) 600 8.7k 0.72 (0.01) 600 8.7k
d=32, N=32 0.93 (0.01) 600 10.3k 0.72 (0.01) 600 10.3k
d=32, N=64 0.93 (0.01) 600 12.4k 0.73 (0.01) 600 12.4k
d=64, N=8 0.79 (0.00) 600 17.5k 0.14 (0.01) 600 17.5k
d=64, N=32 0.79 (0.00) 600 20.6k 0.14 (0.00) 600 20.6k
d=64, N=64 0.79 (0.00) 600 24.8k 0.14 (0.00) 600 24.8k



l=1, d=16 1.00 (0.00) 16 5.8k 1.00 (0.00) 19 6.0k
TRANSl=1, d=32 1.00 (0.00) 12 15.1k 1.00 (0.00) 12 15.4k
FORMER
l=1, d=64 1.00 (0.00) 10 42.3k 1.00 (0.00) 9 42.9k


30


Table A.2: Ablation on KEEP _n_ -TH: MAMBA+PE, S4D+PE, TRANSFORMERS with varying sequence length _T_ = 30 _,_ 40 _,_ 50,
embedding dimension _d_, and state size _N_ .


T=30 T=40 T=50
acc. # epch. # prm. acc. # epch. # prm. acc. # epch. # prm.



MAMBA
(+PE)


S4D
(+PE)



d=8, N=8 0.94 (0.04) 456 2.2k 0.85 (0.15) 325 2.2k 0.22 (0.15) 600 2.2k
d=8, N=32 1.00 (0.00) 285 2.8k 0.58 (0.37) 416 2.8k 0.28 (0.10) 600 2.8k
d=8, N=64 1.00 (0.00) 180 3.5k 0.59 (0.31) 420 3.5k 0.98 (0.02) 346 3.5k
d=32, N=8 1.00 (0.00) 20 9.2k 1.00 (0.00) 50 9.2k 1.00 (0.00) 241 9.2k
d=32, N=32 1.00 (0.00) 20 11.5k 1.00 (0.00) 21 11.5k 1.00 (0.00) 225 11.5k
d=32, N=64 1.00 (0.00) 19 14.5k 1.00 (0.00) 23 14.5k 1.00 (0.0) 130 14.5k
d=64, N=8 1.00 (0.00) 17 18.7k 1.00 (0.00) 164 18.7k 0.99 (0.00) 600 18.7k
d=64, N=32 1.00 (0.00) 24 23.3k 1.00 (0.00) 66 23.3k 0.99 (0.00) 600 23.3k
d=64, N=64 1.00 (0.00) 29 29.4k 1.00 (0.00) 226 29.4k 0.98 (0.01) 600 29.4k

d=8, N=8 0.14 (0.04) 600 2.1k 0.03 (0.00) 600 2.1k 0.03 (0.00) 600 2.1k
d=8, N=32 0.46 (0.05) 600 2.5k 0.15 (0.01) 600 2.5k 0.04 (0.01) 600 2.5k
d=8, N=64 0.43 (0.03) 600 3.0k 0.13 (0.02) 600 3.0k 0.04 (0.00) 600 3.0k
d=32, N=8 0.72 (0.01) 600 8.7k 0.09 (0.00) 600 8.7k 0.08 (0.00) 600 8.7k
d=32, N=32 0.72 (0.01) 600 10.3k 0.09 (0.00) 600 10.3k 0.09 (0.00) 600 10.3k
d=32, N=64 0.73 (0.01) 600 12.4k 0.09 (0.00) 600 12.4k 0.09 (0.00) 600 12.4k
d=64, N=8 0.14 (0.01) 600 17.5k 0.09 (0.00) 600 17.5k 0.09 (0.00) 600 17.5k
d=64, N=32 0.14 (0.00) 600 20.6k 0.10 (0.00) 600 20.6k 0.10 (0.00) 600 20.6k
d=64, N=64 0.14 (0.00) 600 24.8k 0.09 (0.00) 600 24.8k 0.09 (0.00) 600 24.8k



l=1, d=16 1.00 (0.00) 16 5.8k 1.00 (0.00) 23 6.3k 1.00 (0.00) 17 6.4k
TRANSl=1, d=32 1.00 (0.00) 12 15.1k 1.00 (0.00) 12 16.0k 1.00 (0.00) 13 16.4k
FORMER
l=1, d=64 1.00 (0.00) 9 42.3k 1.00 (0.00) 9 44.2k 1.00 (0.00) 9 44.9k


Table A.3: Ablation on KEEP _n_ -TH: MAMBA and S4D with varying sequence length _T_ = 10 _,_ 20, embedding dimension _d_,
and state size _N_ .


T=10 T=20
acc. # epch. # prm. acc. # epch. # prm.



MAMBA


S4D



d=8, N=8 0.20 (0.01) 600 2.3k 0.05 (0.00) 600 2.3k
d=8, N=32 0.18 (0.03) 600 2.9k 0.05 (0.00) 600 2.9k
d=8, N=64 0.17 (0.02) 600 3.6k 0.05 (0.00) 600 3.6k
d=32, N=8 0.21 (0.01) 600 9.3k 0.10 (0.00) 600 9.3k
d=32, N=32 0.25 (0.03) 600 11.6k 0.11 (0.00) 600 11.6k
d=32, N=64 0.28 (0.03) 600 14.7k 0.11 (0.00) 600 14.7k
d=64, N=8 0.19 (0.00) 600 18.8k 0.11 (0.00) 600 18.8k
d=64, N=32 0.20 (0.00) 600 23.4k 0.11 (0.00) 600 23.4k
d=64, N=64 0.21 (0.00) 600 29.6k 0.11 (0.00) 600 29.6k

d=8, N=8 0.08 (0.00) 600 2.2k 0.04 (0.00) 600 2.2k
d=8, N=32 0.08 (0.00) 600 2.6k 0.04 (0.00) 600 2.6k
d=8, N=64 0.08 (0.00) 600 3.2k 0.04 (0.00) 600 3.2k
d=32, N=8 0.20 (0.00) 600 8.8k 0.10 (0.00) 600 8.8k
d=32, N=32 0.20 (0.00) 600 10.4k 0.10 (0.00) 600 10.4k
d=32, N=64 0.20 (0.00) 600 12.5k 0.10 (0.00) 600 12.5k
d=64, N=8 0.20 (0.00) 600 17.7k 0.11 (0.00) 600 17.7k
d=64, N=32 0.20 (0.00) 600 20.8k 0.11 (0.00) 600 20.8k
d=64, N=64 0.20 (0.00) 600 24.9k 0.11 (0.00) 600 24.9k


31


Table A.4: Ablation on KEEP _n_ -TH: MAMBA and S4D with varying sequence length _T_ = 30 _,_ 40 _,_ 50, embedding dimension
_d_, and state size _N_ .


T=30 T=40 T=50
acc. # epch. # prm. acc. # epch. # prm. acc. # epch. # prm.



MAMBA


S4D



d=8, N=8 0.04 (0.00) 600 2.3k 0.04 (0.00) 600 2.3k 0.03 (0.00) 600 2.3k
d=8, N=32 0.04 (0.00) 600 2.9k 0.04 (0.00) 600 2.9k 0.03 (0.00) 600 2.9k
d=8, N=64 0.04 (0.00) 600 3.6k 0.04 (0.00) 600 3.6k 0.03 (0.00) 600 3.6k
d=32, N=8 0.09 (0.00) 600 9.3k 0.09 (0.00) 600 9.3k 0.08 (0.00) 600 9.3k
d=32, N=32 0.09 (0.00) 600 11.6k 0.09 (0.00) 600 11.6k 0.08 (0.00) 600 11.6k
d=32, N=64 0.09 (0.00) 600 14.7k 0.09 (0.00) 600 14.7k 0.08 (0.00) 600 14.7k
d=64, N=8 0.09 (0.00) 600 18.8k 0.09 (0.00) 600 18.8k 0.09 (0.00) 600 18.8k
d=64, N=32 0.09 (0.00) 600 23.4k 0.09 (0.00) 600 23.4k 0.09 (0.00) 600 23.4k
d=64, N=64 0.09 (0.00) 600 29.6k 0.09 (0.00) 600 29.6k 0.09 (0.00) 600 29.6k

d=8, N=8 0.03 (0.00) 600 2.2k 0.03 (0.00) 600 2.2k 0.03 (0.00) 600 2.2k
d=8, N=32 0.03 (0.00) 600 2.6k 0.03 (0.00) 600 2.6k 0.03 (0.00) 600 2.6k
d=8, N=64 0.03 (0.00) 600 3.2k 0.03 (0.00) 600 3.2k 0.03 (0.00) 600 3.2k
d=32, N=8 0.08 (0.00) 600 8.8k 0.09 (0.00) 600 8.8k 0.09 (0.00) 600 8.8k
d=32, N=32 0.08 (0.00) 600 10.4k 0.09 (0.00) 600 10.4k 0.09 (0.00) 600 10.4k
d=32, N=64 0.08 (0.00) 600 12.5k 0.09 (0.00) 600 12.5k 0.09 (0.00) 600 12.5k
d=64, N=8 0.09 (0.00) 600 17.7k 0.09 (0.00) 600 17.7k 0.09 (0.00) 600 17.7k
d=64, N=32 0.09 (0.00) 600 20.8k 0.09 (0.00) 600 20.8k 0.09 (0.00) 600 20.8k
d=64, N=64 0.09 (0.00) 600 24.9k 0.09 (0.00) 600 24.9k 0.09 (0.00) 600 24.9k


32


**D.3. Task MQAR**


**Models** All the results in Fig. A.2 use 1-layer models. The Mamba and Mamba-2 models explicitly disable the gating branch. This
simplification is intended to verify our constructions without gating in Thm. 2 and Thm. 3. The Mamba-S4D model retains the full Mamba
architecture, but only swapping the S6 layer with the S4D layer; not to be confused with the original S4D model proposed in (Gu et al.,
2022a).


**Experiment Set-up** We generate the data described in App. C.1 as follows. For each input sequence of the form


_**x**_ = [ _k_ 1 _, v_ 1 _, . . ., kκ, vκ, . . ., | ki_ 1 _, . . ., ki_ 2 ] _,_


_i.i.d_ _i.i.d_
we draw the key token _ki_ _∼_ Unif( _{_ 1 _, . . ., κ}_ ), and value token _vj_ _∼_ Unif( _{_ 1 _, . . ., |V |}_ ). The target output sequence consists of
masked tokens except at the the query chunk where the input query is a key (e.g., at _ki_ 1 _, ki_ 2 in the example above). We compute loss
during training (and accuracy for evaluation) only at the query positions, informed from the target output sequence.


**Discussion** Here we expand on the results from Fig. 2 in the main text, by reporting the accuracy of Mamba, Mamba-2, and Mamba-S4D
trained on MQAR for varying model sizes. In Fig. A.2, we sweep over values of the value vocabulary size _|V |_, to show how our
theoretical bounds hold while varying this parameter. The bounds are still reasonably tight, and the observations drawn from Fig. 2 still
hold in this case. The extra caveat (which does not however invalidate our claim) is that by increasing _|V |_ we are making the task more
difficult to solve, and our training procedure for the simplest Mamba-S4D fails to achieve satisfactory performance for the model sizes
considered. Notice also that, for Mamba-2, the simplest task _κ_ = 4 can achieve 100% accuracy even _below_ the theoretical curve proposed
in our theorems. This can be attributed to the following factors. On the one hand, our bounds rely on JL Lemma, which provides only
_asymptotical_ behaviors which might not be verified in practice for _κ_ so small. On the other hand, it is perfectly feasible that at this regime
the architecture can recover a more efficient solution than the one theorized.


_|V |_ = 256



|MAMBA|Col2|Col3|
|---|---|---|
|4|4|4|
||||
||||
|6|6|6|
|4<br>8<br>16<br>32<br>8|4<br>8<br>16<br>32<br>8|4<br>8<br>16<br>32<br>8|
|4<br>8<br>16<br>32<br>8|8<br>1|6<br>32|


Number of keys ( _κ_ )


_|V |_ = 512

|MAMBA|Col2|Col3|
|---|---|---|
|4|4|4|
|2<br>|2<br>|2<br>|
||||
||||
|4<br>8<br>16<br>32<br>|4<br>8<br>16<br>32<br>|4<br>8<br>16<br>32<br>|
|4<br>8<br>16<br>32<br>|8<br>1|6<br>32|



Number of keys ( _κ_ )












|S4D|Col2|Col3|
|---|---|---|
|4<br>8<br>6|4<br>8<br>6|4<br>8<br>6|
|2<br>|2<br>|2<br>|
|4<br>8<br>16<br>32<br>6<br>|4<br>8<br>16<br>32<br>6<br>|4<br>8<br>16<br>32<br>6<br>|
|4<br>8<br>16<br>32<br>6<br>|8<br>|6<br>32|


|MAMBA-2|Col2|Col3|
|---|---|---|
||||
||||
||||
||8<br>1|6<br>32|


|S4D|Col2|Col3|
|---|---|---|
|8<br>6|8<br>6|8<br>6|
|2<br>4|2<br>4|2<br>4|
|4<br>8<br>16<br>32<br>6<br>|4<br>8<br>16<br>32<br>6<br>|4<br>8<br>16<br>32<br>6<br>|
|4<br>8<br>16<br>32<br>6<br>|8<br>|6<br>32|


|MAMBA-2|Col2|Col3|
|---|---|---|
||||
||||
||||
||8<br>1|6<br>32|



_Figure A.2._ Trained models accuracy on MQAR (best of 7 seeds) across _κ_, and _d_ . For S4D _N_ = 4, for Mamba _N_ = 2 _× κ_, and for
Mamba2 _N_ = 8 _×_ ln _κ_ . _T_ = 100 and _|V | ∈{_ 256 _,_ 512 _}_ for all runs.


With Fig. A.3, we further complement our results by sweeping over values of the _N_ state size parameter. We remind that, according to
Thm. 2 to 4, our theorized MQAR solutions require a value of at least _N_ = 1, _N_ = _κ_ and _N ∼_ log _κ_ for S4D, Mamba and Mamba-2,
respectively. Indeed, in Fig. A.3 we observe that varying _N_ does not have a particular impact on the final accuracy of S4D. For Mamba,
on the other hand, we see that for _N < κ_ the training procedure fails to recover an exact solution to MQAR. Similarly, for Mamba-2, no
solution is recovered for _N <_ 4 log _κ_ . These results further validate the tightness of our theoretical solutions.


33


S4D










|N = 1|Col2|Col3|
|---|---|---|
|4<br>8<br>6|4<br>8<br>6|4<br>8<br>6|
|2<br>|2<br>|2<br>|
|4<br>8<br>16<br>32<br>6<br>|4<br>8<br>16<br>32<br>6<br>|4<br>8<br>16<br>32<br>6<br>|
|4<br>8<br>16<br>32<br>6<br>|8<br>|6<br>32|


|N = 2|Col2|Col3|
|---|---|---|
||||
||||
||||
||||
||||
|4<br>8<br>16<br>32<br>|4<br>8<br>16<br>32<br>|4<br>8<br>16<br>32<br>|
|4<br>8<br>16<br>32<br>|8<br>|6<br>32|


|N = 4|Col2|Col3|
|---|---|---|
|4<br>8<br>6|4<br>8<br>6|4<br>8<br>6|
|2<br>|2<br>|2<br>|
|4<br>8<br>16<br>32<br>6<br>|4<br>8<br>16<br>32<br>6<br>|4<br>8<br>16<br>32<br>6<br>|
|4<br>8<br>16<br>32<br>6<br>|8<br>|6<br>32|


|N = 8|Col2|Col3|
|---|---|---|
|4<br>8<br>6|4<br>8<br>6|4<br>8<br>6|
|2<br>|2<br>|2<br>|
|4<br>8<br>16<br>32<br>6<br>|4<br>8<br>16<br>32<br>6<br>|4<br>8<br>16<br>32<br>6<br>|
|4<br>8<br>16<br>32<br>6<br>|8<br>1|6<br>32|



|N = κ|Col2|Col3|
|---|---|---|
||||
||||
||||
||||
|4<br>8<br>16<br>32<br>|4<br>8<br>16<br>32<br>|4<br>8<br>16<br>32<br>|
|4<br>8<br>16<br>32<br>|8<br>|6<br>32|


Number of keys ( _κ_ )


MAMBA-2

|N = 2 × log κ|Col2|Col3|
|---|---|---|
||||
||||
||8<br>|6<br>32|



Number of keys ( _κ_ )













Number of keys ( _κ_ )


MAMBA


|N = 0.5 × κ|Col2|Col3|
|---|---|---|
|4|4|4|
|2<br>|2<br>|2<br>|
|4<br>8<br>16<br>32<br>8<br>6<br>|4<br>8<br>16<br>32<br>8<br>6<br>|4<br>8<br>16<br>32<br>8<br>6<br>|
|4<br>8<br>16<br>32<br>8<br>6<br>|8<br>|6<br>32|


|N = 2 × κ|Col2|Col3|
|---|---|---|
|4|4|4|
|2<br>|2<br>|2<br>|
|4<br>8<br>16<br>32<br>8<br>6<br>|4<br>8<br>16<br>32<br>8<br>6<br>|4<br>8<br>16<br>32<br>8<br>6<br>|
|4<br>8<br>16<br>32<br>8<br>6<br>|8<br>|6<br>32|


|N = 4 × κ|Col2|Col3|
|---|---|---|
|4|4|4|
|2<br>|2<br>|2<br>|
|4<br>8<br>16<br>32<br>8<br>6<br>|4<br>8<br>16<br>32<br>8<br>6<br>|4<br>8<br>16<br>32<br>8<br>6<br>|
|4<br>8<br>16<br>32<br>8<br>6<br>|8<br>1|6<br>32|


|N = log κ|Col2|Col3|
|---|---|---|
|4<br>8|4<br>8|4<br>8|
||8<br>|6<br>32|


|N = 4 × log κ|Col2|Col3|
|---|---|---|
|4<br>8|4<br>8|4<br>8|
||8<br>|6<br>32|


|N = 8 × log κ|Col2|Col3|
|---|---|---|
|4<br>8|4<br>8|4<br>8|
||8<br>1|6<br>32|



_Figure A.3._ Trained models accuracy on MQAR (best of 7 seeds) across _N_, _κ_, and _d_ . For Mamba and Mamba-2 _|V |_ = 512, while for
S4D _|V |_ = 256 (as it failed to reach satisfactory accuracy for larger _|V |_ ). _T_ = 100 for all runs.


34


**D.4. Task INDUCTION HEADS**


**Model** All the results in Fig. A.2 use 1-layer Mamba that explicitly disables the gating branch. This simplification is intended to verify
our constructions without gating in Lem. 3.


**Experiment Set-up** We generate the data described in App. C.2 as follows. The data generation requires four scalar parameters:
vocabulary size _|V |_, sequence length _T >_ 2 _|V |_ + 1, hard case probability _p ∈_ [0 _,_ 1], and special range ratio _γ ∈_ (0 _,_ 0 _._ 1]. We first draw
_X ∼_ Bernoulli( _p_ ): if _X_ = 0, we sample from the standard setting, otherwise the hard setting. The standard setting generates the input

sequence _**x**_ = ( _x_ 1 _, . . ., xT_ ) by randomly drawing _xi_ _i.i.d,∼_ _V_ = _{_ 1 _, . . ., |V |}_ for _i_ = 1 _, . . ., T_ . The hard setting is intended to evaluate
the long-range memorization capability (i.e., placing repeated tokens at the beginning and the end of the sequence), which consists of the
following steps.


1. Randomly pick a special token _v_ _[∗]_ _∈_ _V_


_i.i.d,_
2. Generate the input sequence by randomly drawing _**x**_ = ( _x_ 1 _, . . ., xT_ ) where _xi_ _∼_ _V \ v_ _[∗]_ for _i_ = 1 _, . . ., T_


3. Randomly draw a position from _r ∈_ Unif( _{_ 1 _, . . ., γT_ _}_ )


4. Place the special token _v_ _[∗]_ at positions _r, T −_ _r_ .


In the experiments for Fig. 3, we use _T_ = 100 _, |V | ∈{_ 5 _,_ 10 _,_ 20 _,_ 40 _}, p_ = 0 _._ 75 _, γ_ = 0 _._ 1. In Fig. A.4, we further ablate the choice of
state size _N ∈{|V |,_ 2 _|V |,_ 4 _|V |}_ .


**Discussion** We design the hard setting to better differentiate the capabilities from Mamba and our proposed Mamba-∆ _[⊤]_ . Specifically,
solving the standard setting of the INDUCTION HEADS task requires memorizing the latest previous occurrence, or forgetting the
earlier previous occurrences. Note that the input sequence generated from the standard setting consists of many repeated tokens (by the
requirement _T >_ 2 _|V |_ + 1), and the expected time for reappearance of any token is _|V |_ . Thus, for small and medium-size _|V |_, Mamba
can solve for these cases by using the state matrix with negative eigenvalues to discount the remote past pairs, and thereby correctly output
the latest previous occurrence. However, solving the hard setting additionally requires the model to memorize long-range information due
to the special token (occurring at the beginning part and the end part of the sequence). We see that the Mamba solution with negative
eigenvalues is _at odds with_ the long-range memorization, as shown in Lem. 1. On the other hand, Mamba-∆ _[⊤]_ can satisfy both selective
forgetting and long-range memorization via the input-dependence state matrix that erases outdated information specific to the input key,
while retaining other information in the hidden state, as illustrated in the proof of Lem. 3 (see details in App. C.2).


Below we expand on the results in Sec. 5.2 by reporting a sweep on the hidden state size _N_ for the models used in the INDUCTION
HEADS task experiments, complementing the findings shown in Fig. 3.





_N_ = _|V |_





_N_ = 2 _|V |_


Vocabulary size ( _|V |_ )





_N_ = 4 _|V |_



_Figure A.4._ Trained models accuracy on INDUCTION HEADS task (best of 5 seeds), varying _|V |_ and _d_, with _N ∈{|V |,_ 2 _|V |,_ 4 _|V |}_ (left,
middle, right). Mamba-∆ _[⊤]_ ’s performance (outlined) is equal or better than Mamba’s (filled) and only hits 100% above the theoretical
bound from Lem. 3 (black).


35


