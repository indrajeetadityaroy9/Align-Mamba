## **DeltaProduct: Improving State-Tracking in** **Linear RNNs via Householder Products**

**Julien Siems** _[∗♢]_ **, Timur Carstensen** _[∗♢♣]_ **, Arber Zela** _[♢]_ **,**
**Frank Hutter** _[△♣♢]_ **, Massimiliano Pontil** _[♡♠]_ **, Riccardo Grazzi** _[∗†]_ [⋆]

Equal contribution _[∗]_, University of Freiburg _[♢]_, ELLIS Institute T¨ubingen _[♣]_, Microsoft Research [⋆]

Prior Labs _[△]_, CSML, Istituto Italiano di Tecnologia _[♡]_, AI Centre, University College London _[♠]_
```
  juliensiems@gmail.com timurcarstensen@gmail.com riccardograzzi4@gmail.com

```

**Abstract**


Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive
alternatives to Transformers for sequence modeling, offering efficient training and
linear-time inference. However, existing architectures face a fundamental tradeoff between expressivity and efficiency, dictated by the structure of their statetransition matrices. Diagonal matrices, used in models such as Mamba, GLA, or
mLSTM, yield fast runtime but have limited expressivity. To address this, recent
architectures such as DeltaNet and RWKV-7 adopted a diagonal plus rank–1 structure, which allows simultaneous token and channel mixing, improving associative
recall and, as recently shown, state-tracking when allowing state-transition matrices to have negative eigenvalues. Building on the interpretation of DeltaNet’s
recurrence as performing one step of online gradient descent per token on an associative recall loss, we introduce DeltaProduct, which instead takes multiple ( _nh_ )
steps per token. This naturally leads to diagonal plus rank– _nh_ state-transition matrices, formed as products of _nh_ generalized Householder transformations, providing a tunable mechanism to balance expressivity and efficiency. We provide a
detailed theoretical characterization of the state-tracking capability of DeltaProduct in finite precision, showing how it improves by increasing _nh_ . Our extensive
experiments demonstrate that DeltaProduct outperforms DeltaNet in both statetracking and language modeling, while also showing significantly improved length
extrapolation capabilities.


**1** **Introduction**


The Transformer architecture [1] has revolutionized natural language processing through its selfattention mechanism, enabling both parallel computation across the sequence length and effective context retrieval. Consequently, Transformers have largely replaced recurrent models like
LSTMs [2], which exhibit slower training and poorer retrieval performance. However, the quadratic
computational complexity of Transformers with sequence length presents challenges when dealing with longer sequences. Linear RNNs have emerged as a promising solution that combines
parallel training across the sequence length with linear inference-time complexity. At the core of
these models are the state-transition matrices governing the recurrence, which fundamentally determine the expressivity of a linear RNN [3]. Early linear RNNs like S4 [4] and LRU [5] used
token-independent state-transition matrices. For superior expressivity, current linear RNNs now exclusively use token-dependent matrices. Among these Mamba [6, 7], GLA [8], and mLSTM [9]
use diagonal state-transition matrices for efficient sequence processing. Newer architectures have
incorporated non-diagonal structures, often diagonal plus rank-1, enabling simultaneous mixing of


_†_ Work started while at Istituto Italiano di Tecnologia.


39th Conference on Neural Information Processing Systems (NeurIPS 2025).


CodeParrot

|Col1|Col2|DeltaNet<br>DeltaProduct2<br>DeltaProduct3|
|---|---|---|
||||
||||



4096 16384


Sequence Length



Figure 1: ( _Left_ ) DeltaProduct _nh_
learns higher-order permutation
groups like _S_ 5 in one layer, while
DeltaNet ( _nh_ =1) is limited to _S_ 2
(parity). ( _Right_ ) Length extrapolation of DeltaProduct improves
significantly with higher _nh_ .





3 _._ 6


3 _._ 4


3 _._ 2


3 _._ 0


|Group S5|Col2|Col3|Col4|
|---|---|---|---|
|5<br>|5<br>|5<br>|5<br>|
||||~~n~~h<br>1<br>2|
||||3<br>4|



information across both tokens and channels of the hidden state. This innovation has led to more expressive models such as (Gated) DeltaNet [10, 11], TTT-Linear [12], RWKV-7 [13], and Titans [14],
which demonstrate superior language modeling and in-context retrieval performance, often with
only a reasonable decrease in training efficiency.


Recent work has revealed a fundamental trade-off between training efficiency and expressivity of
linear RNNs, dictated by the structure of their state-transition matrices [3, 15, 16]. Models with
diagonal state-transition matrices, such as Mamba and GLA, are highly efficient to train but face
severe expressivity limitations - for instance, they cannot perform addition modulo 3 on sequences
of arbitrary length in finite precision [16, Theorem 2]. Transformers also face similar limitations

[17, 18], since they can be seen as special linear RNNs with a state-transition matrix equal to the
identity, albeit with an infinite dimensional state [19]. DeltaNet partially overcomes these limitations
through generalized Householder matrices, achieving greater expressivity, though it still requires
multiple layers for some tasks. At the other extreme, linear RNNs with full state-transition matrices
offer maximal expressivity [20], capable of recognizing any regular language with a single layer [3],
but are prohibitively expensive to train.


To bridge this gap, we propose _DeltaProduct_, a method that balances expressivity and efficiency of
the recurrence computation. While DeltaNet’s recurrence performs a single gradient descent step
per token on the squared loss of a linear key-to-value mapping [10, 21], DeltaProduct takes _nh_
gradient steps using additional keys and values, yielding state-transition matrices that are products
of _nh_ generalized Householder matrices. This connection between the number of optimization steps
and the matrix structure provides an elegant way to interpolate between diagonal and dense matrices:
increasing the number of gradient steps automatically increases the number of Householder matrices
in the product, providing a tunable mechanism to control the recurrence’s expressivity. DeltaProduct
enables precise control over the norm of state transition matrices, ensuring it remains _≤_ 1 to maintain
[stability during training on long sequences. We contribute DeltaProduct to the flash-linear-attention](https://github.com/fla-org/flash-linear-attention/blob/main/fla/layers/gated_deltaproduct.py)
[library [22], our experiment code is provided here.](https://github.com/automl/DeltaProduct)


Concretely, we make the following contributions:


 - We propose _(Gated) DeltaProduct_, which generalizes (Gated) DeltaNet by using products of
generalized Householder transformations as state-transition matrices (Section 4).

  - We provide a detailed theoretical characterization of the expressivity of DeltaProduct in finite
precision and how it improves by increasing _nh_ (Section 4.1). Notably, we prove that for any
_nh ≥_ 1, DeltaProduct with at most 4 layers (3 if _nh ≥_ 2) can solve any group word problem,
and Gated DeltaProduct with a finite number of layers can recognize any regular language.

  - We empirically validate DeltaProduct’s superior performance across multiple domains: solving complex state-tracking tasks beyond DeltaNet’s capabilities (see Figure 1) and improving
language modeling performance with significantly enhanced length extrapolation (Section 5),
which we study through analysis of the hidden state’s effective rank.


**2** **Background**


**Linear RNNs.** Linear RNNs consist of stacked layers, each processing an input sequence of vectors
_**x**_ 1 _, . . .,_ _**x**_ _t ∈_ R _[l]_ (output of the previous layer) to produce an output sequence ˆ _**y**_ 1 _, . . .,_ ˆ _**y**_ _t ∈_ R _[p]_ . We
write the forward pass of each layer placing emphasis on the linear recurrence (as in [16])

_**H**_ _i_ = _**A**_ ( _**x**_ _i_ ) _**H**_ _i−_ 1 + _**B**_ ( _**x**_ _i_ ) _,_ _**y**_ ˆ _i_ = dec( _**H**_ _i,_ _**x**_ _i_ ) where _i ∈_ 1 _, . . ., t_ (1)

_**H**_ 0 _∈_ R _[n][×][d]_ is the initial hidden state, _**A**_ : R _[l]_ _→_ R _[n][×][n]_ maps the input to a state-transition matrix,
_**B**_ : R _[l]_ _→_ R _[n][×][d]_ controls the information added to the new hidden state, and dec : R _[n][×][d]_ _×_


2


R _[l]_ _→_ R _[p]_ determines the output of the recurrence. The functions _**A**_, _**B**_, and dec are learnable,
with dec typically containing a feedforward neural network. Different linear RNN variants are
distinguished by their specific implementations of these functions. For example, Mamba [6, 7],
GLA [8], and mLSTM [9] use variations of a diagonal state-transition matrix _**A**_ ( _**x**_ _i_ ). The linearity
of the recurrence allows it to be parallelized along the sequence length, either via a chunkwise
parallel form [8, 23, 24] or using a parallel scan [6, 25–28]. For a comparison of different linear
RNN architectures see Yang et al. [10, Table 4].


**DeltaNet.** We base our work on the DeltaNet architecture [29, 30], which has recently seen renewed
interest through the work of Yang et al. [10, 11] who demonstrate how to parallelize DeltaNet across
the sequence length on GPUs. The DeltaNet recurrence is parameterized as


_**A**_ ( _**x**_ _i_ ) = _**I**_ _−_ _βi_ _**k**_ _i_ _**k**_ _i_ _[⊤][,]_ _**[ B]**_ [(] _**[x]**_ _[i]_ [) =] _[ β][i]_ _**[k]**_ _[i]_ _**[v]**_ _i_ _[⊤][,]_ [ dec(] _**[H]**_ _[i][,]_ _**[ x]**_ _[i]_ [) =] _[ ψ]_ [(] _**[H]**_ _i_ _[⊤]_ _**[q]**_ _[i]_ [)] (2)

where _βi ∈_ [0 _,_ 1], _**q**_ _i,_ _**k**_ _i ∈_ R _[n]_ (with _∥_ _**q**_ _i∥_ = _∥_ _**k**_ _i∥_ = 1), _**v**_ _i ∈_ R _[d]_ are outputs of learnable functions of _**x**_ _i_ . _**A**_ ( _**x**_ _i_ ) is a generalized Householder transformation [31], which is symmetric and has
eigenvalues 1 (multiplicity _n −_ 1) and 1 _−_ _βi_ (multiplicity 1). From a geometric perspective, _βi_
determines the transformation type, interpolating between identity ( _βi_ = 0) and projection ( _βi_ = 1).
DeltaNet also has a natural interpretation from an online learning perspective [10], as each step of
its recurrence is one step of online gradient descent on a quadratic loss with step size _βi_ :

_Li_ ( _**H**_ ) = 1 _/_ 2 _∥_ _**H**_ _[⊤]_ _**k**_ _i −_ _**v**_ _i∥_ [2] 2 [;] _**[ H]**_ _[i]_ [=] _**[ H]**_ _[i][−]_ [1] _[−]_ _[β][i]_ _[∇L][i]_ [(] _**[H]**_ _[i][−]_ [1][) =] _**[ H]**_ _[i][−]_ [1] _[−]_ _[β][i]_ _**[k]**_ _[i]_ [(] _**[k]**_ _i_ _[⊤]_ _**[H]**_ _[i][−]_ [1] _[−]_ _**[v]**_ _i_ _[⊤]_ [)]

DeltaNet: _**H**_ _i_ = ( _I −_ _βi_ _**k**_ _i_ _**k**_ _i_ _[⊤]_ [)] _**[H]**_ _[i][−]_ [1] [+] _[ β][i]_ _**[k]**_ _[i]_ _**[v]**_ _i_ _[⊤]_


**State-Tracking and Word Problems.** State-tracking is the ability of a model to keep track of
the state of a system while only observing the updates that are applied to it. It can be modeled as a
monoid word problem, which consists in mapping sequences _x_ 1 _, . . ., xt_, with _xi_ being an element of
a monoid _G_, into sequences _y_ 1 _, . . ., yt_, where _yi_ = _xi · xi−_ 1 _· · · x_ 1 and _·_ is the associative operation
of the monoid. Recognizing a regular language can be accomplished by solving the word problem
of a finite monoid associated to the language and will be the focus of this work. Problems where _G_
is also a group (group word problems) with finite elements, are notoriously hard to solve for both
Transformers and linear RNNs. Group word problems for the symmetric or permutation groups
are particularly important, since any group is isomorphic to a subgroup of a symmetric group. For
instance, if we denote with _Sn_ the group of permutations of _n_ elements, parity corresponds to the _S_ 2
word problem, which cannot be solved in finite precision by Transformers [17] and diagonal Linear
RNNs [15] with positive values, while the _S_ 5 word problem cannot be solved by these models even
when the precision can grow logarithmically with the sequence length, since both Transformers and
Linear RNNs belong to the TC [0] circuit complexity class while _S_ 5 is in NC [1] [3, 18, 32]. In contrast,
with unconstrained, full state-transition matrices any regular language can be recognized in one layer
(see e.g. [3, Theorem 5.2]). However, training using full unstructured matrices is very inefficient and
also unstable without any control on the norm.


**3** **Related Work**


Linear RNNs have recently been studied from two main perspectives: state-space models and causal
linear attention. State-space models, originating from continuous dynamical systems, inspired variants such as S4 [4], H4 [33], and LRU [5] (see Tiezzi et al. [34] for a comprehensive survey). Models
like Mamba [6, 7] further enhance these by incorporating input-dependent gating mechanisms, significantly improving language modeling performance. In parallel, Katharopoulos et al. [19] showed
that causal linear attention Transformers can be reformulated as RNNs with linear sequence-length
scaling. Following this, Gated Linear Attention (GLA) [8] introduced gating mechanisms similar
to Mamba. Recent studies explored more expressive recurrences via non-diagonal transition matrices, such as DeltaNet [10, 29, 35], TTT-Linear [12], RWKV-7 [13], B’MOJO [36], and Titans

[14]. Additionally, Beck et al. [9] introduced xLSTM, combining linear and nonlinear RNN architectures inspired by LSTM [2]. Another line of work explores recurrences in depth, which have been
shown to increase the expressivity and reasoning capabilities [37, 38]. For instance, concurrent work
explores how fixed-point iterations of a diagonal linear RNN can increase its expressivity turning
it non-linear at the fixed-point [39, 40]. Unlike our approach, which enhances the expressivity by
increasing the complexity of the linear recurrence, their approach works by applying the same recurrence multiple times, effectively increasing the depth of the model without increasing the parameter
count. This approach is orthogonal to ours and the two can be potentially combined.


3


Products of structured matrices [41] have previously been used in the state-update of _non-linear_
RNNs – including (Givens) rotation matrices [42–44], Kronecker products [45], Householder reflections [46]—chosen for their orthogonal, norm-preserving properties that encourage long-term dependency learning [47, 48]. Recently, Biegun et al. [49] applied rotation matrices as state-transition
matrices in non-selective state-space models. In contrast, DeltaProduct uses a linear recurrence with
state-transition matrices adaptive to the current token, expressed as products of generalized Householder matrices.


**State-Tracking.** Recent work by Grazzi et al. [16] demonstrates that expanding the eigenvalue
range of linear RNNs’ state transition matrices from [0 _,_ 1] to [ _−_ 1 _,_ 1] significantly enhances their
expressivity. They show how DeltaNet’s eigenvalue range can be extended from [0 _,_ 1] to [ _−_ 1 _,_ 1],
simply by multiplying _βi_ by 2, allowing DeltaNet to perform reflections when _βi_ = 2, which enables
it to handle state-tracking tasks such as parity checking and, more generally, any _group word problem_
where each element of the input sequence corresponds to a permutation of at most two elements,
while for other tasks, DeltaNet requires multiple layers [16, Theorem 2 and 6]. Their theoretical
results also cover the products of Householders used as state-transition matrices of DeltaProduct,
showing that they allow to solve any group word problem in one layer (Theorem 3) and recognize
any regular language (Theorem 4), when _nh_ is large enough and with a finite number of layers. Here,
we extend that work by providing experimental evidence of the benefit of larger _nh_ and, leveraging
the analysis by Peng et al. [13], more refined theoretical results on the expressivity, e.g. with a
greatly improved dependency on _nh_ . The recent RWKV-7 [13] uses a state transition matrix of the
form diag( _**w**_ _t_ ) _−_ _c_ _**k**_ _t_ ( _**k**_ _t ⊙_ _**a**_ _t_ ) _[⊤]_, with _∥_ _**k**_ _t∥_ = 1 _,_ _**a**_ _t,_ _**w**_ _t ∈_ [0 _,_ 1] _[n]_ and _c ∈{_ 1 _,_ 2 _}_, which provides
a potentially asymmetric rank-1 update in contrast to DeltaNet’s symmetric update, allowing it to
recognize any regular language with only 4 layers. However, the increased expressivity comes at the
cost of losing the guarantee on the stability of the recurrence. In contrast, (Gated) DeltaProduct has
a stable recurrence since the spectral norm of every state-transition matrix is always _≤_ 1.


**4** **DeltaProduct**


While DeltaNet’s recurrence can be seen as performing one step of online gradient descent per
token, DeltaProduct builds upon DeltaNet by further refining the hidden state by taking _multiple_
_steps per token_ . This naturally leads to a more expressive state-transition matrix formed as a product
of generalized Householder matrices, where each additional step expands the range of achievable
linear transformations. Formally, for each input token _**x**_ _i_ to the layer we generate _nh_ keys as _**k**_ _i,j_
= _ψ_ ( _**W**_ _j_ _**x**_ _i_ ) _/∥ψ_ ( _**W**_ _j_ _**x**_ _i_ ) _∥_ 2, _nh_ values as _**v**_ _i,j_ = _**V**_ _j_ _**x**_ _i_, and _nh_ betas as _βi,j_ = _ϕ_ ( _**U**_ _j_ _**x**_ _i_ ) where
_**W**_ _j,_ _**V**_ _j,_ _**U**_ _j_, are learnable weight matrices specific to the _j_ -th gradient step, _ψ_ is a nonlinearity
(we pick SiLU as in DeltaNet), while _ϕ_ is either the sigmoid or 2 _×_ the sigmoid to increase the
expressivity. Then, we compute _nh_ gradient descent steps using the losses _Li,j_ ( _**H**_ ) = _∥_ _**H**_ _[⊤]_ _**k**_ _i,j −_
_**v**_ _i,j∥_ [2] 2 _[/]_ [2][, i.e., for] _[ j]_ [ = 1] _[ . . . n][h]_


_**H**_ _i,j_ = _**H**_ _i,j−_ 1 _−_ _βi,j∇Li,j_ ( _**H**_ _i,j−_ 1) =  - _**I**_ _−_ _βi,j_ _**k**_ _i,j_ _**k**_ _i,j_ _[⊤]_  - _**H**_ _i,j−_ 1 + _βi,j_ _**k**_ _i,j_ _**v**_ _i,j_ _[⊤]_ _[,]_


where _**H**_ _i,_ 0 = _**H**_ _i−_ 1 and _**H**_ _i,nh_ = _**H**_ _i_ . Unrolling, we get _**H**_ _i_ = _**A**_ ( _**x**_ _i_ ) _**H**_ _i−_ 1 + _**B**_ ( _**x**_ _i_ ) with



_nh_ _nh_
�� 



- - [�]
_**I**_ _−_ _βi,k_ _**k**_ _i,k_ _**k**_ _i,k_ _[⊤]_ _βi,j_ _**k**_ _i,j_ _**v**_ _i,j_ _[⊤]_ _[.]_ [ (3)]



_**A**_ ( _**x**_ _i_ ) =


Diagonal
_Token Mix:_
_Channel Mix:_
_Expressivity:_
_Examples:_



_nh_
�� _**I**_ _−_ _βi,j_ _**k**_ _i,j_ _**k**_ _i,j_ _[⊤]_ - _,_ _**B**_ ( _**x**_ _i_ ) =



**Diagonal:**
✓
_×_
Parity
Mamba, GLA



_j_ =1














_j_ =1



_k_ = _j_ +1





















































_×_



+









_×_




- - 





+



**Rank 1 Update:**
✓

✓

Reflections
DeltaNet, TTT, RWKV-7



**Rank** _nh_ **Update:**
✓

✓

Reflections, Rotations
**DeltaProduct**



Figure 2: Overview of state-transition matrices _**A**_ ( _**x**_ _i_ ) in linear RNNs.


4


Hence, by taking multiple gradient descent steps per token, DeltaProduct’s state-transition matrices
are _products of generalized Householder transformations_, and by expanding such a product, _**A**_ ( _**x**_ _i_ )
takes the form of identity plus a matrix of rank at most _nh_ as shown in Figure 2. As DeltaNet extends
to Gated DeltaNet by incorporating a forget gate [11], DeltaProduct can similarly be extended to
_Gated DeltaProduct_ by letting _**A**_ ( _**x**_ _i_ ) = _gi_ - _nj_ =1 _h_ [(] _**[I]**_ _[ −]_ _[β][i,j]_ _**[ k]**_ _[i,j]_ _**[k]**_ _i,j_ _[⊤]_ [)][ where the scalar gate] _[ g][i][ ∈]_ [[0] _[,]_ [ 1]]

is adopted from Mamba 2 [7] and _**B**_ ( _**x**_ _i_ ) remains unchanged. This formulation enables DeltaProduct
to interpolate between generalized Householder ( _nh_ = 1 as in DeltaNet) and dense matrices (of
norm _≤_ 1), since increasing _nh_ can increase the rank of _**A**_ ( _**x**_ _i_ ).



**Expressivity of Householder products.** While any state transition matrix of DeltaNet can model a single Householder reflection (with _βi_ = 2),
DeltaProduct’s can model any orthogonal matrix. This is a consequence
of the Cartan-Dieudonn´e theorem, which establishes that any _n × n_ orthogonal matrix can be expressed as a product of at most _n_ reflections (as
illustrated in Figure 3 for _n_ = 2). The Householder product exhibits interesting properties in special cases. When all Householder keys are identical,
the product simplifies to a single Householder with a scaled beta parameter,
offering no additional expressivity (Prop. 1.1). Conversely, when the keys
are mutually orthogonal, the Householder product simplifies to an identity
plus a symmetric rank _nh_ matrix (Prop. 1.2). Only when the keys are nontrivially linearly dependent can we obtain non-symmetric matrices, potentially yielding complex eigenvalues (Prop. 1.3). An important consequence
of using Householder products is that it allows us to effectively bound the
norm of _**A**_ ( _**x**_ _i_ ). This is because the norm of the product is upper bounded
by the product of the norms (each _≤_ 1), which ensures the stability of the
recurrence [16, Prop. 1.1]. This bound would not be possible with the more
direct parametrization _**A**_ ( _**x**_ _i_ ) = _I −_ [�] _[n]_ _j_ =1 _[h]_ _[β][i,j]_ _**[k]**_ _[i,j]_ _**[k]**_ _i,j_ _[⊤]_ [, which also restricts]
the matrix to be symmetric.







_H_ 1







_**k**_ 1


_x_ _[′′]_





Figure 3: Two reflections produce a 2D
rotation: Reflecting _x_
across planes _H_ 0 and
_H_ 1 (with normals _**k**_ 0
and _**k**_ 1) yields a rotation by 2 _θ_, where _θ_ is
the angle between the
planes.



**4.1** **State-Tracking Capabilities of (Gated) DeltaProduct**

We present two theorems that characterize the state-tracking capabilities of DeltaProduct. Compared
to Grazzi et al. [16, Theorem 3 and 4], we focus on results that hold for any _nh ≥_ 1. We defer proofs,
and more details to Section B, where we also include results on dihedral groups (Theorem 7) and on
finite subgroups of the orthogonal and special orthogonal groups (Theorem 4).
**Theorem 1.** _For any n ∈_ N _there exists a DeltaProduct model with one of the following configu-_
_rations that can solve the_ _**word problem**_ _of the symmetric group Sn: (i) one layer with nh_ = _n−_ 1

_[16, Theorem 3] (ii) 3 layers with nh>_ 1 _(iii) 4 layers with nh_ = 1 _. The construction for (ii) and_
_(iii) requires that the MLP at the second last layer computes a lookup-table of size_ 2 _m ×_ ( _n_ !) [2] _[m]_ _,_
_function of the last_ 2 _m input tokens and the position modulo_ 2 _m with m_ = _⌈_ ( _n−_ 1) _/nh⌉._
**Theorem 2.** _For any nh ≥_ 1 _and any regular language, there exists a Gated DeltaProduct model_
_with a finite number of layers (dependent on the language) that recognizes it._

The proof for Theorem 1 uses the same idea as the construction for the theoretical results of Peng
et al. [13] for RWKV-7. Each element of _Sn_ can be mapped to a permutation matrix, but DeltaProduct’s state transition matrices can only model permutations of up to _nh_ + 1 elements. Therefore, if
_nh_ + 1 _< n_, early layers decompose each product of _m_ = _⌈_ ( _n −_ 1) _/nh⌉_ consecutive permutations
into _m_ simpler permutations, which are applied in the recurrence of the last layer but in a delayed
fashion. To get such a decomposition and account for the delay, the MLP at the second-last layer
computes a potentially large lookup table, function of the past 2 _m_ tokens and the position modulo
2 _m_ . To prove Theorem 2, we use the Krohn-Rhodes decomposition [50], similarly to Grazzi et al.

[16, Theorem 4], where each automaton is decomposed into multiple _permutation-reset_ automata,
and model each using the same technique of Theorem 1, exploiting gates for the resets.


**Comparison to other non-diagonal Linear RNNs.** Table 1 provides a comparison of the expressivity of different non-diagonal linear RNNs. (Gated) DeltaProduct with _nh >_ 1 has improved
expressivity compared to DeltaNet, and, up to 3 layers, even compared to RWKV-7. Moreover,
increasing _nh_ has clear benefits: reducing the number of layers or the size of the lookup table.
Since DeltaProduct can solve the _S_ 5 word problem, it is outside of the TC [0] complexity class, just
as RWKV-7. One might expect DeltaProduct to be able to model any useful state-transition matrix since it can model updates of arbitrarily high rank when _nh_ is equal to the number of rows of


5


Table 1: Expressivity of non-diagonal Linear RNNs shown through the formal language problems
they can solve in finite precision. _Sn_, Z _n_, _Dn_ are the symmetric, cyclic and dihedral groups of order
_n_, while O( _n_ ) _,_ SO( _n_ ) are the orthogonal and special orthogonal group of order _n_ . For _Sn_, “only
_k_ -permutations” means that input sequences can contain only permutations of up to _k_ elements. LS
is the size of the lookup table computed in the second-last layer’s MLP. _|_ Σ _|_ and _|Q|_ are the sizes
of the alphabet and set of states of a finite state automaton recognizing the regular language. Gated
variants’ state updates can also model constant ( _reset_ ) transitions.


Layers **(Gated) DeltaNet** **RWKV-7** **(Gated) DeltaProduct** _nh>_ 1

1 _Sn_ only 2-permutations. [a] _Sn_ only 2-permutations _Sn_ only ( _nh_ + 1)-permut. [a]

Finite subgroups of O( _nh_ ),
SO( _nh_ + 1) if _nh_ is even. [g]



c Z _n_, _Dn_



2 Z _n_



b, _Dn_



3 _Sn_ with LS 2 _m ×_ ( _n_ !) [2] _[m]_

where _m_ = _⌈_ ( _n −_ 1) _/nh⌉_ . [d]


4 _Sn_ with LS _Sn_ with LS as DeltaNet. Reg.
2( _n −_ 1) _×_ ( _n_ !) [2(] _[n][−]_ [1)] . [d] lang. with LS 2 _|Q|×_ ( _|_ Σ _|_ ) [2] _[|][Q][|]_ . [e]


_f_ ( _|Q|_ ) Gated: Regular languages [f] Gated: Regular languages [f]


a [16, Thm. 3] b [16, Thm. 6] c Thm. 7. d Thm. 1. e [13, Thm. 3] f Thm. 2 g Thm. 4


the hidden state. This is because DeltaProduct’s state-transition matrices _**A**_ ( _**x**_ _i_ ) satisfy the spectral
norm condition _∥_ _**A**_ ( _**x**_ _i_ ) _∥_ = max _∥_ _**y**_ _∥_ =1 _∥_ _**A**_ ( _**x**_ _i_ ) _**y**_ _∥_ 2 _≤_ 1, ensuring a stable recurrence. RWKV-7
relaxes this constraint and can represent matrices with higher spectral norms. In particular, copy
_√_
matrices – identity matrices where one column is replaced by another – with _∥_ _**A**_ ( _**x**_ _i_ ) _∥_ = 2 (when

_c_ = 2), allowing RWKV-7 to recognize any regular language in just four layers. However, this may
lead to instability in the recurrence (see Section B.6). We could enhance expressivity at the cost
of stability by replacing the Householder matrices in DeltaProduct with RWKV-7’s state transition
matrices. This modification enables us to prove a result analogous to Theorem 1, but for regular
languages rather than group word problems—see Section B.4 for details. Specifically, this approach
allows the resulting linear RNN to recognize any regular language within a _single layer_, provided
_nh_ is sufficiently large.

**Remark 1.** _For any_ _**regular language**_ _recognized by a finite-state automaton (FSA) having n states_
_there exists a_ _**one layer linear RNN**_ _using nh_ = _n products of RWKV-7 matrices as state-transition_
_matrices that can recognize it. This is because a linear RNN with unconstrained state-transition_
_matrices can recognize any regular language in a single layer [3, Theorem 5.2] by modeling FSA_
_evaluation through matrix-vector products [51]. Peng et al. [13, Lemma 3] further showed that any_
_transition matrix of an FSA with n states can be expressed as products of n matrices, each of which_
_is either a swap, copy, or the identity matrix, all of which are representable by an RWKV-7 matrix._


The above discussion oulines a trade-off between the expressivity of RWKV-7 matrices and the guaranteed stability of generalized Householders used in DeltaProduct. It is an open question whether
there exists a continuous parameterization of state-transition matrices which yields stable recurrences and still allows to recognize any regular language in a finite and fixed number of layers.


**5** **Experiments**


We evaluate DeltaProduct on state-tracking and standard language modeling to assess its expressivity and efficiency. Throughout the experiments we use either the suffix [ _−_ 1 _,_ 1] or [0 _,_ 1] after
each method, to denote the eigenvalue ranges of its state transition matrices. We present additional
experiments on languages of different levels of the Chomsky hierarchy [52] in Section C.2.


**5.1** **Implementation**

We use the same macro architecture used by Gated DeltaNet. Since each step of (Gated) DeltaProduct follows the same recurrence structure as (Gated) DeltaNet, we can reuse its implementation
written in Triton [53], available through the FLASH-LINEAR-ATTENTION library [22], which uses
the chunk-wise parallel form for the recurrence.


6


|Col1|Col2|n|
|---|---|---|
||||
||||
||||













































Figure 5: Accuracy on state-tracking tasks for permutation groups _S_ 3, _S_ 4, _A_ 5, and _S_ 5, plotted against sequence length (x-axis). _(Top row)_ Varying the number of Householder products
_nh_ for a single layer DeltaProduct _nh_ [ _−_ 1 _,_ 1]. _(Bottom row)_ Varying the number of layers _l_ of
DeltaProduct1[ _−_ 1 _,_ 1]/DeltaNet[ _−_ 1 _,_ 1] (single Householder). Dashed vertical line at training context length 128. Higher _nh_ improves extrapolation to longer sequences of permutations, e.g., _S_ 3 can
be learned with _nh_ = 2 with a single layer while three layers are required when keeping _nh_ = 1.



However, DeltaProduct differs by using _nh_ keys, values and betas per token, resulting in a recurrence _nh_ times longer than
DeltaNet’s. Therefore, we arrange keys (and similarly values and betas) as [ **k** 1 _,_ 1 _, . . .,_ **k** 1 _,nh,_ **k** 2 _,_ 1 _, . . .,_ **k** 2 _,nh, . . ._ ] _,_ while
for gating we construct the expanded sequence of gates as:

[ _g_ 1 _,_ 1 _, . . .,_ 1 _, g_ 2 _,_ 1 _, . . .,_ 1 _, . . ._ ] where each gate _gi_ is followed by
( _nh_ _−_ 1) ones to match the number of keys and values, so that we
use only one gate for each token. Once the recurrence is evaluated, we keep only every _nh_ -th element of the output, so that the
output sequence retains the same length as the input sequence.





**Throughput.** The training (and prefill time) required for the
recurrence increases linearly with _nh_, since we use the same
chunk size for the chunkwise parallel form. In contrast, since
we keep the embedding dimension fixed, the cost for the MLP

Figure 4: Training throughput

following the recurrence does not vary with _nh_ . To remedy the

of parameter matched 1.3B

parameter-overhead introduced by the additional key and value
projections due to increased _nh_, we demonstrate the throughput DeltaProduct _nh_ on a H100.

Matched via: _(Top)_ scaling

when matching parameters in Figure 4. Matching parameters

the number of heads, _(Bottom)_

simply by scaling the head dimension is unfavorable (bottom

scaling the head dimension.

subplot, _nh_ = 2) since head dimensions that are not a power
of 2 will get padded to the next power thereof, effectively giving up the remaining dimensions at
no reduction in runtime. See Section C.3.2 for additional results on smaller models. Note that if
_nh ≥_ 1, we could parallelize the recurrence to have a faster runtime also during autoregressive
generation. Note that the throughput results are obtained using an optimized Triton kernel implementation (developed by Songlin Yang and Yu Zhang, available in the flash-linear-attention library)
that achieves a 20% faster forward pass than DeltaNet’s kernel.







Figure 4: Training throughput
of parameter matched 1.3B
DeltaProduct _nh_ on a H100.
Matched via: _(Top)_ scaling
the number of heads, _(Bottom)_
scaling the head dimension.



**5.2** **State-Tracking**


**Setup.** We evaluate DeltaProduct’s ability to capture complex state dynamics using group word
problems of increasing difficulty, specifically on the permutation groups _S_ 3, _S_ 4, _A_ 5, and _S_ 5, as
implemented by Merrill et al. [3]. These tasks consist in tracking how a sequence of permutations
rearranges elements. An intuitive parallel is the shell game, where one needs to track the position of
a hidden object after each shuffle. We train on sequences of 128 permutations and measure extrapolation up to 512. Throughout, we use the extended eigenvalue range, allowing eigenvalues in [ _−_ 1 _,_ 1].
We find that DeltaProduct models fail to learn even the training context-length when restricted to


7


the standard eigenvalue range [0 _,_ 1], regardless of the number of Householder transformations _nh_ as
shown in Figure 12. See Section C.1 for details on the experiments.


**Single layer, varying** _**nh**_ **.** Figure 5 (top row) demonstrates the benefits of increasing the number of
Householders _nh_ per token for a single layer DeltaProduct. Grazzi et al. [16, Theorem 3] presents
a construction for permutations of _n_ elements requiring _n −_ 1 Householders and keys of size _n_ .
In agreement, we find that for _S_ 3 achieving reliable performance beyond sequence lengths of 128
requires _nh_ = 2, while _S_ 5 needs _nh_ = 4. Unexpectedly, _S_ 4 and _A_ 5 can extrapolate robustly using
only _nh_ = 2 despite the theorem suggesting 3 and 4, respectively. This efficiency arises from their
isomorphism to subgroups of SO(3 _,_ R), i.e. the group of 3D rotations, [54, Ch. 1, Sec. 2.4] which
only require _nh_ = 2 and keys of size 3 (see Theorem 4). Specifically, _S_ 4 is isomorphic to the
rotation group of the cube (illustrated in Figure 6) and _A_ 5 to the rotation group of the dodecahedron.
See Section C.1 for details on the isomorphisms.


of the cube, we verified two hypotheses: whether
both Householders act as reflections ( _βi,_ 0 = _βi,_ 1 =

whether the keys are in a three-dimensional subspace.

first and second Householder in the product) across Figure 6: Rotating a cube permutes its diall 24 permutations of _S_ 4, we find that a single head agonals according to the _S_ 4 group. This exhas indeed learned to use both Householder transfor- ample shows how a 90 _[◦]_ rotation of the cube
mations as reflections where _βi,_ 0 = _βi,_ 1 = 2, ef- leads to the 4-cycle (1 _→_ 2 _→_ 3 _→_ 4 _→_ 1).
fectively creating rotation matrices as shown in Section C.1. This pattern is evident in Figure 7 (left), where this head predicts both _βi,_ 0 and _βi,_ 1
approximately at 2, confirming that the model successfully learns to approximate rotations by combining two reflections. Note that the eigenvalues of the Householder product become complex in
this case allowing it to perform rotations (Prop 1.3). To further verify whether the keys are in a
three-dimensional subspace, we apply Principal Component Analysis [55] to the key vectors of this
head. The results in Figure 7 (right) demonstrate that three principal components account for over
95% of the variance in the key space. This finding strongly supports our theoretical understanding,
as it indicates that the model primarily operates in a three-dimensional subspace, which aligns with
the structure of SO(3 _,_ R).

























Figure 6: Rotating a cube permutes its diagonals according to the _S_ 4 group. This example shows how a 90 _[◦]_ rotation of the cube
leads to the 4-cycle (1 _→_ 2 _→_ 3 _→_ 4 _→_ 1).



**Multiple layers,** _nh_ = 1 **.** The bottom row of
Figure 5 explores the expressivity of multi-layered
DeltaNet[ _−_ 1 _,_ 1] (i.e., _nh_ = 1). While increasing
layers with _nh_ = 1 improves performance, it is
less effective than increasing _nh_ and degrades lengthextrapolation performance. Specifically, to fit the
training context length, _S_ 3 required 3 layers, _S_ 4
needed 6 layers, and _A_ 5 required 3. For _S_ 5, even 10
layers proved insufficient. This suggests that simply
adding depth is less effective in practice than increasing _nh_, despite theoretical constructions showing that
_S_ 3 can be solved with just 2 layers (Theorem 7) and
any group word problem can be solved with 4 layers
(with a very wide MLP).


**5.3** **Language Modeling**










|1.00|Col2|Col3|Col4|
|---|---|---|---|
|0.50<br>0.75<br>||||
|0.50<br>0.75<br>||||
|0.50<br>0.75<br>||||


|2.0|Col2|
|---|---|
|1.8<br>2.0<br>0<br>1.8<br>2.0<br>1||
|1.8<br>2.0<br>0<br>1.8<br>2.0<br>1||
|1.8<br>2.0<br>0<br>1.8<br>2.0<br>1||
|1.8<br>2.0<br>0<br>1.8<br>2.0<br>1|10<br>20<br>Permutation Index|



Figure 7: _(Left)_ Estimated _β_ values for
DeltaProduct2[ _−_ 1 _,_ 1] on all permutations
of _S_ 4, clustering near 2 (reflection). _(Right)_
PCA of key vectors shows that the first
three components explain most of the variance.



**Setup.** We trained two model variants: DeltaProduct _nh_ [ _−_ 1 _,_ 1] and Gated DeltaProduct _nh_ [ _−_ 1 _,_ 1]
using the FineWeb dataset [57]. We provide details about the training pipeline and hyperparameters in Section C.3.1. To assess length extrapolation, we measured the cross-entropy loss beyond
the training context length of 4096 tokens on CodeParrot [58] for coding, OpenThoughts-114kMath [59] for math, and TriviaQA [60] for knowledge retrieval. We evaluated the models using
language understanding, reasoning, and retrieval benchmarks from lm-eval-harness [61], with task
specifics in Section C.3.3. Throughout our experiments we find that the training process remained
stable even as we increased _nh_ (see Section C.3.4).


8


3 _._ 1


3 _._ 0


2 _._ 9











0 5000 10000 15000
Sequence Length



0 5000 10000 15000
Sequence Length



0 5000 10000 15000
Sequence Length



DeltaNet DeltaProduct2 DeltaProduct3
Figure 8: Length extrapolation results. Solid and dashed lines represent models with 8 and 12 heads
respectively. Note that DeltaProduct2[ _−_ 1 _,_ 1] with 8 heads (392M parameters) matches the parameter
count of DeltaNet ( _nh_ = 1) with 12 heads (dotted line), while achieving significantly better length
extrapolation. For each index of the sequence, we report the moving average over 501 tokens as
suggested by Lin et al. [56].

















Figure 9: Effective rank of _**H**_ _i_ for 4 of 8 heads in layer 20/24 on trivia-qa sequences. Solid vertical
lines mark new question-answer pairs; dashed vertical line indicates 4096-token training context
length; colored lines show effective rank per head over the sequence.


**Length extrapolation results.** Remarkably, as shown in Figure 8, DeltaProduct’s length extrapolation performance increases sharply when going from one to two Householders, and at _nh_ = 3, the
performance degradation is minimal across the sequence length. We hypothesize that DeltaProduct
achieves better length extrapolation by enhancing DeltaNet’s forgetting mechanism. While DeltaNet
requires _n_ rank-1 updates to reset its state to zero, DeltaProduct can accelerate this forgetting process
by a factor of _nh_ . However, our experiments show that DeltaProduct2[ _−_ 1 _,_ 1] still performs better
with a forget gate, as demonstrated by its improved results when compared to the non-gated version
(see Section C.3.5).



**Analyzing state dynamics through effective rank.** To test our hypothesis towards a better forgetting mechanism, we compare how the information density of the hidden state _**H**_ _i_ changes over time for (Gated)
DeltaProduct3[ _−_ 1 _,_ 1] and (Gated) DeltaNet[ _−_ 1 _,_ 1]. We propose to measure the information density of _**H**_ _i_ using its effective rank [62] defined as
erank( _**H**_ _i_ ) = exp( _−_ [�] _[p][k]_ [ log] _[ p][k]_ [)][, where] _[ p][k]_ [ =] _[ σ][k][/]_ [ �] _[|][σ][i][|]_ [ and] _[ σ][i]_ [ is]







_k_ _[p][k]_ [ log] _[ p][k]_ [)][, where] _[ p][k]_ [ =] _[ σ][k][/]_ [ �]





erank( _**H**_ _i_ ) = exp( _−_ [�] _k_ _[p][k]_ [ log] _[ p][k]_ [)][, where] _[ p][k]_ [ =] _[ σ][k][/]_ [ �] _i_ _[|][σ][i][|]_ [ and] _[ σ][i]_ [ is]

the _i_ -th singular value of _**H**_ _i_, which satisfies 1 _≤_ erank( _**H**_ _i_ ) _≤_ rank( _**H**_ _i_ ).
Note that Parnichkun et al. [63] also used the effective rank in the context of
linear RNNs, but measured on a different quantity: a linear operator associated to the recurrence. Similarly, Peng et al. [13] conducted an interpretability study on the hidden state of RWKV-7 using the average stable rank [64]
across layers. In Figure 9, we present the effective rank of DeltaProduct3, Figure 10: Scaling
Gated DeltaProduct3, Gated DeltaNet, and DeltaNet across a sequence of analysis w.r.t. ( _top_ )
tokens from the TriviaQA dataset, which consists of question-answer pairs final perplexity on
that test common knowledge. We observe that some heads of DeltaProduct FineWeb, ( _bottom_ )
learn to update their state with new information at the beginning of sequence Lambada and lm-eval
(BOS) tokens and then decay over the rest of the sequence. In comparison, tasks.
a few heads of Gated DeltaNet and Gated DeltaProduct learn to reduce the
effective rank of the hidden state close to zero after each BOS token, while the other heads maintain
a very low effective rank throughout the sequence. In contrast, DeltaNet’s effective rank increases
substantially beyond the training context. We attribute DeltaNet’s inability to extrapolate to longer
sequences to this issue, as the training and extrapolation regimes differ, resulting in a distribution
shift. We present additional results for other layers and the CodeParrot dataset in Section C.3.5.



















Figure 10: Scaling
analysis w.r.t. ( _top_ )
final perplexity on
FineWeb, ( _bottom_ )
Lambada and lm-eval
tasks.



9


**Scaling Analysis.** We test whether it is favorable to increase model size through Householder products as opposed to increasing model capacity through e.g., the head dimension. We adjust either the
head dimension in DeltaNet, or the number of Householder products ( _nh_ ) in DeltaProduct to reach
a given size. Figure 10 (top) shows that DeltaProduct scales better in terms of training perplexity.
The results on lm-eval-harness tasks, in Figure 10 (bottom), reinforce our findings as DeltaProduct
maintains its performance advantage at the largest scale. In total, we test two methods to reach
parameter equivalence at each model scale: scaling the number of heads or the head dimension.
We find that the latter shows more consistent scaling. Results for scaling the number of heads can
be found in Section C.3.6. In Section C.3.3 we report additional results, including gated variants,
where we find that both DeltaProduct and Gated DeltaProduct on average outperform their baseline
counterparts (DeltaNet[ _−_ 1 _,_ 1] and Gated DeltaNet[ _−_ 1 _,_ 1]) across the considered language modeling benchmarks from lm-eval harness when we increase _nh_ . Interestingly, DeltaProduct3[ _−_ 1 _,_ 1]
achieves comparable performance to Gated DeltaNet[ _−_ 1 _,_ 1], despite lacking a forget gate.


**6** **Conclusion and Future Work**


We present DeltaProduct, an extension of DeltaNet that uses products of Householder transformations as state-transition matrices. Our approach bridges the gap between structured and dense
matrices, with each recurrence step interpretable as multiple steps of gradient descent on an associative recall loss (compared to DeltaNet’s single step). The number of Householder matrices
( _nh_ ) serves as a tunable parameter balancing expressivity and computational efficiency. Our experiments demonstrate DeltaProduct’s superior performance over DeltaNet in state tracking, formal
language recognition, and language modeling, with particularly strong length extrapolation results.
DeltaProduct represents a promising step towards developing sequence models that are more capable while still remaining scalable. **Limitation.** The main limitation of DeltaProduct is its increased
computational cost, which scales linearly with _nh_ during training. **Future Work.** Future research
could explore more expressive and possibly stable matrix parameterizations or an adaptive version
of DeltaProduct determining the number of Householders per token similar to Graves [65] in order to reduce computation. The additional parameters introduced with higher _nh_ could be reduced
through LoRA MLPs as done in RWKV-7 [13]. In addition, one could combine DeltaProduct with
fixed point RNNs [39, 40]. Our DeltaProduct implementation could be further optimized through
custom kernels as suggested in the recent works by Cirone and Salvi [66] or Beck et al. [67]. We also
identify promising applications for DeltaProduct in reasoning tasks, where the higher token counts
align well with the strength of linear RNNs. Given that state-tracking benefits reasoning tasks [39],
future work should examine how increasing _nh_ affects reasoning.

**Acknowledgements**


We would like to thank Songlin Yang, Eddie Bergman, Arjun Krishnakumar, Alma Lindborg, and
Julie Naegelen for constructive discussions and feedback. We acknowledge the support and assistance of the Data Science and Computation Facility and its Support Team, in particular Mattia Pini, in using the IIT High-Performance Computing Infrastructure, on which we run part of
our experiments. This research was partially supported by the following sources: PNRR MUR
Project PE000013 CUP J53C22003010006 “Future Artificial Intelligence Research (FAIR)“, funded
by the European Union – NextGenerationEU, and EU Project ELSA under grant agreement No.
101070617. TAILOR, a project funded by EU Horizon 2020 research and innovation programme
under GA No 952215; the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
under grant number 417962828; the European Research Council (ERC) Consolidator Grant ’Deep
Learning 2.0’ (grant no. 10). This research was partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 539134284, through EFRE (FEIH
2698644) and the state of Baden-W¨urttemberg. Frank Hutter acknowledges financial support by
the Hector Foundation. The authors acknowledge support from ELLIS and ELIZA. Funded by the
European Union. The authors gratefully acknowledge the computing time made available to them
on the high-performance computers and at the NHR Centers at TU Dresden and KIT. These centers
are jointly supported by the Federal Ministry of Research, Technology and Space of Germany and
the state governments participating in the NHR. Views and opinions expressed are however those of
the author(s) only and do not necessarily reflect those of the European Union or the ERC. Neither
the European Union nor the ERC can be held responsible for them.


10


**References**


[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. von Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Proceedings of the 31st International_
_Conference on Advances in Neural Information Processing Systems (NeurIPS’17)_ . Curran Associates, Inc., 2017.


[2] Sepp Hochreiter and J¨urgen Schmidhuber. Long Short-Term Memory. _Neural Computation_, 9
(8):1735–1780, 1997.


[3] W. Merrill, J. Petty, and A. Sabharwal. The Illusion of State in State-Space Models. In
R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller, N. Oliver, J. Scarlett, and F. Berkenkamp,
editors, _Proceedings of the 41st International Conference on Machine Learning (ICML’24)_,
volume 251 of _Proceedings of Machine Learning Research_ . PMLR, 2024.


[4] A. Gu, K. Goel, and C. Re. Efficiently Modeling Long Sequences with Structured State Spaces.
In _The Tenth International Conference on Learning Representations (ICLR’22)_ . ICLR, 2022.


[5] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resurrecting
recurrent neural networks for long sequences. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, _Proceedings of the 40th International Conference on_
_Machine Learning (ICML’23)_, volume 202 of _Proceedings of Machine Learning Research_ .
PMLR, 2023.


[6] Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces.
In _First Conference on Language Modeling_, 2024.


[7] T. Dao and A. Gu. Transformers are SSMs: Generalized models and efficient algorithms
through structured state space duality. In R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller,
N. Oliver, J. Scarlett, and F. Berkenkamp, editors, _Proceedings of the 41st International Con-_
_ference on Machine Learning (ICML’24)_, volume 251 of _Proceedings of Machine Learning_
_Research_ . PMLR, 2024.


[8] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. Gated Linear Attention Transformers with
Hardware-Efficient Training. In R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller, N. Oliver,
J. Scarlett, and F. Berkenkamp, editors, _Proceedings of the 41st International Conference on_
_Machine Learning (ICML’24)_, volume 251 of _Proceedings of Machine Learning Research_ .
PMLR, 2024.


[9] M. Beck, K. P¨oppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xLSTM: Extended Long Short-Term Memory. In A. Globerson,
L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, _Proceedings_
_of the 37th International Conference on Advances in Neural Information Processing Systems_
_(NeurIPS’24)_, 2024.


[10] S. Yang, B. Wang, Y. Zhang, Y. Shen, and Y. Kim. Parallelizing Linear Transformers with the
Delta Rule over Sequence Length. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, _Proceedings of the 37th International Conference on_
_Advances in Neural Information Processing Systems (NeurIPS’24)_, 2024.


[11] S. Yang, J. Kautz, and A. Hatamizadeh. Gated delta networks: Improving mamba2 with delta
rule. In _The Thirteenth International Conference on Learning Representations (ICLR’25)_ .
ICLR, 2025.


[12] Y. Sun, X. Li, K. Dalal, J. Xu, A. Vikram, G. Zhang, Y. Dubois, X. Chen, X. Wang, S. Koyejo,
T. Hashimoto, and C. Guestrin. Learning to (learn at test time): RNNs with expressive hidden
states. _arXiv:2407.04620 [cs.LG]_, 2024.


[13] Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Haowen Hou, Janna Lu, William
Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, Nathan Wilce, Johan S. Wind, Tianyi
Wu, Daniel Wuttke, and Christian Zhou-Zheng. RWKV-7 ”Goose” with Expressive Dynamic
State Evolution, 2025.


11


[14] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time.
_arXiv preprint arXiv:2501.00663_, 2024.


[15] Y. Sarrof, Y. Veitsman, and M. Hahn. The Expressive Capacity of State Space Models: A
Formal Language Perspective. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet,
J. Tomczak, and C. Zhang, editors, _Proceedings of the 37th International Conference on Ad-_
_vances in Neural Information Processing Systems (NeurIPS’24)_, 2024.


[16] R. Grazzi, J. Siems, A. Zela, J. Franke, F. Hutter, and M. Pontil. Unlocking State-Tracking in
Linear RNNs Through Negative Eigenvalues. In _The Thirteenth International Conference on_
_Learning Representations (ICLR’25)_ . ICLR, 2025.


[17] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. _Transac-_
_tions of the Association for Computational Linguistics_, 8:156–171, 2020.


[18] William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision
transformers. _Transactions of the Association for Computational Linguistics_, 11:531–545,
2023.


[19] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In H. Daume III and A. Singh, editors, _Pro-_
_ceedings of the 37th International Conference on Machine Learning (ICML’20)_, volume 98.
Proceedings of Machine Learning Research, 2020.


[20] N. M. Cirone, A. Orvieto, B. Walker, C. Salvi, and T. Lyons. Theoretical Foundations of
Deep Selective State-Space Models. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, _Proceedings of the 37th International Conference on_
_Advances in Neural Information Processing Systems (NeurIPS’24)_, 2024.


[21] Ke Alexander Wang, Jiaxin Shi, and Emily B Fox. Test-time regression: A unifying framework
for designing sequence models with associative memory. _arXiv preprint arXiv:2501.12352_,
2025.


[22] Songlin Yang and Yu Zhang. FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism, January 2024. URL `[https://github.com/](https://github.com/sustcsonglin/flash-linear-attention)`
`[sustcsonglin/flash-linear-attention](https://github.com/sustcsonglin/flash-linear-attention)` .


[23] W. Hua, Z. Dai, H. Liu, and Q. Le. Transformer quality in linear time. In K. Chaudhuri,
S. Jegelka, L. Song, C. Szepesv´ari, G. Niu, and S. Sabato, editors, _Proceedings of the 39th_
_International Conference on Machine Learning (ICML’22)_, volume 162 of _Proceedings of_
_Machine Learning Research_ . PMLR, 2022.


[24] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang,
and Furu Wei. Retentive network: A successor to transformer for large language models. _arXiv_
_preprint arXiv:2307.08621_, 2023.


[25] Guy E. Blelloch. Prefix sums and their applications. Technical Report CMU-CS-90-190,
School of Computer Science, Carnegie Mellon University, 1990.


[26] E. Martin and C. Cundy. Parallelizing Linear Recurrent Neural Nets Over Sequence Length.
In _The Sixth International Conference on Learning Representations (ICLR’18)_ . ICLR, 2018.


[27] J. Smith, A. Warrington, and S. Linderman. Simplified State Space Layers for Sequence
Modeling. In _The Eleventh International Conference on Learning Representations (ICLR’23)_ .
ICLR, 2023.


[28] Ting-Han Fan, Ta-Chung Chi, and Alexander Rudnicky. Advancing Regular Language Reasoning in Linear Recurrent Neural Networks. In _Proceedings of the 2024 Conference of the_
_North American Chapter of the Association for Computational Linguistics: Human Language_
_Technologies (Volume 2: Short Papers)_, pages 45–53, 2024.


[29] I. Schlag, K. Irie, and J. Schmidhuber. Linear transformers are secretly fast weight programmers. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference_
_on Machine Learning (ICML’21)_, volume 139 of _Proceedings of Machine Learning Research_ .
PMLR, 2021.


12


[30] I. Schlag, T. Munkhdalai, and J. Schmidhuber. Learning Associative Inference Using
Fast Weight Memory. In _The Ninth International Conference on Learning Representations_
_(ICLR’21)_ . ICLR, 2021.


[31] Alston S Householder. Unitary triangularization of a nonsymmetric matrix. _Journal of the_
_ACM (JACM)_, 5(4):339–342, 1958.


[32] B. Liu, J. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Transformers Learn Shortcuts to
Automata. In _The Eleventh International Conference on Learning Representations (ICLR’23)_ .
ICLR, 2023.


[33] D. Fu, T. Dao, K. Saab, A. Thomas, A. Rudra, and C. Re. Hungry Hungry Hippos: Towards
Language Modeling with State Space Models. In _The Eleventh International Conference on_
_Learning Representations (ICLR’23)_ . ICLR, 2023.


[34] Matteo Tiezzi, Michele Casoni, Alessandro Betti, Tommaso Guidi, Marco Gori, and Stefano
Melacci. Back to recurrent processing at the crossroad of transformers and state-space models.
_Nature Machine Intelligence_, may 2025. ISSN 2522-5839. doi: 10.1038/s42256-025-01034-6.


[35] Kazuki Irie, R´obert Csord´as, and J¨urgen Schmidhuber. Practical computational power of linear
transformers and their recurrent and self-referential extensions. In _The 2023 Conference on_
_Empirical Methods in Natural Language Processing_, 2023.


[36] L. Zancato, A. Seshadri, Y. Dukler, A. Golatkar, Y. Shen, B. Bowman, M. Trager, A. Achille,
and S. Soatto. B’MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic
and Fading Memory. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, _Proceedings of the 37th International Conference on Advances in_
_Neural Information Processing Systems (NeurIPS’24)_, 2024.


[37] D. Mostafa, G. Stephan, V. Oriol, J. Uszkoreit, and L. Kaiser. Universal Transformers. In _The_
_Seventh International Conference on Learning Representations (ICLR’19)_ . ICLR, 2019.


[38] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: A recurrent depth approach. _arXiv preprint arXiv:2502.05171_,
2025.


[39] Mark Sch¨one, Babak Rahmani, Heiner Kremer, Fabian Falck, Hitesh Ballani, and Jannes
Gladrow. Implicit Language Models are RNNs: Balancing Parallelization and Expressivity.
_arXiv preprint arXiv:2502.07827_, 2025.


[40] Sajad Movahedi, Felix Sarnthein, Nicola Muca Cirone, and Antonio Orvieto. Fixed-point
RNNs: From diagonal to dense in a few iterations. In _First Workshop on Scalable Optimization_
_for Efficient and Adaptive Foundation Models_, 2025.


[41] Matthias Kissel and Klaus Diepold. Structured matrices and their application in neural networks: A survey. _New Generation Computing_, 41(3):697–722, 2023.


[42] Victor D. Dorobantu, Per Andre Stromhaug, and Jess Renteria. DizzyRNN: Reparameterizing Recurrent Neural Networks for Norm-Preserving Backpropagation. _arXiv preprint_
_arXiv:1612.04035_, 2016.


[43] L. Jing, Y. Shen, T. Dubcek, J. Peurifoy, S. Skirlo, Y. LeCun, M. Tegmark, and M. Soljacic.
Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs. In D. Precup and Y. Teh, editors, _Proceedings of the 34th International Conference on Machine Learn-_
_ing (ICML’17)_, volume 70. Proceedings of Machine Learning Research, 2017.


[44] Rumen Dangovski, Li Jing, Preslav Nakov, Mi´co Tatalovi´c, and Marin Soljaˇci´c. Rotational
unit of memory: a novel representation unit for rnns with scalable applications. _Transactions_
_of the Association for Computational Linguistics_, 7:121–138, 2019.


[45] C. Jose, M. Cisse, and F. Fleuret. Kronecker recurrent units. In J. Dy and A. Krause, editors, _Proceedings of the 35th International Conference on Machine Learning (ICML’18)_, volume 80. Proceedings of Machine Learning Research, 2018.


13


[46] Z. Mhammedi, A. Hellicar, A. Rahman, and J. Bailey. Efficient orthogonal parametrisation of
recurrent neural networks using householder reflections. In D. Precup and Y. Teh, editors, _Pro-_
_ceedings of the 34th International Conference on Machine Learning (ICML’17)_, volume 70.
Proceedings of Machine Learning Research, 2017.


[47] Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. _Diploma, Technische_
_Universit¨at M¨unchen_, 91(1):31, 1991.


[48] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with
gradient descent is difficult. _IEEE transactions on neural networks_, 5(2):157–166, 1994.


[49] Kai Biegun, Rares Dolga, Jake Cunningham, and David Barber. RotRNN: Modelling Long
Sequences with Rotations. _arXiv preprint arXiv:2407.07239_, 2024.


[50] Kenneth Krohn and John Rhodes. Algebraic theory of machines. i. prime decomposition theorem for finite semigroups and machines. _Transactions of the American Mathematical Society_,
116:450–464, 1965.


[51] Anil Nerode. Linear automaton transformations. _Proceedings of the American Mathematical_
_Society_, 9(4):541–544, 1958.


[52] G. Del´etang, A. Ruoss, J. Grau-Moya, T. Genewein, L. K. Wenliang, E. Catt, C. Cundy,
M. Hutter, S. Legg, J. Veness, and P. A. Ortega. Neural Networks and the Chomsky Hierarchy. In _The Eleventh International Conference on Learning Representations (ICLR’23)_ .
ICLR, 2023.


[53] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: An intermediate language and
compiler for tiled neural network computations. In _Proceedings of the 3rd ACM SIGPLAN_
_International Workshop on Machine Learning and Programming Languages_, pages 10–19,
2019.


[54] Yvette Kosmann Schwarzbach. Groups and symmetries from finite groups to lie groups, 2010.


[55] Karl Pearson. LIII. On lines and planes of closest fit to systems of points in space. _The London,_
_Edinburgh, and Dublin philosophical magazine and journal of science_, 2(11):559–572, 1901.


[56] Zhixuan Lin, Evgenii Nikishin, Xu Owen He, and Aaron Courville. Forgetting transformer:
Softmax attention with a forget gate. _arXiv preprint arXiv:2503.02130_, 2025.


[57] Guilherme Penedo, Hynek Kydl´ıˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell,
Colin Raffel, Leandro Von Werra, and Thomas Wolf. The FineWeb Datasets: Decanting the
Web for the Finest Text Data at Scale, 2024.


[58] Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. _Natural Language Processing with_
_Transformers_ . O’Reilly Media, Inc., 2022.


[59] OpenThoughts Team. Open Thoughts. https://open-thoughts.ai, January 2025.


[60] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale
distantly supervised challenge dataset for reading comprehension. In _Proceedings of the 55th_
_Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_,
pages 1601–1611, 2017.


[61] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles
Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas
Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,
Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 07 2024.


[62] Olivier Roy and Martin Vetterli. The effective rank: A measure of effective dimensionality. In
_2007 15th European signal processing conference_, pages 606–610. IEEE, 2007.


[63] Rom N Parnichkun, Neehal Tumma, Armin W Thomas, Alessandro Moro, Qi An, Taiji Suzuki,
Atsushi Yamashita, Michael Poli, and Stefano Massaroli. Quantifying Memory Utilization
with Effective State-Size. _arXiv preprint arXiv:2504.19561_, 2025.


14


[64] Mark Rudelson and Roman Vershynin. Sampling from large matrices: An approach through
geometric functional analysis. _Journal of the ACM (JACM)_, 54(4):21–es, 2007.


[65] Alex Graves. Adaptive computation time for recurrent neural networks. _arXiv preprint_
_arXiv:1603.08983_, 2016.


[66] Nicola Muca Cirone and Cristopher Salvi. ParallelFlow: Parallelizing Linear Transformers via
Flow Discretization. _arXiv preprint arXiv:2504.00492_, 2025.


[67] Maximilian Beck, Korbinian P¨oppel, Phillip Lippe, and Sepp Hochreiter. Tiled Flash Linear
Attention: More Efficient Linear RNN and xLSTM Kernels. In _ICLR 2025 Workshop on_
_Foundation Models in the Wild_, 2025.


[68] Robert Schreiber and Beresford Parlett. Block reflectors: Theory and computation. _SIAM_
_Journal on Numerical Analysis_, 25(1):189–205, 1988.


[69] Joseph Gallian. _Contemporary abstract algebra_ . Chapman and Hall/CRC, 2021.


[70] Lorraine L Foster. On the symmetry group of the dodecahedron. _Mathematics Magazine_, 63
(2):106–107, 1990.


[71] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In _The Seventh Interna-_
_tional Conference on Learning Representations (ICLR’19)_ . ICLR, 2019.


[72] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison, A. K¨opf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative
style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d’Alche Buc, E. Fox, and R. Garnett, editors, _Proceedings of the 32nd International Confer-_
_ence on Advances in Neural Information Processing Systems (NeurIPS’19)_, 2019.


[73] I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. In _The_
_Fifth International Conference on Learning Representations (ICLR’17)_ . ICLR, 2017.


[74] J. Fiotto-Kaufman, A. R. Loftus, E. Todd, J. Brinkmann, K. Pal, D. Troitskii, M. Ripa,
A. Belfki, C. Rager, C. Juang, A. Mueller, S. Marks, A. Sen Sharma, F. Lucchetti, N. Prakash,
C. Brodley, A. Guha, J. Bell, B. C. Wallace, and D. Bau. Nnsight and ndif: Democratizing
access to foundation model internals. In _The Twelfth International Conference on Learning_
_Representations (ICLR’24)_ . ICLR, 2024.


[75] Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,
Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikitlearn: Machine learning in Python. _the Journal of machine Learning research_, 12:2825–2830,
2011.


[76] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In _SC20: International Conference for_
_High Performance Computing, Networking, Storage and Analysis_, pages 1–16. IEEE, 2020.


[77] Denis Paperno, Germ´an Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella
Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern´andez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In _Proceedings of the_
_54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-_
_pers)_, pages 1525–1534, 2016.


[78] Yonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning
about physical commonsense in natural language. _Proceedings of the AAAI Conference on_
_Artificial Intelligence_, 34(05):7432–7439, Apr. 2020.


[79] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can
a Machine Really Finish Your Sentence? In _Proceedings of the 57th Annual Meeting of the_
_Association for Computational Linguistics_, pages 4791–4800, 2019.


15


[80] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99–106,
2021.


[81] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
and Oyvind Tafjord. Think you have solved question answering? Try arc, the ai2 reasoning
challenge. _arXiv preprint arXiv:1803.05457_, 2018.


16


**Supplementary Material**


The supplementary material is structured as follows:


Section A presents a proposition that provides special cases of the generalized Householder product
for specific choices of keys and betas and characterizes the spectrum of the product of two generalized Householders.


Section B characterizes the expressivity of DeltaProduct.


   - B.2 demonstrates how DeltaProduct can solve any group word problem in a single layer
when _nh_ is sufficiently high, or alternatively using up to 4 layers when _nh_ is limited.


   - B.3 shows how DeltaProduct can recognize any regular language in a finite number of
layers using the Krohn-Rhodes decomposition.


   - B.4 details the expressivity of a linear RNN with products of RWKV-7 state-transition
matrices.


    - B.5 demonstrates how DeltaNet can solve any dihedral group in 2 layers using a specific
construction.


    - B.6 discusses the tradeoff between expressivity and stability in linear RNNs.


Section C provides comprehensive details on our experiments and additional results.


We provide the code for our experiments at `[https://github.com/automl/DeltaProduct](https://github.com/automl/DeltaProduct)`


**Notation.** Mathematical objects are typically styled as follows. Matrices: uppercase letters (e.g.,
_**A**_ _,_ _**H**_ _,_ _**M**_ ). Vectors: lowercase letters (e.g., _**k**_ _,_ _**v**_ _,_ _**x**_ ). Standard sets: R for reals, C for complexes,
N for naturals. Common symbols and operations are denoted as follows. _**I**_ : Identity matrix. _⊤_ :
Transpose operator (e.g., _**k**_ _[⊤]_ ). _∥_ _**v**_ _∥_ or _∥_ _**v**_ _∥_ 2: Euclidean norm of a vector _**v**_ . _∥_ _**A**_ _∥_ : the operator
norm for a matrix _**A**_ . _| · |_ : Absolute value for real scalars, or modulus for complex numbers. _⊙_ :
Element-wise (Hadamard) product. _σ_ ( _**A**_ ) _, ρ_ ( _**A**_ ): Spectrum (set of eigenvalues) and spectral radius
of matrix _**A**_ . tr( _**A**_ ) _,_ det( _**A**_ ): Trace and determinant of matrix _**A**_ . _δij_ : Kronecker delta (1 if _i_ = _j_, 0
otherwise). _**e**_ _i ∈{_ 0 _,_ 1 _}_ _[n]_ is the _i_ -th element of the canonical basis of R _[n]_ .


**A** **Spectral Properties and Simplifications of Householder Product Matrices**


The following proposition characterizes conditions under which the product structure simplifies and
the spectrum is real, contrasting with the general case which allows for complex spectra. It provides
illustrative special cases for the more general Proposition 1 in Grazzi et al. [16].

**Proposition 1.** _Let_ _**A**_ _∈_ R _[n][×][n]_ _be a matrix defined as the product of nh ≥_ 1 _generalized House-_
_holder transformations:_ _**A**_ = [�] _j_ _[n]_ =1 _[h]_ _**[H]**_ _[j][ where each]_ _**[ H]**_ _[j]_ [ =] _**[ I]**_ _[ −]_ _[β][j]_ _**[k]**_ _[j]_ _**[k]**_ _j_ _[⊤][, with]_ _**[ k]**_ _[j][ ∈]_ [R] _[n][ being a unit]_
_vector (∥_ _**k**_ _j∥_ 2 = 1 _) and βj ∈_ [0 _,_ 2] _. Let σ_ ( _**A**_ ) _⊂_ C _denote the spectrum (set of eigenvalues) of_ _**A**_ _._
_Then, all eigenvalues λ ∈_ _σ_ ( _**A**_ ) _satisfy |λ| ≤_ 1 _and the following hold._

_1._ _**(Identical Direction Vector)**_ _Let_ _**k**_ _∈_ R _[n]_ _be nonzero and_ _**k**_ _j_ = _**k**_ _/ ∥_ _**k**_ _∥_ _for all j_ = 1 _..m. Then_

 - _mj_ =1� _**I**_ _−_ _βj_ _**kk**_ _[⊤]_ [�] = _**I**_ _−_ _β_ _[∗]_ _**kk**_ _[⊤]_ _for some real scalar β_ _[∗]_ _depending on {β}_ _[m]_ _j_ =1 _[. The product]_
_collapses to a single effective transformation of the same form. Consequently, if_ _**A**_ _is formed_
_using only a single direction vector_ _**k**_ 1 _, it is symmetric and its spectrum is real._

_2._ _**(Orthogonal Vectors)**_ _If the direction vectors {_ _**k**_ _j}_ _[n]_ _j_ =1 _[h]_ _[form an orthonormal set (i.e.,]_ _**[ k]**_ _j_ _[⊤]_ _**[k]**_ _[l]_ [ =]
_δjl; this requires nh ≤_ _n), then the factors_ _**H**_ _j commute, and the product simplifies to_ _**A**_ =
_**I**_ _−_ [�] _[n]_ _j_ =1 _[h]_ _[β][j]_ _**[k]**_ _[j]_ _**[k]**_ _j_ _[⊤][. This matrix]_ _**[ A]**_ _[ is symmetric, and its spectrum is purely real:][ σ]_ [(] _**[A]**_ [) =]
_{_ 1 _−_ _β_ 1 _, . . .,_ 1 _−_ _βnh} ∪{_ 1 _(multiplicity n −_ _nh_ ) _}. When βj_ = 2 _for all j ∈{_ 1 _, ..., nh} then_
_**A**_ _is known as a block reflector [68]._


_3._ _**(Complex Spectrum via Non-orthogonal Directions)**_ _For nh_ = 2 _,_ _**A**_ _has complex eigenvalues_
_if two consecutive direction vectors, e.g._ _**k**_ 1 _,_ _**k**_ 2 _satisfy_ 0 _< |_ _**k**_ 1 _[⊤]_ _**[k]**_ [2] _[|][ <]_ [ 1] _[ and their coefficients]_
_β_ 1 _, β_ 2 _exceed a threshold β_ _[∗]_ ( _θ_ ) _<_ 2 _dependent on the angle θ between them. Conversely, if_
0 _≤_ _β_ 1 _≤_ 1 _or_ 0 _≤_ _β_ 2 _≤_ 1 _, these eigenvalues from the 2D span are guaranteed to be real._


_Proof._ **Identical Direction Vector** If _m_ = 1, then the statement is trivially satisfied with _β_ _[∗]_ = _β_ 1.
Suppose the statement is true for _m ≥_ 1, i.e., [�] _j_ _[m]_ =1 [(] _**[I]**_ _[ −]_ _[β][j]_ _**[ kk]**_ _[⊤]_ [) =] _**[ I]**_ _[ −]_ _[β]_ [(] _[m]_ [)] _**[ kk]**_ _[⊤][.]_ [ Multiplying by]


17


|2 0.0 ◦|0|0.0 ◦|
|---|---|---|
|0<br><br>0<br>|||
|0<br><br>0<br>|||


|Col1|Col2|
|---|---|
|||
|||


|Col1|Col2|
|---|---|
|||
|||


|Col1|Col2|
|---|---|
|||
|||


|Col1|Col2|
|---|---|
|||
|||


|Col1|Col2|
|---|---|
|||
|||


|Col1|Col2|
|---|---|
|||
|||


|Col1|Col2|
|---|---|
|||
|||


|Col1|Col2|
|---|---|
|||
|||


|Col1|Col2|
|---|---|
|||
|||


|Col1|Col2|
|---|---|
|||
|||


|Col1|Col2|
|---|---|
|||
|||


|Col1|Col2|
|---|---|
|||
|||



Figure 11: Visualization of complex eigenvalues of ( _**I**_ _−β_ 1 _**k**_ 1 _**k**_ 1 _[⊤]_ [)(] _**[I]**_ _[−][β]_ [2] _**[k]**_ [2] _**[k]**_ 2 _[⊤]_ [)][ with][ cos] _[ θ]_ [ =] _**[ k]**_ 1 _[⊤]_ _**[k]**_ [2][.]
Complex region in red, real in white.


( _**I**_ _−βm_ +1 _**kk**_ _[⊤]_ ) produces ( _**I**_ _−β_ [(] _[m]_ [)] _**kk**_ _[⊤]_ )( _**I**_ _−βm_ +1 _**kk**_ _[⊤]_ ) = _**I**_ _−_ - _β_ [(] _[m]_ [)] + _βm_ +1 _−β_ [(] _[m]_ [)] _βm_ +1� _**kk**_ _[⊤]_ _._
Hence, by induction, the product of any number of such factors remains of the form _**I**_ _−_ _β_ _[∗]_ _**kk**_ _[⊤]_ .
Since the resulting matrix _**A**_ = _**I**_ _−_ _β_ _[∗]_ _**kk**_ _[⊤]_ (where _**k**_ = _**k**_ 1) is symmetric, its eigenvalues are real.

**Orthogonal Vectors** Assume _{_ _**k**_ _j}j_ _[n]_ =1 _[h]_ [is an orthonormal set (] _**[k]**_ _j_ _[⊤]_ _**[k]**_ _[l]_ [ =] _[ δ][jl]_ [,] _[ n][h][ ≤]_ _[n]_ [). Let] _**[ P]**_ _[j]_ [ =]
_**k**_ _j_ _**k**_ _j_ _[⊤]_ [. Then] _**[ P]**_ _[j]_ _**[P]**_ _[l]_ [ =] _[ δ][jl]_ _**[P]**_ _[j]_ [. The factors] _**[ H]**_ _[j]_ [ =] _**[ I]**_ _[ −]_ _[β][j]_ _**[P]**_ _[j]_ [ commute because] _**[ P]**_ _[j]_ _**[P]**_ _[l]_ [ =] **[ 0]** [ for]
_j ̸_ = _l_ . The product simplifies via induction to _**A**_ = _**I**_ _−_ [�] _[n]_ _j_ =1 _[h]_ _[β][j]_ _**[P]**_ _[j]_ [. This matrix is symmetric. Its]
eigenvalues are (1 _−_ _βl_ ) for _l_ = 1 _, . . ., nh_ (eigenvector _**k**_ _l_ ) and 1 with multiplicity _n_ _−_ _nh_ (subspace
orthogonal to all _**k**_ _j_ ). The spectrum is real.


**Complex Spectrum via Non-orthogonal Directions and Real Subcase** Let _**k**_ 1 _,_ _**k**_ 2 span a 2D subspace _S ⊂_ R _[n]_ with cos _θ_ = _**k**_ 1 _[⊤]_ _**[k]**_ [2] [such that][ 0] _[ <][ |]_ [ cos] _[ θ][|][ <]_ [ 1][. The product] _**[ A]**_ [ =] _**[ H]**_ [2] _**[H]**_ [1] [acts as the]
identity on _S_ _[⊥]_ (preserving _n −_ 2 eigenvalues at 1) and non-trivially on _S_ . The restriction _**A**_ _S_ of _**A**_
to _S_ has trace tr( _**A**_ _S_ ) = 2 _−_ _β_ 1 _−_ _β_ 2 + _β_ 1 _β_ 2 cos [2] _θ_ and determinant det( _**A**_ _S_ ) = (1 _−_ _β_ 1)(1 _−_ _β_ 2).
The discriminant of its characteristic equation is _D_ = [tr( _**A**_ _S_ )] [2] _−_ 4 det( _**A**_ _S_ ). Complex eigenvalues
arise if _D <_ 0.


To find the explicit bounds, we expand the inequality _D <_ 0:


(2 _−_ _β_ 1 _−_ _β_ 2 + _β_ 1 _β_ 2 cos [2] _θ_ ) [2] _−_ 4(1 _−_ _β_ 1)(1 _−_ _β_ 2) _<_ 0


Rearranging this expression as a quadratic in _x_ = cos [2] _θ_ yields:


( _β_ 1 _β_ 2) [2] _x_ [2] + 2(2 _−_ _β_ 1 _−_ _β_ 2) _β_ 1 _β_ 2 _x_ + ( _β_ 1 _−_ _β_ 2) [2] _<_ 0


This inequality holds if and only if _x_ = cos [2] _θ_ lies strictly between the two roots of the corresponding equation. Solving for the roots gives the explicit bounds for complex eigenvalues:



_−_ _[√]_ _β_ 2 _−_ 1) [2] _[√][β]_ [2] _[ −]_ [1)][2]

_<_ cos [2] _θ <_ [(] _[√][β]_ [1] _[ −]_ [1 +]
_β_ 1 _β_ 2 _β_ 1 _β_ 2



( _[√]_ _β_ 1 _−_ 1 _−_ _[√]_ _β_ 2 _−_ 1) [2]



_β_ 1 _β_ 2



This inequality is only satisfiable when _β_ 1 _, β_ 2 _∈_ (1 _,_ 2]. For the special case of two standard reflections where _β_ 1 = _β_ 2 = 2, the condition simplifies to 0 _<_ cos [2] _θ <_ 1, confirming that the product of
any two distinct reflections is a rotation.


Conversely, we show that if at least one coefficient _βi ∈_ [0 _,_ 1], the eigenvalues are real as _D ≥_ 0.
We analyze this by cases:


- **Case 1: One coefficient is in** [0 _,_ 1] **, the other is in** (1 _,_ 2] **.** Without loss of generality, let _β_ 1 _∈_ [0 _,_ 1]
and _β_ 2 _∈_ (1 _,_ 2]. This implies (1 _−_ _β_ 1) _≥_ 0 and (1 _−_ _β_ 2) _<_ 0, so their product det( _**A**_ _S_ ) _≤_ 0.
The term _−_ 4 det( _**A**_ _S_ ) is therefore non-negative. Since [tr( _**A**_ _S_ )] [2] _≥_ 0, their sum _D_ must be
non-negative.


- **Case 2: Both coefficients are in** [0 _,_ 1] **.** By the AM-GM inequality on the non-negative terms
(1 _−_ _β_ 1) and (1 _−_ _β_ 2), we have (1 _−_ _β_ 1) + (1 _−_ _β_ 2) _≥_ 2�(1 _−_ _β_ 1)(1 _−_ _β_ 2) = 2 ~~�~~ det( _**A**_ _S_ ).



(1 _−_ _β_ 1)(1 _−_ _β_ 2) = 2 ~~�~~



(1 _−_ _β_ 1) and (1 _−_ _β_ 2), we have (1 _−_ _β_ 1) + (1 _−_ _β_ 2) _≥_ 2 (1 _−_ _β_ 1)(1 _−_ _β_ 2) = 2 det( _**A**_ _S_ ).

Since tr( _**A**_ _S_ ) includes an additional non-negative term _β_ 1 _β_ 2 cos [2] _θ_, it also holds that tr( _**A**_ _S_ ) _≥_
2 ~~�~~ det( _**A**_ _S_ ). Squaring both sides gives [tr( _**A**_ _S_ )] [2] _≥_ 4 det( _**A**_ _S_ ), ensuring _D ≥_ 0.



det( _**A**_ _S_ ). Squaring both sides gives [tr( _**A**_ _S_ )] [2] _≥_ 4 det( _**A**_ _S_ ), ensuring _D ≥_ 0.



This analysis confirms that complex eigenvalues, which enable rotations, can only arise if and only
if both _β_ 1 _>_ 1 and _β_ 2 _>_ 1. When at least one _βi ≤_ 1, real eigenvalues restrict the transformations
in that subspace to scaling or reflection. This clear distinction in behavior, dictated by the _β_ values
and _θ_, is illustrated in Figure 11.


18


**B** **Expressivity of DeltaProduct**


In this section, we characterize the expressivity of (Gated) DeltaProduct in solving group word problems and recognizing regular languages, in support of Section 4.1. The results hold in finite precision
(since our constructions require a finite number of values), and take inspiration from Peng et al. [13,
Appendix D] and [16]. We begin by stating and discussing our key assumptions in Section B.1. We
then present our main results for group word problems in Section B.2, followed by our findings for
regular languages in Section B.3. In Section B.5, we examine a result specific to dihedral groups.
Finally, we explore the fundamental tradeoff between expressivity and stability of the recurrence in
Section B.6.


**B.1** **Assumptions**


We consider a (Gated) DeltaProduct model where each layer is structured as


_**H**_ _i_ = _**A**_ ( _**x**_ _i_ ) _**H**_ _i−_ 1 + _**B**_ ( _**x**_ _i_ ) _,_ _**y**_ ˆ _i_ = dec( _**H**_ _i,_ _**x**_ _i_ ) where _i ∈_ 1 _, . . ., t_



_nh_ _nh_
�� 


_j_ =1




- - [�]
_**I**_ _−_ _βi,k_ _**k**_ _i,k_ _**k**_ _i,k_ _[⊤]_ _βi,j_ _**k**_ _i,j_ _**v**_ _i,j_ _[⊤]_ _[,]_



_**A**_ ( _**x**_ _i_ ) = _gi_



_nh_
�� _**I**_ _−_ _βi,j_ _**k**_ _i,j_ _**k**_ _i,j_ _[⊤]_ - _,_ _**B**_ ( _**x**_ _i_ ) =

_j_ =1



_k_ = _j_ +1



where _gi_ is only present in the gated variant and there is only one head per layer. If _H_ heads are
considered, head _j_ will run the recurrence _**H**_ _i_ _[j]_ [=] _**[ A]**_ _[j]_ [(] _**[x]**_ _[i]_ [)] _**[H]**_ _[i][−]_ [1][ +] _**[ B]**_ _[j]_ [(] _**[x]**_ _[i]_ [)][, (with different learnable]
parameters from all other heads) and all the states will be passed to the decoder to get the output as
_**y**_ ˆ _i_ = dec(( _**H**_ _i_ [1] _[, . . .,]_ _**[ H]**_ _i_ _[H]_ [)] _[,]_ _**[ x]**_ _[i]_ [)][.]

**Each layer receives the outputs of all previous layers.** We assume that the output of all layers is
passed as input to all following layers: this can be achieved by using the residual connections (which
would be inside dec) and by placing the output of different layers onto separate subspaces.


**Task-dependent initial state.** We assume that the initial state _**H**_ 0 can be set appropriately depending on the task and is of rank at most _nh_ .


**Arbitrary decoder and state transition functions.** We also assume that for every _i, j_, _gi ∈_ [0 _,_ 1],
_βi,j ∈_ [0 _,_ 2] _,_ _**k**_ _i,j ∈_ R _[d]_, _∥_ _**k**_ _i,j∥_ = 1 and _**v**_ _i,j ∈_ R _[d]_ are arbitrary continuous functions of _**x**_ _i_ . The
function dec can also be arbitrary (continuous). Since in all our setups the possible values of _**x**_ _i_
and _**H**_ _i_ are finite, this implies that the functions dec, _**A**_ _,_ _**B**_ can model an arbitrary function of their
discrete domain of interest, with the only restriction coming from the structural assumption of the
output spaces of _**A**_ (product of _nh_ Householders) and _**B**_ (rank _nh_ ). This assumption can always be
fulfilled in practice if dec is a sufficiently wide MLP (see the next section for practical concerns)
and, in the case of _**A**_ _,_ _**B**_, which generally do not contain MLPs, by setting the dimension of _**x**_ _i_
sufficiently large, which can be achieved by adjusting the embedding layer or the dimensionality of
the output of dec. Note that the width of the MLP will grow with the complexity of the function to
be approximated, which in our case depends on the complexity of the problem.


**B.1.1** **Practical Considerations**


**Beginning of sequence token.** Alternatively, the assumption on _**H**_ 0 (task-dependent with rank at
most _nh_ ) can be replaced by using a beginning of sequence token _x_ 1 = $ and setting _**H**_ 0 = 0, as
done in practice, so that _**H**_ 1 = _**B**_ ($) is a learnable matrix of rank at most _nh_ which acts as the _**H**_ 0
in our constructions.


**Decoder implementation.** In our implementation, dec is the same as in Gated DeltaNet:


dec(( _**H**_ _t_ [1] _[, . . .]_ _**[ H]**_ _t_ _[H]_ [)] _[,]_ _**[ x]**_ _[t]_ [) = MLP(RMSnorm(] _**[x]**_ _[t]_ [+] _**[ o]**_ _[t]_ [))] _[,]_




_**o**_ _t_ =



_H_

- _**W**_ _o_ _[j]_ [RMSnorm((] _**[H]**_ _t_ _[j]_ [)] _[⊤]_ _**[q]**_ _t_ _[j]_ [)] _[,]_ _**q**_ _t_ _[j]_ [=] _[ ψ]_ [(] _**[W]**_ _q_ _[ j]_ _**[x]**_ _[t]_ [)] _[/][||][ψ]_ [(] _**[W]**_ _q_ _[ j]_ _**[x]**_ _[t]_ [)] _[||]_

_j_ =1




                where _**W**_ _o_ _[j][,]_ _**[ W]**_ _q_ _[ j]_ [are two learned matrices,] _[ ψ]_ [ = SiLU][,][ RMSnorm(] _**[x]**_ [) =] _**[ a]**_ _[ ⊙]_ _**[x]**_ _[/]_ _ϵ_ + _d_ _[−]_ [1] ~~[�]~~ _[d]_ _i_ =1 _[x]_ _i_ [2]

corresponds (when _ϵ_ = 0) to a projection onto an axis aligned ellipse where _**a**_ _∈_ R _[d]_ is learnable
and determines the lengths of the axis and _ϵ >_ 0 is a small value set to avoid numerical errors.


19


Meanwhile, MLP is a two layer MLP. This structure can be limiting. For instance, when _**H**_ _t_ _[j]_ _[∈]_ [R] _[d]_ [,]
then _bt_ = ( _**H**_ _t_ _[j]_ [)] _[⊤]_ _**[q]**_ _[t]_ _[∈]_ [R][ and if] _[ b][t]_ _[>> ϵ]_ [, then] _[ |]_ [RMSNorm(] _[b][t]_ [)] _[| ≈]_ [1][, which means that after]
RMSNorm we are left with effectively only 2 possible values, while our constructions might require
more: as many as the number of possible states. Indeed, for all our constructions to work in practice,
we would need a sufficiently large _ϵ_, so that the output of RMSnorm can retain some magnitude
information. Since in our constructions the number of states is finite, with _ϵ >_ 0 and an appropriate
value for _**q**_ _t_ _[j]_ [we are guaranteed that the map] _**[ H]**_ _t_ _[j]_ _[�→]_ [RMSnorm((] _**[H]**_ _t_ _[j]_ [)] _[⊤]_ _**[q]**_ _t_ _[j]_ [)][, and consequently (by]
appropriately setting _**W**_ _o_ _[j]_ [) the map from states and inputs] _**[ x]**_ _[t]_ [, to the input of the MLP, is injective,]
which we show in the next lemma. Hence, thanks to the MLP, the decoder can approximate any
continuous function of ( _**H**_ _t,_ _**x**_ _t_ ) even after the bottlenecks caused by the scalar product with _**q**_ _t_ _[j]_ [and]
the RMSnorm.
**Lemma 1.** _Let S ⊂_ R _be a finite set of values. Define δ_ min = min _x,y∈S,x_ = _y |x −_ _y| and δ_ max =
max _x,y∈S |x −_ _y|. Let_ _**b**_ = ( _b, b_ [2] _, . . ., b_ _[d]_ ) _[⊤]_ _and set_ _**q**_ = _**b**_ _/ ∥_ _**b**_ _∥. If b satisfies b ≥_ _[δ]_ _δ_ [max] min [+ 1] _[, then the]_

_mapping f_ : _S_ _[d]_ _→_ R _given by f_ ( _**H**_ ) = RMSnorm( _**H**_ _[⊤]_ _**q**_ ) _is injective._


_Proof._ The mapping _f_ is a composition _f_ = _g_ 3 _◦_ _g_ 2 _◦_ _g_ 1, where the component functions are:



_g_ 1( _**H**_ ) =



_d_

- _hib_ _[i]_ _,_ _g_ 2( _x_ ) = _x_ _g_ 3( _x_ ) = RMSnorm( _x_ ) _,_

_∥_ _**b**_ _∥_ _[,]_
_i_ =1



where _**H**_ = ( _h_ 1 _, . . ., hd_ ) _[⊤]_ . The overall mapping is injective if each component is injective.


**1. Injectivity of** _g_ 1 **:** Intuitively, _g_ 1 encodes the vector _**H**_ as a scalar as a number in base _b_, where
each _hi_ acts as a “digit” drawn from the finite set _S_ . By choosing _b_ sufficiently large relative to
the spread of _S_ (captured by _δ_ max _/δ_ min), we ensure that different vectors produce distinct scalars.
To prove this formally, we show that for any non-zero difference vector **∆** = (∆1 _, . . .,_ ∆ _d_ ) _[⊤]_ =
_**H**_ 1 _−_ _**H**_ 2, the difference _g_ 1( _**H**_ 1) _−_ _g_ 1( _**H**_ 2) = [�] _i_ _[d]_ =1 [∆] _[i][b][i]_ [ is also non-zero. This is true since]
_b_ is by definition greater than Cauchy polynomial upper bound for the roots of the polynomial
_P_ ( _b_ ) = [�] _i_ _[d]_ =1 [∆] _[i][b][i]_ [. For an elementary proof, let] _[ k]_ [ = max] _[{][i][ |]_ [ ∆] _[i][ ̸]_ [= 0] _[}]_ [. The magnitude of the]
highest-order term is lower-bounded by:

_|_ ∆ _kb_ _[k]_ _|_ = _|_ ∆ _k|b_ _[k]_ _≥_ _δ_ min _b_ _[k]_



The magnitude of the sum of lower-order terms is upper-bounded as

_k−_ 1 _k−_ 1 _k−_ 1      - _k_

  - ∆ _ib_ _[i]_ _≤_   - _|_ ∆ _i|b_ _[i]_ _≤_   - _δ_ max _b_ _[i]_ = _δ_ max _b_ _−_ _b_

_b −_ 1

����� _i_ =1 ����� _i_ =1 _i_ =1



_k−_ 1

- ∆ _ib_ _[i]_ _≤_

_i_ =1 �����



_k−_ 1




_k−_ 1





- _|_ ∆ _i|b_ _[i]_ _≤_

_i_ =1



_k−_ 1





- _δ_ max _b_ _[i]_ = _δ_ max

_i_ =1




- _k_
_b_ _−_ _b_



_b −_ 1




_≤_ _δ_ min( _b_ _[k]_ _−_ _b_ ) _,_



where we used the triangle inequality and the assumption on _b_, which implies _δ_ max _≤_ _δ_ min( _b −_ 1).
Comparing the bounds, we see that



_|_ ∆ _kb_ _[k]_ _| ≥_ _δ_ min _b_ _[k]_ _> δ_ min( _b_ _[k]_ _−_ _b_ ) _≥_

�����



_k−_ 1

- ∆ _ib_ _[i]_

_i_ =1 �����



Since the magnitude of the highest-order term is strictly greater than that of the sum of all other
terms, their sum cannot be zero. Thus, _g_ 1 is injective.

**2. Injectivity of** _g_ 2 **and** _g_ 3 **:** The function _g_ 2 is a linear scaling by the non-zero constant 1 _/ ∥_ _**b**_ _∥_
and is therefore injective. For _g_ 3( _x_ ) = RMSnorm( _x_ ), its derivative is strictly positive for _ϵ >_ 0,
meaning _g_ 3 is strictly monotonic and also injective.

Since _g_ 1, _g_ 2, and _g_ 3 are all injective, their composition _f_ is also injective.



When the state is one-hot, i.e. _**H**_ _t_ _[j]_ [=] _**[ e]**_ _[i]_ _[∈{]_ [0] _[,]_ [ 1] _[}][d]_ [, with][ 1] _[ ≤]_ _[i][ ≤]_ _[d]_ [ (i-th element of the canonical]
basis), an alternative to the above construction is to replicate the recurrence onto _d_ heads, where the
_√_ _√_
_j_ -th head has _**W**_ _o_ _[j]_ [=] _**[ q]**_ _t_ _[j]_ [=] _**[ e]**_ _[j]_ [, so that, assuming that in the RMSnorm] _**[ a]**_ [ = (] _d, . . .,_ _d_ ) _[⊤]_ and



_√_
_d, . . .,_



_j_ -th head has _**W**_ _o_ _[j]_ [=] _**[ q]**_ _t_ _[j]_ [=] _**[ e]**_ _[j]_ [, so that, assuming that in the RMSnorm] _**[ a]**_ [ = (] _d, . . .,_ _d_ ) _[⊤]_ and

_ϵ_ = 0, we get _**o**_ _t_ = _**H**_ _t_ = _**e**_ _i_ . This is the strategy used in Peng et al. [13, Appendix D]. However,
for some problems using the one-hot encoding states _**e**_ 1 _, . . ._ _**e**_ _n_ is not very efficient. For instance, to
solve the _Sn_ word problem one would need _n_ !-dimensional one-hot vectors as states, while in our
Theorem 1 we use _n_ -dimensional vectors. Moreover, learning multiple identical heads is redundant
and indeed we observe that in our synthetic experiments, the model is learning to use only one head
to solve the tasks (see Section 5.2).



20


**B.2** **Group Word Problems**

The next theorem establishes that DeltaProduct can solve the word problem for the symmetric group
_Sn_, which implies that it can also solve any group word problem, since for every group _G_ there exists
_n_ such that _G_ is isomorphic to a subgroup of _Sn_ .
**Theorem 3** (Restatement of Theorem 1) **.** _For any n ∈_ N _there exists a DeltaProduct model with_
_one of the following configurations that can solve the word problem of the symmetric group Sn: (i)_
_one layer with nh_ = _n−_ 1 _[16, Theorem 3] (ii) 3 layers with nh>_ 1 _(iii) 4 layers with nh_ =1 _. The_
_construction for (ii) and (iii) requires that the MLP at the second last layer computes a lookup-_
_table of size_ 2 _m ×_ ( _n_ !) [2] _[m]_ _, function of the last_ 2 _m input tokens and the position modulo_ 2 _m with_
_m_ = _⌈_ ( _n−_ 1) _/nh⌉._


_Proof._ One way to solve the group word problem for the symmetric group _Sn_ is to map each element
of the group _g ∈_ _Sn_ to the corresponding permutation matrix _**P**_ _g ∈{_ 0 _,_ 1 _}_ _[n]_ and then for each input
sequence _x_ 1 _, . . ., xt_ with _xi ∈_ _Sn_ compute each element of the output sequence _y_ 1 _, . . ., yt_ as

_yi_ = _xi · xi−_ 1 _· · · x_ 1 = _ϕ_ ( _**P**_ _xi · · ·_ _**P**_ _x_ 1 _**u**_ 0) _,_ _**u**_ 0 = (1 _, . . ., n_ ) _[⊤]_ _,_
where _ϕ_ is a surjective map from vectors in _{_ 1 _, . . ., n}_ _[n]_ to the _n_ ! elements of _Sn_, which we consider
integers for simplicity, i.e. _xi, yi ∈{_ 1 _, . . ., n_ ! _}_ .

**(i).** Since a permutation of _n_ elements is a series of at most _n−_ 1 swaps of 2 elements, if _nh_ = _n−_ 1,
then we can solve the problem with a 1-layer DeltaProduct by setting _**H**_ 0 = _**u**_ 0, dec( _**H**_ _i, xi_ ) =
_ϕ_ ( _**H**_ _i_ ), _**B**_ ( _xi_ ) = 0 ( _**v**_ _i,j_ = 0), _**A**_ ( _xi_ ) = _**P**_ _xi_ . The latter is achieved by setting for _√_ the _j_ -th element
in the product [�] _j_ _[n]_ =1 _[h]_ [(] _[I][ −]_ _[β][i,j]_ _**[k]**_ _[i,j]_ _**[k]**_ _i,j_ _[⊤]_ [), either] _[ β][i,j]_ [ = 2][ and] _**[ k]**_ _[i,j]_ [ = (] _**[e]**_ _[k][ −]_ _**[e]**_ _[p]_ [)] _[/]_ 2 with _**e**_ _i_ being

the _i_ -th element of the canonical basis of R _[n]_ (swap element at index _k_ with the one at index _p_ ), or
_βi,j_ = 0 (identity).

**(ii) and (iii).** If _nh < n_ _−_ 1, then the state transition matrix is not sufficiently expressive to represent
all permutations of _n_ elements. However, we can use additional layers to overcome this issue as
follows. We divide the input sequence into blocks of _m_ elements: we factorize the position _i ∈_
_{_ 1 _, . . . t}_ into _i_ = _lm_ + [˜] _i_ where _l ≥_ 0 is the index of the previous block and [˜] _i ∈{_ 1 _, . . ., m}_
is the position within the current block (index _l_ + 1). First, consider the case when _l ≥_ 1. Let
_**P**_ ˜ _l_ = _**P**_ _x_ ( _l−_ 1) _m_ + _m . . ._ _**P**_ _x_ ( _l−_ 1) _m_ +1 be the product of the permutations of the previous block. Since ˜ _**P**_ _l_
is a permutation matrix of _n_ elements, we can factor it into _**P**_ [˜] _l_ = _**G**_ _l,m · · ·_ _**G**_ _l,_ 1 where we recall that
_m_ = _⌈_ ( _n_ _−_ 1) _/nh⌉_ and each of _**G**_ _l,_ 1 _, . . .,_ _**G**_ _l,m_ is a product of _nh_ generalized Householder matrices
and thus can be a state-transition matrix of our model. We fix one factorization for each possible
permutation matrix and we set _**P**_ [˜] 0 = _**G**_ 0 _,m · · ·_ _**G**_ 0 _,_ 1, with _**G**_ 0 _,i_ = _I_ to handle the case when _l_ = 0.


Now let _**x**_ _i_ be the input of the last layer. if _**x**_ _i_ contains enough information about previous tokens
(as we specify later), we can set the recurrence and decoder of the last layer as




             _**H**_ _i_ = _**G**_ _l,_ ˜ _i_ _**H**_ _i−_ 1 _,_ dec( _**H**_ _i,_ _**x**_ _i_ ) = _ϕ_ _**P**_ _xi · · ·_ _**P**_ _xlm_ +1

                  - ~~��~~ ~~�~~
current block



_**G**_ _l,m · · ·_ _**G**_ _l,_ ˜ _i_ +1

~~�~~ ~~��~~ ~~�~~
previous block




 _**H**_ _i_ _._



where _**H**_ 0 = _**u**_ 0, _**B**_ ( _**x**_ _i_ ) = 0, _**A**_ ( _**x**_ _i_ ) = _**G**_ _l,_ ˜ _i_, using the construction at point (i) since _**G**_ _l,_ ˜ _i_ is a
product of at most _nh_ Householers. Note that _**H**_ _i_ contains the product of the input permutations
only up to token _x_ ( _l−_ 1) _m_ and a partial computation of previous block of permutations _**P**_ [˜] _l_ . Hence,
the decoder completes the computation by applying two additional components: (1) the remaining
transformations _**G**_ _l,_ ˜ _i_ +1 through _**G**_ _l,m_ needed to complete _**P**_ [˜] _l_, and (2) the actual permutations from
the current partial block _**P**_ _xlm_ +1 through _**P**_ _xi_ . The delay in the recurrence is necessary, since to
compute even the first matrix of the factorization for a block of _m_ elements of the input sequence,
all the elements in such a block need to be processed.


We can check that this ends up computing the correct output _yi_ by substituting the expression for
_**H**_ _i_ and unrolling the recurrence as follows.
dec( _**H**_ _i,_ _**x**_ _i_ ) = _ϕ_ ( _**P**_ _xlm_ +˜ _i · · ·_ _**P**_ _xlm_ +1 _**G**_ _l,m · · ·_ _**G**_ _l,_ 1 _**G**_ _l−_ 1 _,m · · ·_ _**G**_ _l−_ 1 _,_ 1 _. . ._ _**G**_ 0 _,m . . ._ _**G**_ 0 _,_ 1 _**H**_ 0) _._

= _ϕ_ ( _**P**_ _xlm_ +˜ _i · · ·_ _**P**_ _xlm_ +1 _**P**_ [˜] _l_ _**P**_ [˜] _l−_ 1 _. . ._ _**P**_ [˜] 0 _**u**_ 0) _._

= _ϕ_ ( _**P**_ _xlm_ +˜ _i · · ·_ _**P**_ _xlm_ +1 _**P**_ _x_ ( _l−_ 1) _m_ + _m · · ·_ _**P**_ _x_ ( _l−_ 1) _m_ +1 _. . ._ _**P**_ _xm · · ·_ _**P**_ _x_ 1 _**P**_ [˜] 0 _**u**_ 0) _._

= _ϕ_ ( _**P**_ _xi · · ·_ _**P**_ _x_ 1 _**u**_ 0) = _yi,_


21


Note that to compute _**A**_ ( _**x**_ _i_ ) = _**G**_ _l,_ ˜ _i_ and dec( _**H**_ _i,_ _**x**_ _i_ ), _**x**_ _i_ should contain [˜] _i_ = _i_ mod _m_ and the last
_m_ + [˜] _i_ (in general the last 2 _m_ ) tokens, corresponding to the current and previous blocks. Hence, the
layers before the last one are dedicated to compute at each time-step _i_ a lookup table for the possible
values of ( _i_ mod 2 _m, xi, . . ., xi−_ 2 _m_ +1) whose output will be included in the input of the last layer
_**x**_ _i_ . The first layers (two layers if _nh_ = 1, one if _nh >_ 1) can provide _i_ mod 2 _m_ by using Lemma 2
with _d_ = 2 _m_ . Finally, the second to last layer can output any function of the last 2 _m_ tokens and the
position modulo 2 _m_ through Lemma 3 with _d_ = 2 _m_ and _at_ = _xt_, by using _i_ mod 2 _m_ from the first
layer(s).


**Lemma 2.** _The following DeltaProduct configurations can count modulo d ∈_ N _. (i) 2 layers each_
_with one head and nh_ = 1 _[16, Theorem 6]. (ii) 1 layer with one head and nh ≥_ 2 _._


_Proof._ For (i), we can use the same construction as in [16, Theorem 6], where the first layer does
counting modulo 2 and the second layer computes addition modulo _d_ . In this case, since we just
want to count modulo _d_ we can ignore input tokens and add 1 modulo _d_ at each time-step. For (ii),
note that if _nh >_ 2, we can set, for any time-step _t_, _**B**_ ( _**x**_ _t_ ) = 0 and the state transition matrix _**A**_ ( _**x**_ _t_ )
equal to a 2D rotation with an angle of 2 _π/d_ by appropriately setting two keys, say _**k**_ _t,_ 1 _,_ _**k**_ _t,_ 2, setting
_βt,_ 1 _, βt,_ 2 = 2 (while for the other Householders we set _βi,j_ = 0). Then, we can count modulo _d_ by
setting _**H**_ 0 in the span of _**k**_ _t,_ 1 _,_ _**k**_ 1 _,_ 2 and dec appropriately to map the _d_ values that _**H**_ _t_ can take to
the correspondent element in _{_ 1 _, . . ., d}_ .


**Lemma 3.** _A DeltaProduct layer with nh_ = 1 _, receiving in its input at time-step t the tuple_ ( _t_ mod
_d, at_ ) _(t ≥_ 1 _) where at ∈_ _D ⊂_ R _with D being a discrete set of values, can implement any function_
_of_ ( _t_ mod _d, at−d_ +1 _, . . ., at_ ) _, where for simplicity we set ai_ = _a /∈_ _D for i ∈{_ 2 _−_ _d, . . .,_ 0 _}._


_Proof._ Let _t_ [˜] = _t_ mod _d_ + 1 Set _**H**_ 0 = 0 _∈_ R _[d]_ and the recurrence update as

_**H**_ _t_ = ( _I −_ _**e**_ _t_ ˜ _**e**_ _[⊤]_ _t_ ˜ [)] _**[H]**_ _t−_ 1 [+] _**[ e]**_ _t_ [˜] _[a]_ _t_ _[,]_

where _**e**_ _i_ is the _i_ -th element of the canonical basis of R _[d]_ . This can be implemented by setting _βt,_ 1 =
1, _**k**_ _t,_ 1 = _**e**_ _t_ ˜, _**v**_ _t,_ 1 = _at_ and _βt,j,_ _**k**_ _t,j,_ _**v**_ _t,j_ = 0. With this choice, _**H**_ _t_ contains _at−d_ +1 _, . . ., at_ . The
result follows since dec can be an arbitrary continuous function of both _**H**_ _t_ and _**x**_ _t_ and the latter
contains _t_ mod _d_ .


Finally, the next results concern finite subgroups of the orthogonal and special orthogonal groups.


**Theorem 4.** _Let G be a group isomorphic either to a subgroup of_ O( _n_ ) _, or to a subgroup of_ SO( _n_ +
1) _if n is even, then if nh_ = _n, there exists a DeltaProduct model that solves the group word problem_
_for G._


_Proof._ From the assumption we can map each element _g ∈_ _G_ to an orthogonal matrix _**G**_ _g_ . For the
word problem for _G_, each element of the input sequence belongs to _G_ : _xi ∈_ _G_ for every _i_ .

If _**G**_ _g ∈_ O( _n_ ), then, since _nh_ = _n_ and every orthogonal _n × n_ matrix can be written as the product
of at most _n_ Householder matrices, we can set _**H**_ 0 = _I ∈_ R _[n][×][n]_ and _**A**_ ( _xi_ ) = _**G**_ _xi_, _**B**_ ( _xi_ ) = 0
and dec( _**H**_ _t, xi_ ) = _ϕ_ ( _**H**_ _i_ ) with _ϕ_ : O( _n_ ) _→_ _G_ bijective (which exists due to the isomorphism).
The Householder product structure enables _**A**_ ( _xi_ ) to represent general orthogonal matrices _**G**_ _xi_,
including rotations.


If instead _**G**_ _g ∈_ SO( _n_ + 1), since _n_ is even in this case then we can still write _**G**_ _g_ as a product of an
even number (at most _n_ since _n_ + 1 is odd) of Householder matrices of dimension _n_ + 1 _× n_ + 1.
This is because the determinant of _**G**_ _g_ is +1, which is only possible if it is a product of an even
number of Householder matrices, each having determinant _−_ 1. Thus, we can set _**A**_ ( _xi_ ) = _**G**_ _xi ∈_
R _[n]_ [+1] _[×][n]_ [+1], _**B**_ ( _xi_ ) = 0. Now if we let _**G**_ [¯] = _**G**_ _xi_ _**G**_ _xi−_ 1 _· · ·_ _**G**_ _x_ 1 and set _**H**_ 0 = diag(1 _, . . .,_ 1 _,_ 0) _∈_
R [(] _[n]_ [+1)] _[×]_ [(] _[n]_ [+1)] (we are only allowed a rank n matrix), then _**H**_ _i_ = _**GH**_ [¯] 0 will have all the first _n_
columns equal to _**G**_ [¯] and the last set to zero. However, the last column can be found as a function of
the others since it must be the unique unit vector orthogonal to all other columns of _**G**_ [¯] and for which
det( _**G**_ [¯] ) = +1. Therefore, there exists a bijective functon from states to elements of the group,
which can be implemented in dec.


22


**B.3** **Regular Languages**

This section details how Gated DeltaProduct networks can recognize any regular language in a finite
number of layers. The core idea is to show that Gated DeltaProduct can simulate any Finite State
Automaton (FSA), since FSAs are the computational models that define regular languages. The
proof proceeds in two main steps: First, we leverage the Krohn-Rhodes theorem, a fundamental
result in automata theory, which states that any FSA can be decomposed into a cascade of simpler
FSAs known as permutation-reset automata. These simpler automata only perform two types of
operations: permuting their states or resetting all states to a single state. Second, we demonstrate in
Lemma 4 that Gated DeltaProduct is well-equipped to simulate these permutation-reset automata.
The “DeltaProduct” mechanism, using products of Householder transformations, naturally handles
permutations, while the “Gated” aspect allows for the reset operations by nullifying the previous
state’s influence and setting a new one. By simulating these building blocks and cascading them,
Gated DeltaProduct can thus simulate any FSA and, consequently, recognize any regular language.

**Definition 1** (Finite state automaton (FSA)) **.** _A finite state automaton (FSA) is a tuple_
(Σ _, Q, q_ 0 _, δ, F_ ) _, where_ Σ _is a finite set called alphabet, Q is the finite set of states, q_ 0 _∈_ _Q is_
_the initial state, for every w ∈_ Σ _, δw_ : _Q →_ _Q is a state transition function and F ⊂_ _Q is the set of_
_accepting states._
**Definition 2** (Permutation-reset automaton) **.** _An FSA is permutation-reset if for every w ∈_ Σ _, δw is_
_either bijective or constant._

**Definition 3** (Regular language) **.** _A regular language is a set of sequences L such that there exists_
_an FSA that accepts it, i.e. such that L ⊂_ Σ _[∗]_ _, where_ Σ _[∗]_ _is the set of sequences with elements in_ Σ _,_
_and that for every word_ _**w**_ = _w_ 1 _w_ 2 _. . . wt ∈_ Σ _[∗]_

_δ_ _**w**_ ( _q_ 0) := _δwt ◦_ _δwt−_ 1 _◦· · · ◦_ _δw_ 1( _q_ 0) _∈_ _F ⇐⇒_ _**w**_ _∈_ _L._ (4)


Notably, the computation of any FSA can be also done using only matrix and vector multiplications.
Indeed, if we let _Q_ = _{_ 1 _, . . ., n}_ (for simplicity), then we can map each state _q_ to the one hot vector
_**e**_ _q_ (element of the canonical basis of R _[n]_ ) and each transition _δw_ to the matrix _**M**_ _w ∈{_ 0 _,_ 1 _}_ _[n][×][n]_ with
element at row _q_ and column _q_ _[′]_ being 1 if and only if _δ_ ( _q_ _[′]_ ) = _q_ . This way, by setting _**r**_ _∈{_ 0 _,_ 1 _}_ _[n]_
such that _rq_ = 1 if _q ∈_ _F_ and _rq_ = 0 otherwise, we have that for every word _**w**_ = _w_ 1 _w_ 2 _. . . wn ∈_ Σ _[∗]_

_**r**_ _[⊤]_ _**M**_ _wt_ _**M**_ _wt−_ 1 _· · ·_ _**M**_ _w_ 1 _**e**_ _q_ 0 = 1 _⇐⇒_ _**w**_ _∈_ _L._ (5)

We observe that if _δw_ is bijective and changes _k_ states, then the corresponding _**M**_ _w_ is a permutation
matrix that can be written as a product of _k −_ 1 Householder matrices, each corresponding to a swap
of two elements. Moreover, if _δw_ is a reset (constant), i.e. if _δw_ ( _q_ ) = ¯ _q_ for every _q ∈_ _Q_, then
_**M**_ _w_ _**e**_ _q_ = _**e**_ _q_ ¯. As we will see, constant transitions can be modeled by setting the gate to zero. We
are now ready to state our main result.
**Theorem 5** (Restatement of Theorem 2) **.** _For any regular language L and any nh ∈_ N _, there exists_
_a Gated DeltaProduct model with a finite number of layers that recognizes the language, i.e., for_
_every word_ _**w**_ _∈_ Σ _[∗]_ _outputs_ 1 _if_ _**w**_ _∈_ _L and_ 0 _otherwise._


_Proof._ Using the landmark theorem by Krohn and Rhodes [50] we can decompose the FSA corresponding to the regular language _L_ into a cascade of permutation-reset FSA. We can use a group
of at most 4 consecutive layers to represent each automaton in the cascade via Lemma 4. Then, we
can combine the different FSA in the cascade in a feedforward manner using the same construction
as the one in the proof of [16, Theorem 3], where the input of each FSA is the output concatenated
with the input of the previous FSA in the cascade.


**Lemma 4.** _For any permutation-reset FSA with |Q|_ = _n and |_ Σ _|_ = _s, where each bijective state-_
_transition function δw changes at most k states, there exists a Gated DeltaProduct model with the_
_following configuration that can implement it, i.e., for any word_ _**w**_ = _w_ 1 _, . . ., wt ∈_ Σ _in input, it_
_can output the corresponding sequence of states q_ 1 _, . . ., qt of the FSA. (i) one layer with nh_ = _k_ _−_ 1 _._
_(ii) 3 layers with nh >_ 1 _. (iii) 4 layers with nh_ = 1 _. The construction for (ii) and (iii) requires that_
_the MLP at the second last layer computes a lookup-table of size_ 2 _m × s_ [2] _[m]_ _, function of the last_ 2 _m_
_input tokens and the position modulo_ 2 _m with m_ = _⌈_ ( _k −_ 1) _/nh⌉._


_Proof._ We use the matrix vector multiply construction to implement the FSA. For every time-step _i_
we set _xi_ = _wi_ as the input to the model. The proof follows a path similar to the one of Theorem 3


23


(where more details are provided), where in addition to modeling permutations, the state-transition
matrix uses the gate to model constant transitions.


**(i).** Set _**H**_ 0 = _**e**_ _q_ 0. When _δwi_ is bijective, by assumption it changes at most _k_ states. Thus, by
setting _nh_ = _k −_ 1 we can represent the corresponding _**M**_ _wi_ matrix using _**A**_ ( _wi_ ) with gate _gi_ = 1
(product of _k −_ 1 generalized Householder matrices), and thus we set _**B**_ ( _wi_ ) = 0. If instead _δwi_ is
constant, i.e., _δi_ ( _q_ ) = ¯ _q_ and _**M**_ _w_ _**e**_ _q_ = _**e**_ _q_ ¯ for every _q ∈_ _Q_, then we can set the gate _gi_ = 0 so that
_**A**_ ( _wi_ ) = 0 and _**B**_ ( _wi_ ) = _**k**_ _i_ = _**e**_ _q_ ¯. Finally, we set dec( _**H**_ _i, xi_ ) = _**H**_ _i_ _[⊤]_ [(1] _[, . . ., n]_ [)] _[⊤]_ [to retrieve the]
correct state at step _i_ (for simplicity _Q_ = _{_ 1 _, . . ., n}_ ).


**(ii) and (iii).** If _nh < k −_ 1, then the state transition matrix is not sufficiently expressive to represent
all permutations of _k_ elements. However, we can use additional layers to overcome this issue.

We factorize the position _i ∈{_ 1 _, . . . t}_ into _i_ = _lm_ + [˜] _i_ for integers _l ≥_ 0 and [˜] _i ∈{_ 1 _, . . ., m}_ .
First, consider the case when _l ≥_ 1. The product _**M**_ ˜ _l_ = _**M**_ _w_ ( _l−_ 1) _m_ +1 _. . ._ _**M**_ _w_ ( _l−_ 1) _m_ + _m_ is either
a permutation matrix of _k_ elements or, if for some _i δwi_ is constant, then there exists ¯ _q_ such that
_**M**_ ˜ _l_ _**e**_ _q_ = ¯ _q_ for every _q ∈_ _Q_ . Therefore, we factor _**M**_ ˜ _l_ into _**M**_ ˜ _l_ = _**G**_ _l,m · · ·_ _**G**_ _l,_ 1 where each of
_**G**_ _l,_ 1 _, . . .,_ _**G**_ _l,m_ is either a product of _nh_ generalized Householder matrices or _**G**_ _l,i_ _**e**_ _q_ = _**e**_ _q_ ¯ for
every _q ∈_ _Q_, which can be modeled setting the gate to zero as in point (i). We fix one factorization
for each possible permutation matrix. In the last layer and with enough information in its input _**x**_ _i_
about past tokens, we can thus set _**H**_ 0 = _**e**_ 0 and

_**H**_ _i_ = _**G**_ _l,_ ˜ _i_ _**H**_ _i−_ 1 _,_ dec( _**H**_ _i,_ _**x**_ _i_ ) = ( _**M**_ _lm_ +˜ _i · · ·_ _**M**_ _lm_ +1 _**G**_ _l,m · · ·_ _**G**_ _l,_ ˜ _i_ +1 _**H**_ _i_ ) _[⊤]_ (1 _, . . ., n_ ) _[⊤]_


The case when _l_ = 0 is handled by setting _**M**_ [˜] 0 = _**G**_ 0 _,m · · ·_ _**G**_ 0 _,_ 1 with _**G**_ 0 _,i_ = _I_ . Note that both
_**M**_ _l,_ ˜ _i_ and dec( _**H**_ _t,_ _**x**_ _t_ ) are functions of [˜] _i_ = _i_ mod _m_ and the last _m_ + [˜] _i_ (in general the last 2 _m_ )
tokens. Hence, the layers before the last are dedicated to output at each time-step _i_ a lookup table
for the possible values of ( _i_ mod 2 _m, wi, . . ., wi−_ 2 _m_ +1). The first layers (2 if _nh_ = 1, 1 if _nh >_ 1)
can provide _i_ mod 2 _m_ by using Lemma 2 with _d_ = 2 _m_ . Finally, the second last layer can output
any function of the last 2 _m_ tokens and the position modulo 2 _m_ through Lemma 3 with _d_ = 2 _m_ and
_at_ = _wt_, by using _i_ mod 2 _m_ from the first layer(s).


**B.4** **Regular language recognition through products of RWKV-7 matrices**


To enhance the expressivity at the cost of stability, we can replace the product of Householder
matrices of DeltaProduct with a product of RWKV-7 matrices, i.e. for each layer set



_**A**_ ( _**x**_ _i_ ) =



_nh_
�(diag( _**w**_ _i,j_ ) _−_ _c_ _**k**_ _i,j_ ( _**k**_ _i,j ⊙_ _**a**_ _i,j_ )) _,_ (6)

_j_ =1



where _**w**_ _i,j_, _**a**_ _i,j_, _**k**_ _i,j_ are computed from _**x**_ _i_ such that _**a**_ _i,j,_ _**w**_ _i,j ∈_ [0 _,_ 1] _[n]_, _||_ _**k**_ _i,j||_ = 1 and
_c ∈{_ 1 _,_ 2 _}_ . Even without using gates, the resulting model will be capable of recognizing regular languages effectively as the following theorem shows.


**Theorem 6.** _For any_ _**regular language**_ _recognized by a finite-state automaton (FSA) with n ∈_ N
_states, there exists a linear RNN using products of RWKV-7 matrices as state-transition matrices (as_
_in (6) with c_ = 2 _) with one of the following configurations that can recognize it: (i) one layer with_
_nh_ = _n (ii) 3 layers with nh >_ 1 _(iii) 4 layers with nh_ = 1 _[13, Theorem 3]. The construction for (ii)_
_and (iii) requires that the MLP at the second last layer computes a lookup-table of size_ 2 _m×_ ( _n_ !) [2] _[m]_ _,_
_function of the last_ 2 _m input tokens and the position modulo_ 2 _m with m_ = _⌈n/nh⌉._


_Proof._ The computation of an FSA with _n_ states can be done using matrix-vector multiplications as
shown in (4), where the _n × n_ state transition matrices _**M**_ _wt_ have elements in _{_ 0 _,_ 1 _}_ and a single
one in each column. Peng et al. [13, Lemma 3] prove that any of those matrices can be expressed
as products of _n_ matrices, each of which is either a swap (identity with two columns swapped),
copy (identity with one row copied onto another), or the identity matrix and can be modeled by a
single RWKV-7 matrix. The proof for (ii) and (iii) follows similarly to Theorem 1 but now tackling
all state transition matrices, while Theorem 1 could handle only permutations since a generalized
Householders matrix cannot be a copy matrix.


24


**B.5** **Dihedral Groups**

In Grazzi et al. [16, Theorem 6] it is shown that with 2 layers and the extended eigenvalue range,
DeltaNet can compute addition modulo _m_, which corresponds to solving the group word problem
for the cyclic group Z _m_, for any _m ∈_ N. We extend this result and prove that, under identical assumptions, DeltaNet (DeltaProduct with _nh_ = 1) can solve the group word problem for the dihedral
group _Dm_, for any _m ∈_ N. The dihedral group _Dm_ represents the symmetries (both rotations and
reflections) of a regular _m_ -sided polygon. As a notable example, _D_ 3 is isomorphic to the symmetric
group _S_ 3.


The linear RNN construction used in this result can be implemented using a 2-layer DeltaNet Model
with two heads in the first layer. In the first layer, the linear RNN will compute parity for rotations
and reflections separately, i.e. it will record if the number of past rotations (reflections) is even or
odd. The recurrent state of the second layer will have 2 _m_ possible values (same as the order of
_Dm_ ) and each will be decoded differently based on the parity of reflections. The parity of rotations,
combined with the group element, determines which reflection matrix to use as the state transition
matrix of the second layer.

**Theorem 7** (Dihedral group word problems with reflections) **.** _For any m ∈_ _N_ _, consider the group_
_word problem of the dihedral group Dm. There exist DeltaProduct models with the following con-_
_figurations that can solve it. (ii) Two layers with nh_ = 1 _and at least two heads in the first layer and_
_one in the second layer. (ii) One layer with nh ≥_ 2 _._


_Proof._ The elements of the dihedral group _Dm_ can be divided into _m_ rotations _R_ = _{r_ 0 _, . . ., rm−_ 1 _}_
and _m_ reflections _S_ = _{s_ 0 _, . . ., sm−_ 1 _}_ . The identity is _r_ 0. To be able to solve the corresponding
word problem, we would like to map sequences of group elements _x_ 1 _, . . ., xt_ with _xi ∈R ∪S_ into
sequences _y_ 1 _, . . ., yt_ with _yi_ = _xi · xi−_ 1 _· · · x_ 1 and _·_ is the group operation, that for dihedral groups
is defined as


_ri · rj_ = _ri_ + _j_ mod _m,_ _ri · sj_ = _si_ + _j_ mod _m,_ _si · rj_ = _si−j_ mod _m,_ _si · sj_ = _ri−j_ mod _m._ (7)


Note that a product of two rotations is commutative, while the product of two reflections or a reflection with a rotation is not. Indeed for _m ≥_ 3 _Dm_, is not an abelian group.

**(i)** The constructions of the two layers of DeltaProduct with _nh_ = 1 builds upon the one for the
cyclic group _Zm_ outlined in [16, Theorem 6]. The first layer functions as a pre-processor, calculating
auxiliary information from the input sequence. Specifically, for each time step _t_, it determines two
parities: the parity of the total number of reflections and the parity of the total number of rotations
in the sequence _x_ 1 _, . . ., xt_ . This information is then passed to the second layer.


The second layer is responsible for computing the cumulative group product. It uses a 2D hidden
state to geometrically model the group elements and their compositions. The core challenge lies
in modeling the group operations, i.e. rotations and reflections, using only a reflection as statetransition matrices (rotation matrices cannot be represented with _nh_ = 1). To address this, the
state representation in the second layer must encode more than just the previous group product. It
is designed to also incorporate the rotation parity computed by the first layer. This is achieved by
maintaining two distinct sets of _m_ state vectors each and two distinct sets of 2 _m_ reflection matrices
each to represent the group elements. The choice of which set to use is determined by the rotation
parity. Moreover, the reflection parity is also used but only in the decoder. This design allows a
single, unified update mechanism, based solely on geometric reflections, to correctly implement all
four of the distinct multiplication rules defined in (7).


We define rotation by and reflection matrices as



�cos( _α_ ) _−_ sin( _α_ )
Rotation: _**R**_ ( _α_ ) =
sin( _α_ ) cos( _α_ )




- �cos( _α_ ) sin( _α_ )
Reflection: _**H**_ ( _α_ ) =
sin( _α_ ) _−_ cos( _α_ )




_,_ (8)



where _**R**_ ( _α_ ) is a rotation by an angle of _α_, while _**H**_ ( _α_ ) is a reflection by a line having an angle of
_α/_ 2 with the line passing from the origin and the point (1 _,_ 0). Note that both _**R**_ and _**H**_ are periodic
with period 2 _π_ . Moreover, let _α, γ ∈_ R, the following are standard identities of products of matrix
representations of 2D rotations and reflections.


_**R**_ ( _α_ ) _**R**_ ( _γ_ ) = _**R**_ ( _α_ + _γ_ ) _,_ _**H**_ ( _α_ ) _**H**_ ( _γ_ ) = _**R**_ ( _α −_ _γ_ ) _,_
(9)
_**R**_ ( _α_ ) _**H**_ ( _γ_ ) _,_ = _**H**_ ( _α_ + _γ_ ) _**H**_ ( _γ_ ) _**R**_ ( _α_ ) = _**H**_ ( _γ −_ _α_ ) _._


25


For the first layer we use the following diagonal recurrence which indicates in the first (second)
coordinate whether the number of rotations (reflections) is even (0) or odd (1).

_**h**_ [(1)] 0 = 0 _,_ _**h**_ [(1)] _t_ = _**a**_ ( _xt_ ) _⊙_ _**h**_ [(1)] _t−_ 1 [+] _**[ b]**_ [(] _[x][t]_ [)] _[,]_ _**y**_ _t_ [(1)] = dec [(1)] ( _**h**_ _t, xt_ ) = ( _xt, ht,_ 1 _, ht,_ 2) _._

        - _−_ 1 if _xi ∈R_         - _−_ 1 if _xi ∈S_
_**a**_ ( _xi_ )1 = _**a**_ ( _xi_ )2 =
1 if _xi ∈S_ 1 if _xi ∈R_

�1 if _xi ∈R_ �1 if _xi ∈S_
_**b**_ ( _xi_ )1 = _**b**_ ( _xi_ )2 =
0 if _xi ∈S_ 0 if _xi ∈R_


This recurrence can be implemented also by DeltaProduct with _nh_ = 1 using 2 heads each with
scalar hidden states: one for the rotations and the other for the reflections. For the second layer, we
have instead the following constructions, which selects the appropriate reflection based on the parity
of the rotations and uses the parity of the reflections for dec.

_**h**_ [(2)] 0 = (1 _,_ 0) _[⊤]_ _,_ _**h**_ [(2)] _t_ = _**A**_ [(2)] ( _**y**_ _t_ [(1)] ) _**h**_ [(2)] _t−_ 1 _[,]_ _**y**_ _t_ [(2)] = dec [(2)] ( _**h**_ [(2)] _t_ _[,]_ _**[ y]**_ _t_ [(1)] ) _,_

_**A**_ [(2)] ( _**y**_ ) = _**H**_ ( _θ_ ( _y_ 1 _, y_ 2)) _,_

       - _ri∗_ if _y_ 3 = 0
dec [(2)] ( _**h**_ _,_ _**y**_ ) = _sm−i∗_ if _y_ 3 = 1 _[,]_ _i_ _[∗]_ = _i∈{_ arg max0 _,...,m−_ 1 _}_ max( _**c**_ _[⊤]_ _i_ _**[h]**_ _[,]_ _**[ d]**_ _i_ _[⊤]_ _**[h]**_ [)]


where _**y**_ = ( _y_ 1 _, y_ 2 _, y_ 3) _[⊤]_ _∈R ∪S × {_ 0 _,_ 1 _} × {_ 0 _,_ 1 _}_ and _θ_ : _R ∪S × {_ 0 _,_ 1 _} →_ R determines the
angle of the reflection and is defined for all _i ∈{_ 0 _, . . ., m −_ 1 _}_ as



_θ_ ( _ri,_ 1) = [(1] _[ −]_ [2] _[i]_ [)] _[π]_




_[i]_ [)] _[π]_ _,_ _θ_ ( _si,_ 1) = _[−]_ [2] _[iπ]_

_m_ _m_




[2] _[i]_ [)] _[π]_

_,_ _θ_ ( _ri,_ 0) = [(1 + 2] _[i]_ [)] _[π]_
_m_ _m_




[2] _[iπ]_

_θ_ ( _si,_ 0) = [(2 + 2] _[i]_ [)] _[π]_
_m_ _[,]_ _m_



_._
_m_



Moreover, _C_ = _{_ _**c**_ 0 _, . . .,_ _**c**_ _m−_ 1 _}_ and _D_ = _{_ _**d**_ 0 _, . . .,_ _**d**_ _m−_ 1 _}_ are two sets of states and are defined as

_**d**_ 0 = _**h**_ [(2)] 0 = (1 _,_ 0) _[⊤]_ _,_ _**c**_ 0 = _**H**_ ( _π/m_ ) _**d**_ 0 _,_
_**d**_ _i_ = _**R**_ (2 _iπ/m_ ) _**d**_ 0 _,_ _**c**_ _i_ = _**R**_ ( _−_ 2 _iπ/m_ ) _**c**_ 0 for all _i ∈{_ 0 _, . . ., m −_ 1 _}._


From our choice of _**d**_ 0 = (1 _,_ 0) _[⊤]_ and _**c**_ 0 and from (8)-(9), for any _α ∈_ R we have

_**R**_ ( _α_ ) _**d**_ 0 = _**H**_ ( _α_ ) _**d**_ 0 _,_ and
_**R**_ ( _α_ ) _**c**_ 0 = _**R**_ ( _α_ ) _**H**_ ( _π/m_ ) _**d**_ 0 = _**R**_ ( _α_ ) _**R**_ ( _π/m_ ) _**d**_ 0 = _**R**_ ( _α_ + _π/m_ + _π/m −_ _π/m_ ) _**d**_ 0
= _**H**_ ( _α_ + 2 _π/m_ ) _**H**_ ( _π/m_ ) _**d**_ 0 = _**H**_ ( _α_ + 2 _π/m_ ) _**c**_ 0 _._

Moreover, from our choice of _θ_, _**d**_ _i_ and _**c**_ _i_, using the identities above and the the fact that _**R**_ is a
periodic function with period 2 _π_ we have that


_**d**_ _i_ = _**R**_ (2 _iπ/m_ ) _**d**_ 0 = _**R**_ (2 _iπ/m_ ) _**H**_ ( _π/m_ ) _**c**_ 0 = _**H**_ ( _θ_ ( _ri,_ 0)) _**c**_ 0
_**c**_ _i_ = _**R**_ ( _−_ 2 _iπ/m_ ) _**c**_ 0 = _**R**_ ( _−_ 2 _iπ/m_ ) _**H**_ ( _π/m_ ) _**d**_ 0 = _**H**_ ( _θ_ ( _ri,_ 1)) _**d**_ 0
_**d**_ _m−i_ = _**R**_ ( _−_ 2 _iπ/m_ ) _**d**_ 0 = _**H**_ ( _−_ 2 _iπ/m_ ) _**d**_ 0 = _**H**_ ( _θ_ ( _si,_ 1)) _**d**_ 0
_**c**_ _m−i_ = _**R**_ (+2 _iπ/m_ ) _**c**_ 0 = _**H**_ ((2 + 2 _i_ ) _π/m_ ) _**c**_ 0 = _**H**_ ( _θ_ ( _si,_ 0)) _**c**_ 0
for every _i ∈{_ 0 _, . . ., m −_ 1 _}_ . Therefore, we can write



_**H**_ ( _θ_ ( _rj,_ 1)) _**d**_ _i_ = _**R**_ ( _θ_ ( _rj,_ 1) _−_ _θ_ ( _ri,_ 0)) _**c**_ 0 = _**R**_ ( _−_ 2( _i_ + _j_ ) _π/m_ ) _**c**_ 0 = _**c**_ _i_ + _j_ mod _m,_
_**H**_ ( _θ_ ( _rj,_ 0)) _**c**_ _i_ = _**R**_ ( _θ_ ( _rj,_ 0) _−_ _θ_ ( _ri,_ 1)) _**d**_ 0 = _**R**_ (2( _i_ + _j_ ) _π/m_ ) _**d**_ 0 = _**d**_ _i_ + _j_ mod _m,_
_**H**_ ( _θ_ ( _sj,_ 1)) _**d**_ _i_ = _**R**_ ( _θ_ ( _sj,_ 1) _−_ _θ_ ( _sm−i,_ 1)) _**d**_ 0 = _**R**_ ( _−_ 2( _i_ + _j_ ) _π/m_ ) _**d**_ 0 = _**d**_ _−i−j_ mod _m,_
_**H**_ ( _θ_ ( _sj,_ 0)) _**c**_ _i_ = _**R**_ ( _θ_ ( _sj,_ 0) _−_ _θ_ ( _sm−i,_ 0)) _**c**_ 0 = _**R**_ (2( _i_ + _j_ ) _π/m_ ) _**c**_ 0 = _**c**_ _−i−j_ mod _m,_



(10)



for every _i, j ∈{_ 0 _, . . ., m−_ 1 _}_ . We proceed to verify that the output of the second layer is computed
correctly: satisfying the product rule for the dihedral group in (7), i.e., we want to verify that



(11)



_yt_ [(2)] =








_ri_ + _j_ mod _m_ if _yt_ [(2)] _−_ 1 [=] _[ r][i][, x][t]_ [ =] _[ r][j]_
_si_ + _j_ mod _m_ if _yt_ [(2)] _−_ 1 [=] _[ r][i][, x][t]_ [ =] _[ s][j]_
_si−j_ mod _m_ if _yt_ [(2)] _−_ 1 [=] _[ s][i][, x][t]_ [ =] _[ r][j]_
_ri−j_ mod _m_ if _yt_ [(2)] _−_ 1 [=] _[ s][i][, x][t]_ [ =] _[ s][j]_







26


Where we set _y_ 0 [(2)] = _r_ 0. First note that when _yt_ [(2)] _∈S_, then _yt,_ [(1)] 3 [= 1][ and when] _[ y]_ _t_ [(2)] _∈R_, then

_yt,_ [(1)] 3 [= 0][. We consider two cases.]

**Case 1.** If _yt_ [(2)] _−_ 1 [=] _[ r][i]_ [ and hence] _[ y]_ _t_ [(1)] _−_ 1 _,_ 3 [= 0][, then using (10) we obtain]



_**h**_ [(2)] _t_ = _**A**_ [(2)] ( _**y**_ [(1)] ) _**h**_ [(2)] _t−_ 1 [=]








_**H**_ ( _θ_ ( _rj,_ 1)) _**d**_ _i_ = _**c**_ _i_ + _j_ mod _m_ if _xt_ = _rj, yt,_ [(1)] 2 [= 1]
_**H**_ ( _θ_ ( _rj,_ 0)) _**c**_ _i_ = _**d**_ _i_ + _j_ mod _m_ if _xt_ = _rj, yt,_ [(1)] 2 [= 0]
_**H**_ ( _θ_ ( _sj,_ 1)) _**d**_ _i_ = _**d**_ _−i−j_ mod _m_ if _xt_ = _sj, yt,_ [(1)] 2 [= 1]
_**H**_ ( _θ_ ( _sj,_ 0)) _**c**_ _i_ = _**c**_ _−i−j_ mod _m_ if _xt_ = _sj, yt,_ [(1)] 2 [= 0]







This, together with the definition of dec [(2)] implies that



_yt_ [(2)] = dec [(2)] ( _**h**_ [(2)] _t_ _[,]_ _**[ y]**_ _t_ [(1)] ) =




_ri_ + _j_ mod _m_ if _xt_ = _rj, yt,_ [(1)] 3 [= 0]
(12)
_si_ + _j_ mod _m_ if _xt_ = _sj, yt,_ [(1)] 3 [= 1]



**Case 2.** If instead _yt_ [(2)] _−_ 1 [=] _[ s][i]_ [ and hence] _[ y]_ _t_ [(1)] _−_ 1 _,_ 3 [= 1][, then using (10) we obtain]



_**h**_ [(2)] _t_ = _**A**_ [(2)] ( _**y**_ [(1)] ) _**h**_ [(2)] _t−_ 1 [=]








_**H**_ ( _θ_ ( _rj,_ 1)) _**d**_ _m−i_ = _**c**_ _j−i_ mod _m_ if _xt_ = _rj, yt,_ [(1)] 2 [= 1]
_**H**_ ( _θ_ ( _rj,_ 0)) _**c**_ _m−i_ = _**d**_ _j−i_ mod _m_ if _xt_ = _rj, yt,_ [(1)] 2 [= 0]
_**H**_ ( _θ_ ( _sj,_ 1)) _**d**_ _m−i_ = _**d**_ _i−j_ mod _m_ if _xt_ = _sj, yt,_ [(1)] 2 [= 1]
_**H**_ ( _θ_ ( _sj,_ 0)) _**c**_ _m−i_ = _**c**_ _i−j_ mod _m_ if _xt_ = _sj, yt,_ [(1)] 2 [= 0]







This, together with the definition of dec [(2)] implies that



_yt_ [(2)] = dec [(2)] ( _**h**_ [(2)] _t_ _[,]_ _**[ y]**_ _t_ [(1)] ) =




_si−j_ mod _m_ if _xt_ = _rj, yt,_ [(1)] 3 [= 1]
(13)
_ri−j_ mod _m_ if _xt_ = _sj, yt,_ [(1)] 3 [= 0] _[ .]_



Note that (12) and (13) imply (11). Setting the output of the linear RNN equal to the output of the
second layer concludes the proof.


**(ii)** It follows from Theorem 4 since _Dm_ is a finite subgroup of O(2), the group of 2D orthogonal
transformations: rotations and reflections.


**B.6** **Stability vs. Expressivity of Linear RNNs**


In this section, we discuss the tradeoff between expressivity and stability of a linear RNN recurrence
_**H**_ _i_ = _**A**_ _i_ _**H**_ _i−_ 1 + _**B**_ _i_, where _**A**_ _i_ = _**A**_ ( _**x**_ _i_ ), _**B**_ _i_ = _**B**_ ( _**x**_ _i_ ). We say that such a recurrence is stable if



_∃M ∈_ [0 _, ∞_ ) such that



������



_i_




_**A**_ _j_
_j_ =1



_< M_ _∀i ∈_ N _,_ (14)
������



where _∥·∥_ is the spectral norm. This property is true if and only if _ρ_ ( [�] _[i]_ _j_ =1 _**[A]**_ _[j]_ [)] _[ ≤]_ [1][ where] _[ ρ]_ [(] _**[M]**_ [)][ is]
the spectral radius of _**M**_, i.e. the maximum modulus of its eigenvalues. When this property is not
satisfied, the norm of the state will diverge. An effective way to satisfy (14) with _M_ = 1 is to enforce
_∥_ _**A**_ _i∥≤_ 1 for every _i_, since the norm of the product is less than or equal to the product of the norms
(due to the submultiplicativity property). However, this restriction excludes some boolean matrices
which are useful for recognizing regular languages. Indeed, in the construction shown in (4), all
matrices involved are _n × n_ with entries taking values in _{_ 0 _,_ 1 _}_ and having only a single one in each
column. This class of matrices _B_ satisfies (14) with _M_ = _[√]_ ~~_n_~~ because it is closed under matrix
multiplication, i.e. _∀_ _**B**_ _,_ _**B**_ _[′]_ _∈B_, we have _**BB**_ _[′]_ _∈B_, and max _**B**_ _∈B ∥_ _**B**_ _∥_ = _[√]_ ~~_n_~~ ~~,~~ which is achieved
by matrices with ones only in one row. In particular, all matrices in _B_ that are not permutations have
spectral norm greater than one and therefore cannot be expressed if we enforce _∥_ _**A**_ _i∥≤_ 1.

The (Gated) DeltaProduct state transition matrix _**A**_ _i_ = [�] _l_ _[n]_ =1 _[h]_ _[g][l]_ [(] _[I][ −]_ _[β][l]_ _**[k]**_ _[l]_ _**[k]**_ _l_ _[⊤]_ [)][ satisfies] _[ ∥]_ _**[A]**_ _[i][∥≤]_ [1]
since _gl ∈_ [0 _,_ 1], _βl ∈_ [0 _,_ 2], and _∥_ _**k**_ _l∥_ = 1. Thus, from the matrices in _B_, it can represent only
permutations of up to _nh_ + 1 elements. Instead, the state-transition matrix of RWKV-7, _**A**_ _i_ =
diag( _wi_ ) _−_ _c_ _**k**_ _i_ ( _**k**_ _i ⊙_ _**a**_ _i_ ) _[⊤]_ with _c_ = 2, can represent not only the identity and permutations of two


27


elements, but also any copy matrix, which is obtained by copying one column of the identity onto
_√_
another and has spectral norm equal to 2. However, as we show in the next theorem, even with the

less expressive _c_ = 1 setup that is used in practice, the RWKV-7 recurrence is not stable unless _**a**_ _i_
is the same for every _i_, which is the case studied in Peng et al. [13, Theorem 1]. Having different
_**a**_ _i_ values is key to modeling the copy matrix, since this requires a value different from that of a
permutation matrix.

**Theorem 8.** _Consider the RWKV-7 state transition matrix_ _**A**_ _i_ = diag( _wi_ ) _−_ _c_ _**k**_ _i_ ( _**k**_ _i ⊙_ _**a**_ _i_ ) _[⊤]_ _with_
_c_ = 1 _(as set in practice), wi_ = (1 _, . . .,_ 1) _[⊤]_ _∈_ R _[n]_ _,_ _**a**_ _i ∈_ [0 _,_ 1] _[n]_ _,_ _**k**_ _i ∈_ R _[n]_ _with ∥_ _**k**_ _i∥_ = 1 _, and_
_n ≥_ 2 _. There exists an infinite set M of matrix pairs such that for every_ ( _**A**_ _,_ _**A**_ _[′]_ ) _∈M, we have_
_ρ_ ( _**AA**_ _[′]_ ) _>_ 1 _._ 2 _, where ρ denotes the spectral radius. Thus, if we set_




  - _**A**_ _if i_ mod 2 = 0
_**A**_ _i_ = _which implies_ lim
_**A**_ _[′]_ _if i_ mod 2 = 1 _[,]_ _i→∞_



������



_i_




_**A**_ _j_
_j_ =1



= lim
_i→∞_ [=] _[ ∞][.]_
������



_Proof._ We demonstrate this by construction for _n_ = 2; the generalization to _n ≥_ 2 is straightforward.



_√_
Let _θ_ = _π/_ 3, _**a**_ = (0 _,_ 1) _[⊤]_, _**a**_ _[′]_ = (1 _,_ 0) _[⊤]_ . _**k**_ = (cos _θ,_ sin _θ_ ) _[⊤]_ = [1 _/_ 2 _,_



Let _θ_ = _π/_ 3, _**a**_ = (0 _,_ 1) _[⊤]_, _**a**_ _[′]_ = (1 _,_ 0) _[⊤]_ . _**k**_ = (cos _θ,_ sin _θ_ ) _[⊤]_ = [1 _/_ 2 _,_ 3 _/_ 2] _[⊤]_ . _**k**_ _[′]_ =

_√_
(sin _θ,_ cos _θ_ ) _[⊤]_ = ( 3 _/_ 2 _,_ 1 _/_ 2) _[⊤]_ . Note that _∥_ _**k**_ _∥_ = _∥_ _**k**_ _[′]_ _∥_ = 1. We construct _**A**_ and _**A**_ _[′]_ as



3 _/_ 2 _,_ 1 _/_ 2) _[⊤]_ . Note that _∥_ _**k**_ _∥_ = _∥_ _**k**_ _[′]_ _∥_ = 1. We construct _**A**_ and _**A**_ _[′]_ as



_√_

        - 3 _/_ 2
_**A**_ = _I −_ _**k**_ ( _**k**_ _⊙_ _**a**_ ) _[⊤]_ = _I −_

1 _/_ 2



_√_

- �1 _−_ 3 _/_ 4

[0 1 _/_ 2] =

0 3 _/_ 4



_√_

- �1 _−_

[0 1 _/_ 2] =








         - 1 _/_ 2
_**A**_ _[′]_ = _I −_ _**k**_ _[′]_ ( _**k**_ _[′]_ _⊙_ _**a**_ _[′]_ ) _[⊤]_ = _I −_ _√_

3 _/_ 2




- - 3 _/_ 4 0

[1 _/_ 2 0] = _√_

_−_ 3 _/_ 4 1



3 _/_ 4 1







Now, consider the product matrix

_√_
�1 _−_ 3 _/_ 4
_**M**_ = _**AA**_ _[′]_ =

0 3 _/_ 4



3 _/_ 4 1



_√_

- - 15 _/_ 16 _−_
= _√_



�� 3 _/_ 4 0
_√_

_−_ 3 _/_ 4 1







15 _/_ 16 _−_ 3 _/_ 4

_√_

_−_ 3 3 _/_ 16 3 _/_ 4



3 _/_ 16 3 _/_ 4



To find the spectral radius _ρ_ ( _M_ ), we examine its eigenvalues. The characteristic equation is _λ_ [2] _−_
Tr( _**M**_ ) _λ_ + det( _**M**_ ) = 0, with Tr( _**M**_ ) = 27 _/_ 16 and det( _**M**_ ) = 9 _/_ 16. Hence, the characteristic
~~_√_~~ ~~_√_~~
equation is 16 _λ_ [2] _−_ 27 _λ_ + 9 = 0. Thus, the eigenvalues are _λ_ 1 = [27+] 32153 and _λ_ 2 = [27] _[−]_ 32153 and



~~_√_~~ ~~_√_~~

32153 and _λ_ 2 = [27] _[−]_ 32



equation is 16 _λ_ [2] _−_ 27 _λ_ + 9 = 0. Thus, the eigenvalues are _λ_ 1 = [27+] 32153 and _λ_ 2 = [27] _[−]_ 32153 and

the spectral radius is _ρ_ ( _**M**_ ) = max _{|λ_ 1 _|, |λ_ 2 _|}_ = _λ_ 1 _≈_ 1 _._ 23. Also, since the spectral radius is a
continuous function of the matrix entries, which are a continuous function of _θ_, then this means that
there is an infinite set of matrices, namely _M_, obtained by varying _θ_ around _π/_ 3 whose product has
spectral radius greater than 1 _._ 2.

From our construction of _**A**_ _i_, we have [�] _j_ [2] =1 _[i]_ _**[A]**_ _[j]_ [ =] _**[ M]**_ _[ i]_ [. By the definition of the spectral norm and]
spectral radius, �� _**M**_ _i_ �� _≥_ �� _λi_ 1 _**[x]**_ �� = _|λ|i_ = _ρ_ ( _**M**_ ) _i_, where _**x**_ is the eigenvector associated with the
dominant eigenvalue _λ_ 1. The result follows since lim _i→∞_ _ρ_ ( _**M**_ ) _[i]_ = _∞_ .


**C** **Experiments**


**C.1** **State-Tracking**


**Clarification on the isomorphisms of** _S_ 3 **,** _S_ 4 **,** _A_ 5 **, and** _S_ 5

_S_ 3: The group consisting of all isometries that map an equilateral triangle onto itself, including both
orientation-preserving rotations and orientation-reversing reflections, is isomorphic to _S_ 3.


_S_ 4: The rotation group of a cube is isomorphic to the symmetric group _S_ 4. This correspondence
arises because the cube has exactly four space diagonals, and every _proper rotation_ —that is, every
orientation-preserving isometry of the cube about an axis through its center—permutes these diagonals in all possible ways (see Figure 6 for an example). In particular, these proper rotations include,
for example, the 90 _[◦]_, 180 _[◦]_, and 270 _[◦]_ rotations about axes passing through the centers of opposite
faces, the 180 _[◦]_ rotations about axes through the midpoints of opposite edges, and the 120 _[◦]_ /240 _[◦]_


28


|Col1|Col2|n<br>h|
|---|---|---|
||||
|||l|
||||


|Col1|n<br>h|
|---|---|
|||
||l<br><br><br>|
|||


|Col1|Col2|n<br>h|
|---|---|---|
||||
|||l<br><br><br>|
||||


|Col1|Col2|n<br>h|1<br>2|
|---|---|---|---|
||||3<br>4|
|||l|1<br>2<br>|
||||~~4~~<br>6<br>8<br>10|



Figure 12: Results for permutation groups _S_ 3, _S_ 4, _A_ 5, and _S_ 5 when limiting the eigenvalue
range of the state-transition matrix to [0 _,_ 1]. _(Top row)_ Varying the number of Householder products _nh_ for a single layer DeltaProduct _nh_ [0 _,_ 1]. _(Bottom row)_ Varying the number of layers _l_ of
DeltaProduct1[0 _,_ 1]/DeltaNet[0 _,_ 1] (single Householder). Dashed vertical line at training context
length 128. Higher _nh_ improves extrapolation to longer sequences of permutations, e.g., _S_ 3 can be
learned with _nh_ = 2 with a single layer while three layers are required when keeping _nh_ = 1.


rotations about axes through opposite vertices. Hence, the proper rotational symmetries of the cube
correspond precisely to the permutations of its four space diagonals [69].


_A_ 5: Similarly, a regular dodecahedron contains exactly five special cubes symmetrically arranged
within it. Each _proper rotation_ of the dodecahedron—that is, every orientation-preserving rigid
motion mapping the dodecahedron onto itself—rearranges these inscribed cubes by an _even permu-_
_tation_ . This property makes the rotation group of the dodecahedron isomorphic to the alternating
group _A_ 5, the group of all even permutations of five elements [70].


_S_ 5: When both proper rotations and reflections (orientation-reversing symmetries) are considered,
the full symmetry group of the dodecahedron corresponds exactly to the symmetric group _S_ 5, since
reflections allow both even and odd permutations of the five hidden cubes [70].


**Experimental Details.** We used the experimental setup from Merrill et al. [3] and sampled
2 _,_ 000 _,_ 000 training datapoints at sequence length 128 and 500 _,_ 000 test datapoints at sequence length
512. We did not use a curriculum over sequence length during training. The models were trained using AdamW optimizer [71] with parameters _β_ 1 = 0 _._ 9, _β_ 2 = 0 _._ 999, and _ϵ_ = 10 _[−]_ [8] in PyTorch [72].
We used a learning rate of 10 _[−]_ [3] with cosine annealing [73] and trained for 100 epochs with a batch
size of 1024, except for the _S_ 3 models which required a batch size of 2048 for more reliable results.
All models used a single-layer DeltaProduct architecture featuring 12 heads (more heads made the
results more reliable) and a head dimension of 32. We applied a weight decay coefficient of 10 _[−]_ [6] .
The _β_ values were extracted from the forward pass of the trained models using NNsight [74]. We
use the PCA implementation in scikit-learn [75].


**C.2** **Chomsky Hierarchy**


**Setup.** We conducted experiments on selected formal language tasks originally introduced by
Del´etang et al. [52]. Our goal was to demonstrate the improvements in length extrapolation that
can be achieved using multiple Householder matrices in the state-transition matrix compared to
DeltaNet. Following Grazzi et al. [16], we focus on three tasks: parity, modular arithmetic without
brackets (both regular languages), and modular arithmetic with brackets (a context-free language).
We trained DeltaProduct _nh_ with _nh ∈{_ 2 _,_ 3 _,_ 4 _}_ on sequences of length 3 to 40 and tested on sequences ranging from 40 to 256 to evaluate generalization to longer inputs. We compare our results
against the results obtained by Grazzi et al. [16] for Transformer, mLSTM and sLSTM from Beck
et al. [9], Mamba [6], and DeltaNet [10]. For both Mamba and DeltaNet, we experiment with an
eigenvalue range restricted to [0 _,_ 1] and extended to [ _−_ 1 _,_ 1].


29


2


0


2


0


2



Head 0 Head 1 Head 2 Head 3


Head 4 Head 5 Head 6 Head 7



Head 8



Head 10


0 2



Head 11


0 2



0
0 2



Head 9


0 2



0


Figure 13: _β_ 0 and _β_ 1 values across all 24 permutations in _S_ 4 in DeltaProduct2[ _−_ 1 _,_ 1]. We find that
only head 6 (shown in Figure 7) learns to use both Householders as reflections ( _β_ 0 _≈_ 2, _β_ 1 _≈_ 2)
allowing it to learn the rotations to solve _S_ 4.


**Experimental Details.** All DeltaProduct and DeltaNet models contain 3 layers with 1 head each
and heads’ dimensions set to 128, except for modular arithmetic with brackets, where we use 12
[heads and set the heads’ dimensions to 32. Both models use a causal depthwise 1D convolution](https://github.com/Dao-AILab/causal-conv1d)
with a kernel size of 4 after the query/key/value projection. For modular arithmetic, we also use a
gradient clipping norm of 1.0. We train each model using AdamW [71] using a learning rate of 5e-4,
batch size of 1024, 0.1 weight decay, and a cosine annealing learning rate schedule [73] (minimum
learning rate: 1e-6) after 10% warm-up steps. We train on the modular arithmetic and parity tasks
for 100k and 20k steps in total, respectively. At each training step, we make sure to generate a valid
random sample from the task at hand (see below). We repeat the runs 3 times with different seeds
each, and later pick the best to report in Table 2.


**Considered Tasks.** We empirically evaluated three tasks—parity, modular arithmetic without brackets, and modular arithmetic with brackets—spanning different levels of the Chomsky Hierarchy. Below, we provide details for each task, where _|_ Σ _|_ denotes the vocabulary size and _Accrand_ represents
the accuracy of random guessing:


- **Parity (** _|_ Σ _|_ = 2 **,** _Accrand_ = 0 _._ 5 **).** Given a binary sequence _**x**_ = _x_ 1 _. . . xt ∈{_ 0 _,_ 1 _}_ _[t]_, the parity
label _yt ∈{_ 0 _,_ 1 _}_ is 1 if the total number of ones in the sequence is odd, and 0 otherwise. This task
is equivalent to computing the sum of all previous values modulo 2, i.e., _yt_ = ( [�] _[t]_ _i_ =1 _[x][i]_ [) mod 2][.]

- **Modular Arithmetic without Brackets (** _|_ Σ _|_ = 10 **,** _Accrand_ = 1 _/_ 5 **).** Given a set of special
tokens Σ _s_ = _{_ + _, −, ∗,_ = _,_ [ `PAD` ] _}_ and a modulus _m ≥_ 1, we define Σ = Σ _s ∪{_ 0 _, . . ., m −_ 1 _}_ .
The label _yt_ corresponds to the result of evaluating the arithmetic operations in the sequence
_**x**_ = _x_ 1 _, . . ., xt_, computed modulo _m_ . In our experiments, we set _m_ = 5. An example is:


`2` + `1` _−_ `2` _∗_ `2` _−_ `3` = `1` [ `PAD` ]


- **Modular Arithmetic with Brackets (** _|_ Σ _|_ = 12 **,** _Accrand_ = 1 _/_ 5 **).** This task follows the same
definition as modular arithmetic without brackets but includes an extended set of special tokens,
Σ _s_ = _{_ + _, −, ∗,_ = _,_ ) _,_ ( _,_ [ `PAD` ] _}_, allowing for nested expressions. Again, we set _m_ = 5. An example
sequence is:


(( `1` _−_ ( _−_ `2` )) + (( `4` ) + `3` )) = `0` [ `PAD` ]


**Results.** As shown in Table 2, DeltaProduct _nh_ with _nh ≥_ 2 has better average accuracy compared
to DeltaNet and other baselines. This performance improvement is particularly pronounced when
using the extended eigenvalue range [ _−_ 1 _,_ 1], which aligns with the findings of Grazzi et al. [16].
Notably, we observe the most significant improvement in the modular arithmetic with brackets task,
which is also the most challenging.


30


Table 2: Performance of DeltaProduct _nh_ [ _−_ 1 _,_ 1], _nh ∈{_ 2 _,_ 3 _,_ 4 _}_, on formal language tasks. We
report the best of 3 runs. Scores are scaled accuracy, with 1.0 indicating perfect performance and
0.0 random guessing. The results for the other models were taken directly from Grazzi et al. [16].


**Mod. Arithm.** **Mod. Arithm.**
**Model** **Parity** **Avg.**
**(w/o brackets)** **(w/ brackets)**


Transformer 0.022 0.031 0.067 0.040


mLSTM 0.087 0.040 0.114 0.080
sLSTM **1.000** **0.787** **0.178** **0.655**


Mamba [0 _,_ 1] 0.000 0.095 **0.123** 0.073
Mamba [ _−_ 1 _,_ 1] **1.000** **0.241** 0.116 **0.452**


DeltaNet [0 _,_ 1] 0.233 0.302 0.253 0.263
DeltaProduct2 [0 _,_ 1] 0.264 **0.402** 0.249 0.305
DeltaProduct3 [0 _,_ 1] 0.285 **0.402** 0.288 **0.325**
DeltaProduct4 [0 _,_ 1] **0.295** 0.369 **0.288** 0.317


DeltaNet [ _−_ 1 _,_ 1] **0.982** **0.915** 0.281 0.726
DeltaProduct2 [ _−_ 1 _,_ 1] 0.896 0.887 0.329 0.704
DeltaProduct3 [ _−_ 1 _,_ 1] 0.932 0.736 0.330 0.666
DeltaProduct4 [ _−_ 1 _,_ 1] **0.982** 0.893 **0.342** **0.739**


**C.3** **Language Modeling**


**C.3.1** **Experimental setup**


We follow the same basic training setup as in [16]. We use the training pipeline `flame` from the
flash-linear-attention [22] repository. All of our models are trained on NVIDIA L40s, NVIDIA
A100 40GB or NVIDIA H100 94GB GPUs. We used 16 to 32 GPUs at a time to train one model,
in a 2 to 8 node setup, depending on resource availability. We used DeepSpeed with ZeRO-2 [76]
for distributed training. All models were trained with an effective batch size of 524 288 tokens, and
a learning rate of 3e-4. We optimized the models with AdamW [71] (0 _._ 01 weight decay) and used
cosine annealing [73] for the learning rate schedule with linear warm up for 512 steps. We used a
total of 10 500 GPU hours to train all of our models.


**C.3.2** **Throughput**







Figure 14: Training throughput of a parameter matched DeltaProduct 1.3B. Parameter matching is
achieved by decreasing the inner dimension in the SwiGLU MLP for _nh >_ 1.


**C.3.3** **Additional Benchmarks**


In Table 3 we report evaluations for the models in Figure 10 on tasks from lm-eval-harness [61]. In
addition, we also train and evaluate models with 2048 context length at the 340 _M_ parameter scale
and report the results in Table 5 and compare them with the results in [10] which are trained under a
comparable setup. We observe that DeltaProduct outperforms DeltaNet in terms of average accuracy
for both training setups.


**Tasks Details.** We use the lm-eval-harness benchmark [61] to assess model performance. Following
Yang et al. [10], the evaluation encompasses multiple task categories: **Language Understanding**
**Tasks.** The evaluation includes LAMBADA (LMB) [77] for testing text comprehension, PIQA

[78] for physical reasoning assessment, HellaSwag (Hella.) [79] for situational understanding, and
Winogrande (Wino.) [80] for commonsense reasoning evaluation. **Reasoning.** The ARC dataset
provides two distinct testing sets: ARC-easy (ARC-e) and ARC-challenge (ARC-c) [81], measuring
varying levels of scientific knowledge comprehension.


31


Table 3: Performance comparison of models shown in Figure 10. Parameter equivalence was
achieved by scaling the head dimension. To account for the increased parameter count we scaled the
training token budget from 19B (213M parameters) to 55B (805M parameters) on FineWeb [57].
Models were trained on 4096 token context length.

|Col1|Model|Wiki. LMB.<br>ppl ↓ ppl ↓|LMB. PIQA Hella. Wino. ARC-e ARC-c Avg.<br>acc ↑ acc ↑ acc n ↑ acc ↑ acc ↑ acc n ↑ ↑|
|---|---|---|---|
|_19B / 213M_|DeltaNet[_−_1_,_ 1]<br>DeltaProduct2[_−_1_,_ 1]<br>DeltaProduct3[_−_1_,_ 1]|32.39<br>107.41<br>31.46<br>78.98<br>30.94<br>70.5|20.4<br>65.3<br>35.1<br>52<br>44.1<br>24.5<br>40.23<br>23.9<br>64.6<br>36.2<br>52.6<br>45<br>23<br>40.88<br>24.6<br>66.3<br>36.8<br>49.1<br>46<br>23.7<br>**41.01**|
|_35B / 392M_|DeltaNet[_−_1_,_ 1]<br>DeltaProduct2[_−_1_,_ 1]<br>DeltaProduct3[_−_1_,_ 1]|25.5<br>40.32<br>24.82<br>34.31<br>24.81<br>37.13|30.2<br>68.5<br>41<br>51.9<br>47.3<br>23.3<br>43.7<br>33.3<br>68.9<br>43.4<br>50.7<br>49.2<br>25<br>**45.08**<br>31.1<br>68.5<br>43.3<br>50<br>48.2<br>23.7<br>44.1|
|_55B / 805M_|DeltaNet[_−_1_,_ 1]<br>DeltaProduct2[_−_1_,_ 1]<br>DeltaProduct3[_−_1_,_ 1]|20.81<br>20.57<br>20.54<br>19.56<br>20.01<br>15.56|37.8<br>71.5<br>48.9<br>55.6<br>51.9<br>25.6<br>48.55<br>38.3<br>71<br>50.7<br>55.2<br>52.1<br>26.7<br>49<br>42.9<br>71.4<br>51.4<br>53<br>54.6<br>26.4<br>**49.95**|



Table 4: Performance comparison of models shown in Figure 21. Parameter equivalence was
achieved by scaling the number of heads in the attention. To account for the increased parameter
count we scaled the training token budget from 19B (213M parameters) to 55B (805M parameters)
on FineWeb [57]. Models were trained on 4096 token context length.

|Col1|Model|Wiki. LMB.<br>ppl ↓ ppl ↓|LMB. PIQA Hella. Wino. ARC-e ARC-c Avg.<br>acc ↑ acc ↑ acc n ↑ acc ↑ acc ↑ acc n ↑ ↑|
|---|---|---|---|
|_19B / 213M_|DeltaNet[_−_1_,_ 1]<br>DeltaProduct2[_−_1_,_ 1]<br>DeltaProduct3[_−_1_,_ 1]|31.96<br>85.36<br>30.87<br>89.23<br>30.85<br>71.52|22.5<br>65.2<br>35.4<br>50.8<br>44.7<br>22.4<br>40.17<br>23.1<br>65.4<br>36.5<br>51.2<br>43.5<br>22.4<br>40.35<br>24.1<br>66.3<br>36.1<br>51.6<br>44.1<br>23.9<br>**41.02**|
|_35B / 392M_|DeltaNet[_−_1_,_ 1]<br>DeltaProduct2[_−_1_,_ 1]<br>DeltaProduct3[_−_1_,_ 1]|24.86<br>39.1<br>24.97<br>35.68<br>25.2<br>40.96|30.8<br>69.2<br>41.4<br>50.5<br>46.7<br>24.4<br>43.83<br>31.9<br>69.6<br>42.5<br>52.6<br>47.4<br>25.9<br>**44.98**<br>30.5<br>69.1<br>42.3<br>51.4<br>47.7<br>23.9<br>44.15|
|_55B / 805M_|DeltaNet[_−_1_,_ 1]<br>DeltaProduct2[_−_1_,_ 1]<br>DeltaProduct3[_−_1_,_ 1]|20.6<br>21.18<br>20.26<br>17.41<br>19.97<br>17.78|38.7<br>71.5<br>48.7<br>52.8<br>51.9<br>25.7<br>48.22<br>40.7<br>72.6<br>50.3<br>53.9<br>52.4<br>24.9<br>49.13<br>40.79<br>72.3<br>50.9<br>52.1<br>53.9<br>26.4<br>**49.4**|



Table 5: Performance comparison of models trained with 2048 context length. (SlimPajama (SPJ)
reproduced from Yang et al. [10], Fine-Web (FW) ours). Results are shown for DeltaProduct and
Gated DeltaProduct. We use 8 heads for each layer, unless otherwise specified.

|Col1|Model|Wiki. LMB.<br>ppl ↓ ppl ↓|LMB. PIQA Hella. Wino. ARC-e ARC-c Avg.<br>acc ↑ acc ↑ acc n ↑ acc ↑ acc ↑ acc n ↑ ↑|
|---|---|---|---|
|_15B tokens SPJ_|_340M params_<br>Transformer++<br>Mamba [0_,_ 1]<br>GLA [0_,_ 1]<br>DeltaNet [0_,_ 1]|28.39<br>42.69<br>28.39<br>39.66<br>29.47<br>45.53<br>**28.24**<br>**37.37**|31.0<br>63.3<br>34.0<br>50.4<br>44.5<br>24.2<br>41.2<br>30.6<br>65.0<br>**35.4**<br>50.1<br>**46.3**<br>23.6<br>41.8<br>31.3<br>**65.1**<br>33.8<br>51.6<br>44.4<br>**24.6**<br>41.8<br>**32.1**<br>64.8<br>34.3<br>**52.2**<br>45.8<br>23.5<br>**42.1**|
|_35B FW_<br>DeltaNet[_−_1_,_ 1] 340M<br>26.92<br>43.07<br>29.8<br>69.0<br>41.0<br>50.9<br>46.6<br>24.5<br>43.6<br>DeltaNet[_−_1_,_ 1] 12 heads, 392M<br>26.57<br>36.76<br>31.8<br>69.2<br>42.3<br>50.9<br>47.2<br>24.4<br>44.3<br>DeltaProduct2[_−_1_,_ 1] 392M<br>26.43<br>30.66<br>34.0<br>68.9<br>42.4<br>53.1<br>48.9<br>25.9<br>45.5<br>DeltaProduct3[_−_1_,_ 1] 443M<br>25.94<br>**29.91**<br>**34.2**<br>**69.9**<br>43.2<br>51.9<br>48.2<br>24.1<br>45.2<br>Gated DeltaNet[_−_1_,_ 1] 340M<br>25.97<br>33.57<br>33.1<br>69.5<br>**44.1**<br>51.1<br>**50.9**<br>**26.7**<br>45.9<br>Gated DeltaProduct2[_−_1_,_ 1] 393M<br>**25.12**<br>30.03<br>**34.2**<br>69.1<br>44.6<br>**55.3**<br>49.8<br>25.3<br>**46.4**|_35B FW_<br>DeltaNet[_−_1_,_ 1] 340M<br>26.92<br>43.07<br>29.8<br>69.0<br>41.0<br>50.9<br>46.6<br>24.5<br>43.6<br>DeltaNet[_−_1_,_ 1] 12 heads, 392M<br>26.57<br>36.76<br>31.8<br>69.2<br>42.3<br>50.9<br>47.2<br>24.4<br>44.3<br>DeltaProduct2[_−_1_,_ 1] 392M<br>26.43<br>30.66<br>34.0<br>68.9<br>42.4<br>53.1<br>48.9<br>25.9<br>45.5<br>DeltaProduct3[_−_1_,_ 1] 443M<br>25.94<br>**29.91**<br>**34.2**<br>**69.9**<br>43.2<br>51.9<br>48.2<br>24.1<br>45.2<br>Gated DeltaNet[_−_1_,_ 1] 340M<br>25.97<br>33.57<br>33.1<br>69.5<br>**44.1**<br>51.1<br>**50.9**<br>**26.7**<br>45.9<br>Gated DeltaProduct2[_−_1_,_ 1] 393M<br>**25.12**<br>30.03<br>**34.2**<br>69.1<br>44.6<br>**55.3**<br>49.8<br>25.3<br>**46.4**|_35B FW_<br>DeltaNet[_−_1_,_ 1] 340M<br>26.92<br>43.07<br>29.8<br>69.0<br>41.0<br>50.9<br>46.6<br>24.5<br>43.6<br>DeltaNet[_−_1_,_ 1] 12 heads, 392M<br>26.57<br>36.76<br>31.8<br>69.2<br>42.3<br>50.9<br>47.2<br>24.4<br>44.3<br>DeltaProduct2[_−_1_,_ 1] 392M<br>26.43<br>30.66<br>34.0<br>68.9<br>42.4<br>53.1<br>48.9<br>25.9<br>45.5<br>DeltaProduct3[_−_1_,_ 1] 443M<br>25.94<br>**29.91**<br>**34.2**<br>**69.9**<br>43.2<br>51.9<br>48.2<br>24.1<br>45.2<br>Gated DeltaNet[_−_1_,_ 1] 340M<br>25.97<br>33.57<br>33.1<br>69.5<br>**44.1**<br>51.1<br>**50.9**<br>**26.7**<br>45.9<br>Gated DeltaProduct2[_−_1_,_ 1] 393M<br>**25.12**<br>30.03<br>**34.2**<br>69.1<br>44.6<br>**55.3**<br>49.8<br>25.3<br>**46.4**|_35B FW_<br>DeltaNet[_−_1_,_ 1] 340M<br>26.92<br>43.07<br>29.8<br>69.0<br>41.0<br>50.9<br>46.6<br>24.5<br>43.6<br>DeltaNet[_−_1_,_ 1] 12 heads, 392M<br>26.57<br>36.76<br>31.8<br>69.2<br>42.3<br>50.9<br>47.2<br>24.4<br>44.3<br>DeltaProduct2[_−_1_,_ 1] 392M<br>26.43<br>30.66<br>34.0<br>68.9<br>42.4<br>53.1<br>48.9<br>25.9<br>45.5<br>DeltaProduct3[_−_1_,_ 1] 443M<br>25.94<br>**29.91**<br>**34.2**<br>**69.9**<br>43.2<br>51.9<br>48.2<br>24.1<br>45.2<br>Gated DeltaNet[_−_1_,_ 1] 340M<br>25.97<br>33.57<br>33.1<br>69.5<br>**44.1**<br>51.1<br>**50.9**<br>**26.7**<br>45.9<br>Gated DeltaProduct2[_−_1_,_ 1] 393M<br>**25.12**<br>30.03<br>**34.2**<br>69.1<br>44.6<br>**55.3**<br>49.8<br>25.3<br>**46.4**|



32


**C.3.4** **Training behavior**


The training behavior of DeltaProduct _nh_ is stable as shown in Figure 15. This is also true for all
considered model sizes in Figures 10 and 21 and Section C.3.5.










|Training Loss vs Steps|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|8<br>9<br>0<br>   <br>2.68<br>2.70<br>Last 1|8<br>9<br>0<br>   <br>2.68<br>2.70<br>Last 1|8<br>9<br>0<br>   <br>2.68<br>2.70<br>Last 1|8<br>9<br>0<br>   <br>2.68<br>2.70<br>Last 1||
|8<br>9<br>0<br>   <br>2.68<br>2.70<br>Last 1|||Last 1|5k Steps|
|8<br>9<br>0<br>   <br>2.68<br>2.70<br>Last 1|||2.70<br>||
|8<br>9<br>0<br>   <br>2.68<br>2.70<br>Last 1|||2.68||
|6<br>|||2.66||
|6<br>|||~~2.64~~<br>||
|~~0~~<br>~~1~~<br>~~2~~<br>~~3~~<br>~~4~~<br>~~5~~<br>~~6~~<br><br>Steps<br>×10<br>4<br>3<br>4<br><br>~~5.25~~<br>~~5.50~~<br>~~5.75~~<br>~~6.00~~<br>~~6.25~~<br>~~6.50~~<br>Steps<br>×10<br>4<br>|||||
|~~0~~<br>~~1~~<br>~~2~~<br>~~3~~<br>~~4~~<br>~~5~~<br>~~6~~<br><br>Steps<br>×10<br>4<br>3<br>4<br><br>~~5.25~~<br>~~5.50~~<br>~~5.75~~<br>~~6.00~~<br>~~6.25~~<br>~~6.50~~<br>Steps<br>×10<br>4<br>|||~~5.25~~<br>~~5.50~~<br>~~5.75~~<br>St|~~6.00~~<br>~~6.25~~<br>~~6.50~~<br>eps<br>×10<br>4|
|~~0~~<br>~~1~~<br>~~2~~<br>~~3~~<br>~~4~~<br>~~5~~<br>~~6~~<br><br>Steps<br>×10<br>4<br>3<br>4<br><br>~~5.25~~<br>~~5.50~~<br>~~5.75~~<br>~~6.00~~<br>~~6.25~~<br>~~6.50~~<br>Steps<br>×10<br>4<br>|||||



1 2 3


Figure 15: Training loss curves of DeltaProduct _nh_ [ _−_ 1 _,_ 1]. The curves demonstrate stable training
behavior as _nh_ increases, with higher values of _nh_ consistently yielding lower losses throughout
training and convergence. While the absolute differences in loss between different _nh_ values are
relatively small, they correspond to significant differences in length extrapolation performance.


**C.3.5** **Additional results on Length Extrapolation**


In this section we show additional plots on length extrapolation. In Figure 16 we show the length
extrapolation behavior of (Gated) DeltaProduct _nh_ scaling up _nh_ without adjusting any of the other
model configuration parameters. As discussed in Section 5.3, increasing _nh_ increases the parameter count of the model. Hence, Figures 17 and 18 show the per-token loss and perplexity of
DeltaProduct _nh_ at three different scales where the parameter counts are matched at the respective
scales following the configuration parameters shown in Table 6. Note that these are the same models
as shown in Figure 21.


33


4096 16384


4096 16384


Sequence Position



3 _._ 2


3 _._ 1


3 _._ 0


2 _._ 9


24


22


20


18



4096 16384


4096 16384


Sequence Position



4096 16384


4096 16384


Sequence Position



3 _._ 3


3 _._ 2


3 _._ 1


3 _._ 0


2 _._ 9


24


22


20


18



2 _._ 80


2 _._ 75


2 _._ 70


2 _._ 65


2 _._ 60
4096 16384


15 _._ 5


15 _._ 0


14 _._ 5


14 _._ 0

4096 16384


Sequence Position



2 _._ 8


2 _._ 7


2 _._ 6


16 _._ 0


15 _._ 5


15 _._ 0


14 _._ 5



_**nh**_
**1** **2** **3**


Model type
Non-gated Gated


Figure 16: Per token loss and perplexity on context lengths up to 32.768 for (Gated) DeltaProduct _nh_ .
_(Top)_ per token loss. _(Bottom)_ Perplexity. Per token losses smoothed with a window-size of 300.


34


3 _._ 00


2 _._ 95


2 _._ 90


2 _._ 85


2 _._ 80
4096 16384


2 _._ 8


2 _._ 7


4096 16384


2 _._ 7


2 _._ 6


2 _._ 5


4096 16384


Sequence Position





4096 16384


4096 16384


4096 16384


Sequence Position



3 _._ 6


3 _._ 5


3 _._ 4


3 _._ 3


3 _._ 2
4096 16384


3 _._ 2


3 _._ 0


4096 16384


3 _._ 2


3 _._ 0


2 _._ 8


4096 16384


Sequence Position



4 _._ 0


3 _._ 8


3 _._ 6


3 _._ 4


3 _._ 2


3 _._ 6


3 _._ 4


3 _._ 2


3 _._ 0


3 _._ 6


3 _._ 4


3 _._ 2


3 _._ 0


2 _._ 8



4096 16384


4096 16384


4096 16384


Sequence Position



3 _._ 00


2 _._ 95


2 _._ 90


2 _._ 85


2 _._ 80


2 _._ 75


2 _._ 70


2 _._ 65


2 _._ 60


2 _._ 55


2 _._ 50



_**nh**_
**1** **2** **3**


Figure 17: Per token loss of DeltaProduct _nh_ on contexts up to 32.768 at different model sizes.
Models are parameter equivalent at each scale. Parameter equivalence is achieved by scaling the
number of heads. Exact model configurations can be found in Table 6 _(Top)_ 213M parameters.
_(Middle)_ 392M parameters. _(Bottom)_ 805M parameters.


35


30


25


26


24


22


20





35



19 _._ 0





30


28


26


23


22


21


20



|CodeParrot|Col2|
|---|---|
|||
|||
|||
|||


4096 16384


|enThoughts 114K Ma|Col2|
|---|---|
|||
|||
|||


|Trivia QA|Col2|
|---|---|
|||
|||
|||


|SlimPajama-6B|Col2|
|---|---|
|||
|||
|||
|||
|||



4096 16384


4096 16384



18 _._ 5


18 _._ 0


17 _._ 5
4096 16384


15 _._ 6


15 _._ 4


15 _._ 2


15 _._ 0


14 _._ 8
4096 16384


13 _._ 0


12 _._ 5



19
4096 16384


20


18



4096 16384


Sequence Position



16



4096 16384


Sequence Position



4096 16384


Sequence Position



25 _._ 0


22 _._ 5


20 _._ 0


17 _._ 5


15 _._ 0



12 _._ 0



18 _._ 50


18 _._ 25


18 _._ 00


17 _._ 75


17 _._ 50
4096 16384


15 _._ 75


15 _._ 50


15 _._ 25


15 _._ 00
4096 16384


14 _._ 0


13 _._ 5


13 _._ 0


4096 16384


Sequence Position



_**nh**_
**1** **2** **3**


Figure 18: Perplexity analogue of Figure 17


Table 6: Model configuration parameters for models shown in Figures 17 and 18. All other configuration parameters are the same as in [16].


**Model Scale** **# Householders** **Hidden size** **# Heads**



**213M**


**392M**


**805M**



1 768 8
2 736 6
3 768 4


1 1024 12
2 1024 8
3 1024 6


1 1536 16
2 1468 12
3 1536 8


36


Figure 19: Effective rank of _**H**_ _i_ for 4 of 8 heads for a selection of the layers on **CodeParrot** sequences. Solid vertical lines mark new code sequences; dashed vertical line indicates 4096-token
training context length; colored lines show effective rank per head over the sequence.


**C.3.6** **Additional Results on Scaling Behavior**


In Figure 10 parameter equivalence is achieved at each scale mainly by decreasing the the head
dimension for models with _nh >_ 1. In Figure 21 we show perplexity of FineWeb for another set of
scaling results where parameter equivalence is reached by reducing the the number of heads in the
attention. The result for this alternative type of scaling still shows the superiority of DeltaProduct
compared to DeltaNet. However, in this case, models with higher _nh_ are not strictly better than
those with fewer Householders.


37


Figure 20: Effective rank of _**H**_ _i_ for 4 of 8 heads for a selection of the layers on **TriviaQA** sequences.
Solid vertical lines mark new question-answer pairs; dashed vertical line indicates 4096-token training context length; colored lines show effective rank per head over the sequence.


38


|17|Col2|Col3|Col4|
|---|---|---|---|
|11<br>12<br>13<br>14<br>15<br>16<br><br>|||DeltaNet<br>~~DeltaProduct~~|
|11<br>12<br>13<br>14<br>15<br>16<br><br>|||2<br>DeltaProduct3|
|11<br>12<br>13<br>14<br>15<br>16<br><br>||||
|11<br>12<br>13<br>14<br>15<br>16<br><br>||||
|11<br>12<br>13<br>14<br>15<br>16<br><br>||||
|11<br>12<br>13<br>14<br>15<br>16<br><br>||||
|||||

















Figure 21: Scaling analysis w.r.t. ( _top_ ) final perplexity on FineWeb, ( _bottom_ ) Lambada and lm-eval
tasks. Parameter equivalence is achieved by scaling the number of heads. Models trained at each
scale with number of tokens as reported in Table 4.


39


