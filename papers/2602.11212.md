### LUMIA Lab 2026-02-09

# **Towards Compressive and Scalable Recurrent** **Memory**

**Yunchong Song** [2] **, Jushi Kai** [1] **, Liming Lu** [1] **, Kaixi Qiu** [1] **, Zhouhan Lin** [1] _[,]_ [2] _[,]_ [3][â€¡]

1 LUMIA Lab, School of Artificial Intelligence, Shanghai Jiao Tong University 2 Shanghai Artificial Intelligence Laboratory
3 Shanghai Innovation Institute


- songyunchong@pjlab.org.cn, lin.zhouhan@gmail.com

- Corresponding Author.


**Abstract** Transformers face a quadratic bottleneck in attention when scaling to long contexts. Recent
approaches introduce recurrent memory to extend context beyond the current window, yet these often
face a fundamental trade-off between theoretical principles and practical scalability. To address this, we
introduce _**Elastic Memory**_, a novel memory architecture grounded in the HiPPO framework for online
function approximation. Elastic Memory treats historical sequence as samples from continuous signals,
applying optimal online compression to encode them into a fixed-size memory state. For retrieval,
we propose a flexible _polynomial sampling_ mechanism that reconstructs a history summary from this
compressed state. Elastic Memory consistently outperformed baselines on long-context (32k+) datasets
across three domains. With equal parameters, it beat Memorizing Transformer by 16x memory and
outperformed Melodi at all memory sizes, even when Melodi had 30% more parameters. When scaling
model size, Elastic Memory stayed ahead of all baselines and was significantly faster than Melodi at
4x size. Furthermore, its decoupled design allows for injecting inductive biases at test-time to boost
performance.

### **1. Introduction**


The central challenge in extending the context length of language models lies in efficiently managing
an ever-growing history while preserving information fidelity. Existing approaches to this problem
often present a fundamental trade-off between theoretical principles and practical scalability. On
one hand, paradigms like associative memory (Munkhdalai et al., 2024; Schlag et al., 2021) offer a
theoretical foundation, framing memory updates as an online optimization of key-value associations.
However, they face a critical scalability bottleneck: their memory capacity is rigidly coupled with model
dimensions such as the number of heads and head dimension, precluding performance improvements
by simply expanding the memory size. On the other hand, methods based on external memory (Wu
et al., 2022) or summary tokens (Bulatov et al., 2022; Chen et al., 2025) provide practical scalability, as
the memory size is a configurable hyperparameter. Yet, these approaches are often rooted in heuristic
designs and lack a formal answer to what constitutes optimal memory compression, potentially
leading to suboptimal results. This dilemma highlights the need for a new memory paradigm that
unifies the rigor of a principled formulation with the flexibility of a scalable architecture. We argue
that the HiPPO (High-order Polynomial Projection Operators) framework (Gu et al., 2020) provides
the theoretical cornerstone for such a paradigm, reframing the problem of memory from heuristic
compression to a principled problem of online function approximation.


In this work, we introduce Elastic Memory, a novel memory architecture that operationalizes the
HiPPO theory to resolve the aforementioned trade-off. Elastic Memory treats key-value sequences as


### Towards Compressive and Scalable Recurrent Memory

**Algorithm 1:** Elastic Memory Attention Block Forward Pass

**Input:** Current input block **H** curr âˆˆ â„ _[ğ¿]_ [Ã—] _[ğ·]_ [model] ; previous memory states **C** _ğ‘–_ [(] - _[ğ¾]_ 1 [)] _[,]_ **[ C]** _ğ‘–_ [(] - _[ğ‘‰]_ 1 [)] [âˆˆ] [â„] _[ğ‘]_ [Ã—] _[ğ·]_ [; block index]
_ğ‘–_ .
**Output:** Output for current block **O** âˆˆ â„ _[ğ¿]_ [Ã—] _[ğ·]_ [model] ; updated memory states **C** _ğ‘–_ [(] _[ğ¾]_ [)] _,_ **C** _ğ‘–_ [(] _[ğ‘‰]_ [)] âˆˆ â„ _[ğ‘]_ [Ã—] _[ğ·]_ .

1: **Q** raw _,_ **K** raw _,_ **V** curr â† Linear( **H** curr) _âŠ²_ Project inputs to raw Q, K, V

2: **Q** curr _,_ **K** curr â† ApplyRoPE( **Q** raw _,_ **K** raw) _âŠ²_ Apply positional encodings

_âŠ²_ _**1. Memory Retrieval**_



3: Retrieve _reconstruction matrix_ **R** _ğ‘–_ âˆ’1 from precomputed bank.

4: **K** mem _,_ **V** mem â† **R** _ğ‘–_ âˆ’1 **C** _ğ‘–_ [(] - _[ğ¾]_ 1 [)] _[,]_ **R** _ğ‘–_ âˆ’1 **C** _ğ‘–_ [(] - _[ğ‘‰]_ 1 [)] _âŠ²_ Eq. 8
_âŠ²_ _**2. Trapezoidal Attention**_

5: **K** aug â†[ **K** mem _,_ **K** curr]; **V** aug â†[ **V** mem _,_ **V** curr]

**Q** curr **K** _[ğ‘‡]_ aug
6: **S** â† ~~âˆš~~

_ğ·_
7: **O** att â† softmax( **S** + **M** ) **V** aug _âŠ²_ Apply trapezoidal mask **M**

_âŠ²_ _**3. Memory Update**_

8: Retrieve _state transition matrix_ **P** _ğ‘–_ and _HiPPO kernel_ **K** **[Â¯]** _ğ‘–_ from precomputed bank.

9: **C** _ğ‘–_ [(] _[ğ¾]_ [)] _,_ **C** _ğ‘–_ [(] _[ğ‘‰]_ [)] â† **P** _ğ‘–_ **C** _ğ‘–_ [(] - _[ğ¾]_ 1 [)] [+] **[ Â¯K]** _[ğ‘–]_ **[K]** [raw] _[,]_ **P** _ğ‘–_ **C** _ğ‘–_ [(] - _[ğ‘‰]_ 1 [)] [+] **[ Â¯K]** _[ğ‘–]_ **[V]** [curr] _âŠ²_ Eq. 6



10: **O** â† Linear( **O** att) _âŠ²_ Project output



11: **return O** _,_ **C** _ğ‘–_ [(] _[ğ¾]_ [)] _,_ **C** _ğ‘–_ [(] _[ğ‘‰]_ [)]



sampling points from continuous signals and applies HiPPO for a mathematically optimal, incremental
compression into a fixed-size memory state. For retrieval, we propose a flexible _polynomial sampling_
mechanism that reconstructs a history summary from this compressed state, which is then seamlessly
integrated into the attention mechanism. Our comprehensive experiments validate the effectiveness
of this design. Across three diverse long-document (32k+) datasets, Elastic Memory achieves state-ofthe-art performance. It exhibits great memory efficiency. With the same model parameters, Elastic
Memory achieves lower PPL than the Memorizing Transformer, a classic memory model, with 16x
larger memory. It also consistently surpasses the SOTA Melodi model, even when Melodi is allocated
twice the memory. This lead holds even when Melodi has 30% more trainable parameters. This
advantage persists as we scale the model size, where Elastic Memory continues to outperform all
baselines in terms of PPL while being 50% faster than Melodi at the 4x model scale.

### **2. Method**


In this section, we present our model, Elastic Memory. We begin by providing the necessary theoretical
background on the HiPPO framework (Section 2.1), which forms the mathematical foundation of
our approach and summarizes the theoretical foundations established by Gu et al. (2020). To ensure
clarity, Equations 1â€“4 in Section 2.1 are adapted from prior literature. We then detail our original
contributions: the memory update mechanism (Section 2.2), based on parallelized block-level HiPPO
compression (Equations 5â€“6), and the memory retrieval mechanism (Section 2.3), which leverages a
novel polynomial sampling technique (Equations 7â€“8). The complete forward pass is summarized in
Algorithm 1.


**2.1. Background: The HiPPO Framework**


**Memory as Function Approximation.** The theoretical cornerstone of our work is the HiPPO (Highorder Polynomial Projection Operators) framework (Gu et al., 2020), which addresses the fundamental
problem of incrementally representing a cumulative history of a signal within a fixed-size state. This

### 2


### Towards Compressive and Scalable Recurrent Memory

challenge has been a long-standing issue in sequence modeling, originally highlighted in the context
of continuous-time signals and recurrent neural networks. HiPPO provides a principled solution by
reframing memory from an engineering problem of caching or heuristic summarization to a formal
problem of online function approximation. The core idea is to continuously compress an input history
signal, conceptualized as a function _ğ‘“_ ( _ğ‘¡_ ), by optimally projecting it onto a basis of polynomials. This
projection yields a set of coefficients that serve as a compact, fixed-size representation of the entire
history. While this theory originates from a continuous-signal perspective, its principle of optimal,
incremental compression provides a powerful foundation for tackling the information bottlenecks
observed in modern language model memory mechanisms. This function approximation viewpoint
offers a mathematically rigorous path for designing memory systems, and we now introduce the
technical details of the HiPPO framework.


**Mathematical Formulation of HiPPO-LegS.** The HiPPO framework formally casts the problem of
maintaining memory as finding a polynomial _ğ‘”_ [(] _[ğ‘¡]_ [)] of degree less than _ğ‘_ that best approximates the
history signal _ğ‘“_ â‰¤ _ğ‘¡_ . This is framed as a continuous optimization problem defined by a time-varying
measure _ğœ‡_ [(] _[ğ‘¡]_ [)], which assigns importance to past events. Our work specifically leverages the HiPPO-LegS
(Scaled Legendre) variant, which is defined by two key mathematical constructs.


First, HiPPO-LegS employs a scaled measure that uniformly weights the entire history from the
beginning up to the current time _ğ‘¡_ . This measure induces an inner product on the space of functions
defined as:



_ğ‘¡_

_ğ‘“_ ( _ğ‘¥_ ) _ğ‘”_ ( _ğ‘¥_ ) [1]
0 _ğ‘¡_



âˆ« _ğ‘¡_
âŸ¨ _ğ‘“, ğ‘”_ âŸ© _ğœ‡_ ( _ğ‘¡_ ) =



(1)
_ğ‘¡_ _[ğ‘‘ğ‘¥.]_



The corresponding optimization objective is to minimize the squared _ğ¿_ 2 norm of the approximation
error: âˆ¥ _ğ‘“_ â‰¤ _ğ‘¡_ - _ğ‘”_ [(] _[ğ‘¡]_ [)] âˆ¥ [2] _ğ¿_ 2 ( _ğœ‡_ [(] _[ğ‘¡]_ [)] ) [. This uniform weighting over the expanding interval][ [][0] _[, ğ‘¡]_ []][ is a crucial property]
that endows the resulting memory system with robustness to varying timescales.


Second, the optimal solution to this problem is found by projecting the signal onto an orthonormal
basis. HiPPO-LegS constructs this basis from the classical Legendre polynomials { _ğ‘ƒğ‘›_ ( _ğ‘¥_ )} _ğ‘›_ _[ğ‘]_ = [âˆ’] 0 [1][. These]
polynomials are first affinely transformed to be orthogonal on the interval [0 _, ğ‘¡_ ], and then normalized
~~âˆš~~
to form an orthonormal basis { _ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [(] _[ğ‘¥]_ [)}] _ğ‘›_ _[ğ‘]_ = [âˆ’] 0 [1][, where] _[ ğ‘”]_ _ğ‘›_ [(] _[ğ‘¡]_ [)] [(] _[ğ‘¥]_ [)][ =] 2 _ğ‘›_ + 1 _ğ‘ƒğ‘›_ ( [2] _ğ‘¡_ _[ğ‘¥]_ [âˆ’] [1][)][.]

The memory state is then defined as the _ğ‘_ -dimensional coefficient vector _ğ‘_ ( _ğ‘¡_ ) âˆˆ â„ _[ğ‘]_ obtained by
projecting the history signal onto this basis:



2 _ğ‘›_ + 1 _ğ‘ƒğ‘›_ ( [2] _[ğ‘¥]_




_[ğ‘¥]_

_ğ‘¡_ [âˆ’] [1][)][.]



_ğ‘¡_

_ğ‘“_ ( _ğ‘¥_ ) _ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [(] _[ğ‘¥]_ [)] [1]
0 _ğ‘¡_



âˆ« _ğ‘¡_
_ğ‘ğ‘›_ ( _ğ‘¡_ ) = âŸ¨ _ğ‘“_ â‰¤ _ğ‘¡, ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [âŸ©] _ğœ‡_ [(] _[ğ‘¡]_ [)][ =]



(2)
_ğ‘¡_ _[ğ‘‘ğ‘¥.]_



This vector _ğ‘_ ( _ğ‘¡_ ) completely characterizes the optimal polynomial approximation:



_ğ‘”_ [(] _[ğ‘¡]_ [)] ( _ğ‘¥_ ) =



_ğ‘_ âˆ’1
âˆ‘ï¸

_ğ‘ğ‘›_ ( _ğ‘¡_ ) _ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [(] _[ğ‘¥]_ [)] _[.]_

_ğ‘›_ =0



It therefore serves as a compact, fixed-size representation of the entire history.


**From Continuous ODE to Discrete Recurrence.** A direct computation of the coefficients _ğ‘_ ( _ğ‘¡_ ) via the
integral in Eq. 2 at each timestep is computationally prohibitive, as it requires iterating over the entire
history. The central insight of the HiPPO framework is that the evolution of this coefficient vector is
governed by a linear Ordinary Differential Equation (ODE), enabling an efficient, incremental update
(see Appendix E.1 for the complete derivation). For HiPPO-LegS, this ODE is given by:



_ğ‘‘_
_ğ‘‘ğ‘¡_ _[ğ‘]_ [(] _[ğ‘¡]_ [)][ =][ âˆ’] [1] _ğ‘¡_




[1] [1]

_ğ‘¡_ **[A]** _[ğ‘]_ [(] _[ğ‘¡]_ [) +] _ğ‘¡_



(3)
_ğ‘¡_ **[B]** _[ ğ‘“]_ [(] _[ğ‘¡]_ [)] _[,]_


### 3


### Towards Compressive and Scalable Recurrent Memory

where **A** _,_ **B** âˆˆ â„ _[ğ‘]_ [Ã—] _[ğ‘]_ are the HiPPO matrices, which are fixed and mathematically derived from the
properties of Legendre polynomials, not learned. To apply this continuous-time model to discrete
sequences such as text, the ODE must be discretized. Using a standard method such as the Zero-Order
Hold (ZOH), which we use in our implementation for its stability, this process converts the continuous
dynamics into a discrete recurrence relation that updates the memory state _ğ‘ğ‘˜_ at step _ğ‘˜_ based on the
state at step _ğ‘˜_ - 1 and the current input _ğ‘“ğ‘˜_ :


_ğ‘ğ‘˜_ = **A** **[Â¯]** _ğ‘˜ğ‘ğ‘˜_ âˆ’1 + **B** **[Â¯]** _ğ‘˜_ _ğ‘“ğ‘˜._ (4)


Here, **A** **[Â¯]** _ğ‘˜_ and **B** **[Â¯]** _ğ‘˜_ are the discretized state matrices, which depend on the step _ğ‘˜_ due to the time-varying
nature of the ODE. This recurrence provides a mechanism to update the memory with constant
computational and memory complexity at each step, making it a highly efficient building block for
long-sequence models.


**2.2. Memory Update: Online Function Approximation**


**Compressing Over Time.** The discrete recurrence in Eq. 4 provides a highly efficient mechanism for
updating a memory state. In our Elastic Memory framework, we integrate this mechanism directly
into the Transformer architecture. Instead of treating the entire hidden state as a single signal, we
apply the HiPPO compression to the Key ( **K** ) and Value ( **V** ) sequences. Specifically, we treat each
feature dimension of the raw **K** and **V** projections as an independent one-dimensional signal evolving
over the sequence length. It is crucial to note that the Key sequences compressed by HiPPO are those
_prior_ to the application of rotational position embeddings (RoPE), as the function approximation
framework is designed to operate on the underlying signal.


**From Recurrent Update to Parallel Block Update.** A naive, token-by-token application of the
recurrence in Eq. 4 within a block of length _ğ¿_ would be inefficient due to its sequential nature. To
enable parallel processing, we transform the serial recurrence into an equivalent block-level update. By
unrolling the recurrence over a block of inputs { _ğ‘“_ 0 _, ğ‘“_ 1 _, . . ., ğ‘“ğ¿_ âˆ’1}, the final state _ğ‘ğ¿_ âˆ’1 can be expressed
as a linear combination of the initial state _ğ‘_ âˆ’1 and all inputs within the block (see Appendix E.3 for
the detailed derivation):




- _ğ‘—_ +1

 
**Â¯A** _ğ‘˜_

_ğ‘˜_ = _ğ¿_ âˆ’1











_ğ¿_ âˆ’1
âˆ‘ï¸


_ğ‘—_ =0



**Â¯B** _ğ‘—_ _ğ‘“_ _ğ‘—._ (5)



_ğ‘ğ¿_ âˆ’1 =




- 0

 
**Â¯A** _ğ‘˜_

_ğ‘˜_ = _ğ¿_ âˆ’1



_ğ‘_ âˆ’1 +



This unrolled equation can be reframed as a single parallel operation. For the _ğ‘–_ -th block of the input
sequence, with initial memory state **C** _ğ‘–_ âˆ’1 and input features **F** _ğ‘–_ = [ _ğ‘“_ 0 _, . . ., ğ‘“ğ¿_ âˆ’1] _[ğ‘‡]_, the updated memory
state **C** _ğ‘–_ is given by:
**C** _ğ‘–_ = **P** _ğ‘–_ **C** _ğ‘–_ âˆ’1 + **K** **[Â¯]** _ğ‘–_ **F** _ğ‘–._ (6)

Here, **P** _ğ‘–_ = [ï¿½] _ğ‘˜_ _[ğ‘–ğ¿]_ =( _ğ‘–_ +1) _ğ¿_ âˆ’1 **[Â¯A]** _[ğ‘˜]_ [is the state transition matrix that evolves the memory state across the]
entire block. The term **K** **[Â¯]** _ğ‘–_ âˆˆ â„ _[ğ‘]_ [Ã—] _[ğ¿]_ is a linear operator we term the _HiPPO kernel_, whose columns
are composed of the products of **A** **[Â¯]** _ğ‘˜_ and **B** **[Â¯]** _ğ‘˜_ matrices from Eq. 5. Crucially, due to the time-varying
nature of the HiPPO-LegS ODE, the discretized matrices **A** **[Â¯]** _ğ‘˜_ and **B** **[Â¯]** _ğ‘˜_ depend on the absolute position
_ğ‘˜_ . Consequently, the state transition matrix **P** _ğ‘–_ and the HiPPO kernel **K** **[Â¯]** _ğ‘–_ are unique for each block
position _ğ‘–_ .


**Efficient Parallel Update via Precomputation.** While Eq. 6 enables parallel computation within
a block, the on-the-fly calculation of the position-dependent matrices **P** _ğ‘–_ and **K** **[Â¯]** _ğ‘–_ would introduce
significant computational overhead. To achieve maximum efficiency, we employ a precomputation
strategy. Before training, we precompute and cache the state transition matrices and HiPPO kernels

### 4


### Towards Compressive and Scalable Recurrent Memory

for all block positions up to a predefined maximum context length. During the forward pass, the
memory update for the _ğ‘–_ -th block becomes a simple and highly efficient two-step process: (1) retrieve
the precomputed matrices **P** _ğ‘–_ and **K** **[Â¯]** _ğ‘–_ ; (2) execute the block update in Eq. 6 using two parallel matrixmultiplication operations. This strategy transforms the theoretically sequential HiPPO recurrence into
a fully parallelizable block-level operation that can be efficiently executed on modern hardware, all
while preserving the mathematical exactitude of the original HiPPO formulation.


**2.3. Memory Retrieval: Polynomial Sampling**


**Precomputed Reconstruction Bank.** The retrieval of historical information is grounded in the
reconstruction property of the HiPPO framework (see Appendix E.4 for the detailed derivation).
The memory state **C** _ğ‘–_ âˆ’1, which holds the coefficients of the optimal polynomial approximation after
processing _ğ‘–_ - 1 blocks (total length _ğ‘¡_ = ( _ğ‘–_ - 1) _ğ¿_ ), can be used to reconstruct the historical signal at
any set of chosen sample points. This reconstruction is a linear operation. To maximize efficiency,
we precompute a _Reconstruction Bank_, which is a collection of position-dependent reconstruction
matrices { **R** 0 _,_ **R** 1 _, . . ._ }. Each matrix **R** _ğ‘–_ âˆˆ â„ _[ğ¿]_ [mem] [Ã—] _[ğ‘]_ is designed to sample _ğ¿_ mem points from the history
of length _ğ‘¡_ = _ğ‘–ğ¿_ . Its entries are defined by evaluating the Legendre basis functions at the chosen
sample points { _ğ‘¥_ _ğ‘—_ } _[ğ¿]_ _ğ‘—_ = [mem] 0 [âˆ’][1] :



~~âˆš~~
( **R** _ğ‘–_ ) _ğ‘—ğ‘›_ = _ğ‘”ğ‘›_ [(] _[ğ‘–ğ¿]_ [)] ( _ğ‘¥_ _ğ‘—_ ) = 2 _ğ‘›_ + 1 _ğ‘ƒğ‘›_




- 2 _ğ‘¥_ _ğ‘—_ 
_ğ‘–ğ¿_ [âˆ’] [1]



_._ (7)



At the beginning of processing block _ğ‘–_, the memory Keys and Values are retrieved in parallel by looking
up the appropriate reconstruction matrix **R** _ğ‘–_ âˆ’1 and performing a matrix multiplication:


**K** mem = **R** _ğ‘–_ âˆ’1 **C** _ğ‘–_ [(]           - _[ğ¾]_ 1 [)] _[,]_ **V** mem = **R** _ğ‘–_ âˆ’1 **C** _ğ‘–_ [(]           - _[ğ‘‰]_ 1 [)] _[.]_ (8)


**Fixed-Length Polynomial Sampling.** A key innovation of our work is the strategic selection of the
_ğ¿_ mem sample points { _ğ‘¥_ _ğ‘—_ } that define each reconstruction matrix **R** _ğ‘–_ . This process, which we term
_Polynomial Sampling_, determines which parts of the history are retrieved. We explore two strategies:


 - **Uniform Sampling (** _**Elastic Memoryuni**_ **)** : The sample points are uniformly spaced across the
history [0 _, ğ‘–ğ¿_ ), defined as _ğ‘¥_ _ğ‘—_ = _ğ‘—_    - _ğ¿_ mem _ğ‘–ğ¿_ [for] _[ ğ‘—]_ [=][ 0] _[, . . ., ğ¿]_ [mem][ âˆ’] [1. This treats all parts of the history]
as equally important.

 - **Exponential Sampling (** _**Elastic Memoryexp**_ **)** : The sample points are concentrated near the
present and become exponentially sparser into the past. This is motivated by the linguistic
intuition that recent context is most critical for immediate prediction, while distant history
provides broader context.


**Integration via Trapezoidal Attention.** The retrieved memory tokens are seamlessly integrated
into the Transformerâ€™s attention mechanism. The retrieved **K** mem and **V** mem are prepended to the
current blockâ€™s Keys **K** curr and Values **V** curr, forming augmented sequences **K** aug = [ **K** mem _,_ **K** curr] and
**V** aug = [ **V** mem _,_ **V** curr]. The attention scores for the current blockâ€™s queries **Q** curr are then computed as:


**Q** curr **K** aug _[ğ‘‡]_
**S** = ~~âˆš~~ _._ (9)

_ğ·_


A trapezoidal attention mask **M** is applied to these scores. This mask allows all queries to attend to all
_ğ¿_ mem memory tokens, while enforcing causality within the current block. The final output from the
attention computation is:
**O** att = softmax( **S** + **M** ) **V** aug _._ (10)

### 5


### Towards Compressive and Scalable Recurrent Memory

**Implicit Temporal Encoding.** Notably, the retrieved memory tokens do not receive external positional encodings. This design is intentional: the reconstruction process is _inherently time-aware_ . As
formalized in Equation 7, the reconstruction matrix **R** _ğ‘–_ is constructed by evaluating Legendre basis
functions at specific temporal coordinates { _ğ‘¥_ _ğ‘—_ }. Since the Legendre polynomial values _ğ‘ƒğ‘›_ ( _ğ‘¥_ _ğ‘—_ ) are
unique, non-linear functions of the temporal coordinate _ğ‘¥_ _ğ‘—_, the position information is mathematically
intrinsic to each reconstructed vector. The retrieved key **K** mem [(] _[ ğ‘—]_ [)] [represents the reconstructed signal]
value _ğ‘“_ ( _ğ‘¥_ _ğ‘—_ ) â‰ˆ [ï¿½] _ğ‘›_ _[ğ‘][ğ‘›][ğ‘ƒ][ğ‘›]_ [(] _[ğ‘¥]_ _[ğ‘—]_ [)][, where the temporal position is encoded through the basis coefficients rather]
than through additive positional embeddings.

### **3. Experiments**


To evaluate the performance of Elastic Memory, we conduct a comprehensive set of experiments on
long-document language modeling. We structure our evaluation to assess several key dimensions:
(1) core language modeling performance against state-of-the-art memory baselines across diverse
domains; (2) the scalability of our model with respect to both memory capacity and model parameter
size; (3) in-depth analyses of the memory mechanism itself, probing its flexibility and robustness; and
(4) the computational efficiency of our method during training. The experimental results show that
Elastic Memory achieves strong performance across all evaluation criteria, validating its effectiveness
as a efficient, scalable, and theoretically-grounded memory architecture.


**3.1. Experimental Setup**


**Datasets.** We evaluate our model on three challenging long-document datasets from diverse domains
to test for generalization: **PG-19** (Rae et al., 2020), a collection of long-form books; **Proof-Pile** (Azerbayev et al., 2023), a dataset of mathematical papers; and **FineWeb-Edu**, a high-quality educational
subset of the FineWeb dataset (Penedo et al., 2024). To construct a clean benchmark for long-context
modeling, we preprocess these datasets by first filtering for documents exceeding 32,768 tokens. We
then segment these long documents into non-overlapping chunks of exactly 32,768 tokens, discarding
any trailing partial chunks. This process yields three datasets, each containing approximately 2 billion
tokens, where every sample is an integer multiple of 32k tokens, providing a controlled, real-world
environment for studying long-range dependencies. We set the block size to 2,048 tokens for all
experiments.


**Table 1** | Recurrent memory model comparison. Memory size and additional trainable parameters
are counted per layer. _ğ‘¯_ represents the number of attention heads.


**Model** **Memory Size** **Add. Params** **Update** **Retrieval**


**kNN+**
**MemTrans.** _ğ‘µ_ queue **Ã— (** _ğ’…_ key **+** _ğ’…_ value **) Ã—** _ğ‘¯_ _ğ‘¯_ **FIFO queue** **cross-attn**


**Hebb rule+**
**InfiniTrans.** _ğ’…_ key **Ã—** _ğ’…_ value **Ã—** _ğ‘¯_ _ğ‘¯_ **delta rule** **linear-attn**


**self-attn+**
**Melodi** **(** _ğ‘µ_ mem. **+** _ğ‘µ_ sum. **) Ã—** _ğ’…_ value **Ã—** _ğ‘¯_ _ğ‘µ_ sum. **Ã—** _ğ’…_ value **Ã—** _ğ‘¯_ **linear-mix.** **cross-attn**


**function** **poly. sampling+**
**ElasticMem.** _ğ‘µ_ HiPPO **Ã— (** _ğ’…_ key **+** _ğ’…_ value **) Ã—** _ğ‘¯_ **0** **approx.** **cross-attn**


**Baselines.** We compare Elastic Memory with representative state-of-the-art memory architectures,
as summarized in Table 1: **Memorizing Transformer** (Wu et al., 2022) for _external memory_, **Infini-**

### 6


### Towards Compressive and Scalable Recurrent Memory

**Transformer** (Munkhdalai et al., 2024) for _associative memory_, and **Melodi** (Chen et al., 2025) for
_summary tokens_ . We also include a Llama 3 architecture baseline, denoted as **Transformer++**,
as a memory-free reference. Due to challenges in reproducing prior work, we re-implemented all
baselines to ensure a fair and controlled comparison. All models are built upon a unified Llama
3 architecture, with modifications confined to the attention or memory mechanisms within the
designated layers. This approach minimizes implementation differences and ensures that performance
variations primarily reflect the efficacy of the memory systems themselves. Our implementations
include slight modifications for fairness and stability: for Memorizing Transformer, we replace the
original kNN retrieval with an end-to-end trainable dense attention mechanism, a modification also
adopted by Chen et al. (2025); for Infini-Transformer, we add RMSNorm to stabilize training; for
Melodi, we implement only its intra-layer memory propagation to align with the design of other
baselines.


**Training.** To better isolate the effects of their memory mechanisms, all models are trained from
scratch. We use a consistent training setup for all experiments, including a Llama 2 tokenizer, identical
hyperparameters, and the same data sequence for each optimization step. All models are trained for
**40 billion tokens** . If training instability such as loss spikes was observed, the experiment was restarted
with the number of warmup steps doubled until training stabilized. To ensure a fair comparison, the
baseline memory size (1x) for all models is aligned with that of Infini-Transformer, whose capacity
is tied to its head dimension. We follow the memory injection strategies reported in the original
papers: Memorizing Transformer and our Elastic Memory are injected into the 9th layer, while
Infini-Transformer and Melodi are applied at every layer.


**Metrics.** We report two test metrics. **Perplexity (PPL)** is the standard and most widely used language modeling metric, computed using a sliding-window approach to better reflect performance
on contiguous long text. **LongPPL** is a complementary long-context evaluation metric that focuses
on _key tokens_ whose prediction is significantly influenced by the long context, and has been shown
to correlate strongly with long-text downstream performance (Fang et al., 2025). We compute
LongPPL using the authorsâ€™ released implementation with default hyperparameters, and use **Llama**
**3.1-8B** as the reference model for selecting key tokens: it shares the same Transformer++ backbone
architecture as our evaluated models and has a native 128k context window, fully covering our 32k
training/evaluation context (Meta AI, 2024a;b).


**3.2. Comparing with Baselines**


**Table 2** | Language modeling results on PG-19, Proof-Pile, and FineWeb-Edu. Samples across all the
datasets contain more than 32k tokens. Base model contains 100M parameters. We report PPL and
LongPPL (lower is better). Memory size and additional parameters are counted for the whole model.


**Mem.** **Add.** **PG-19** **Proof-Pile** **FineWeb-Edu**
**Model**
**Size** **Params** **PPL** **LongPPL** **PPL** **LongPPL** **PPL** **LongPPL**


**Transformer++** 0 0 11.232 15.955 3.322 3.866 12.897 29.882
**Mem. Transformer** 0.8M 96 11.050 15.794 3.305 3.843 12.924 29.963
**Infini-Transformer** 0.8M 96 10.907 9.588 2.950 2.736 12.736 24.417
**Melodi** 0.8M 1.5M 10.684 10.478 2.940 2.980 12.509 23.532


**Elastic Memoryuni** 0.8M 0 10.692 **8.646** 2.958 **2.560** 12.419 **10.915**
**Elastic Memoryexp** 0.8M 0 **10.651** 8.885 **2.915** 2.586 **12.318** 11.932


**Setting and Results.** We first compare the main language modeling performance of Elastic Memory
against the baselines. As shown in Table 2, Elastic Memory achieves strong performance under both

### 7


### Towards Compressive and Scalable Recurrent Memory

PPL and LongPPL across the three diverse long-text datasets, while requiring zero additional trainable
parameters.


**PPL.** _Elastic Memoryexp_ achieves the best PPL across all settings, validating the effectiveness of
exponentially biased sampling for improving the standard LM metric.


**LongPPL.** In contrast, _Elastic Memoryuni_ achieves the best LongPPL across all settings. This suggests
a natural trade-off: exponential sampling favors near-term prediction and improves the average-token
metric (PPL), while uniform sampling better supports key-token prediction that depends on long-range
context (LongPPL).


We treat LongPPL as a complementary metric and report it alongside PPL; note that LongPPL
intentionally focuses on a subset of long-context-dependent tokens, so absolute gaps can appear
larger than in standard PPL (Fang et al., 2025).


**3.3. Scaling Up Memory Size**


**Table 3** | Memory scaling experiments on Proof-Pile. The base model contains 100M parameters. _Add._
denotes additional trainable parameters, _PPL_ denotes perplexity, and _Long._ denotes LongPPL.


**1x Memory (0.8M)** **2x Memory (1.6M)** **4x Memory (3.2M)** **8x Memory (6.4M)** **16x Memory (12.8M)**
**Model**
**Add.** **PPL** **Long.** **Add.** **PPL** **Long.** **Add.** **PPL** **Long.** **Add.** **PPL** **Long.** **Add.** **PPL** **Long.**


**MemT.** 96 3.31 3.84 96 3.28 3.76 96 3.12 3.22 96 3.01 2.86 96 2.94 2.48
**Melodi** 1.5M 2.94 2.98 3.1M 2.93 2.97 6.4M 2.88 2.84 13.5M 2.84 2.67 30.1M 2.84 2.63


**Elas.uni** 0 2.96 **2.56** 0 2.91 **2.39** 0 2.86 **2.28** 0 2.81 **2.19** 0 2.78 **2.14**
**Elas.exp** 0 **2.92** 2.59 0 **2.86** 2.43 0 **2.82** 2.31 0 **2.78** 2.22 0 **2.75** 2.15


**Setting and Results.** To evaluate the efficiency and scalability of our memory compression, we
conduct experiments scaling the memory capacity from 1x to 16x on Proof-Pile. The results are
presented in Table 3.


**PPL.** _Elastic Memoryexp_ achieves the best PPL at all memory sizes. Notably, with just its baseline 1x
memory size, it already achieves lower PPL than a 16x Memorizing Transformer. Elastic Memory also
outperforms the powerful Melodi baseline, even when Melodi is allocated twice the memory capacity
(e.g., 2x Elastic Memory outperforms 4x Melodi, and 4x outperforms 8x). This is particularly notable
given Melodiâ€™s parameter overhead, which grows substantially with memory size (adding over 30M
parameters at 16x).


**LongPPL.** Across all memory sizes, _Elastic Memoryuni_ achieves lower LongPPL and scaling memory
improves LongPPL consistently when compared to _Elastic Memoryexp_ . While LongPPL tends to favor
uniform sampling within Elastic Memory, both variants remain competitive against strong baselines
under this key-token-focused long-context metric.


**3.4. Scaling Up Model Size**


**Setting and Results.** We investigate whether the benefits of Elastic Memory persist as the base model
size increases. We scale the underlying Transformer from 100M to 400M parameters, adjusting the
memory size of all models accordingly to maintain a fair comparison (Table 4).


**PPL.** _Elastic Memoryexp_ remains the best-performing variant across all model scales in terms of
PPL, indicating that the exponential retrieval bias continues to benefit standard LM evaluation as
model capacity grows.


**LongPPL.** At the same time, _Elastic Memoryuni_ achieves consistently lower LongPPL across scales,

### 8


### Towards Compressive and Scalable Recurrent Memory

**Table 4** | Model scaling experiments on Proof-Pile, with model scales from 100M to 200M and 400M
parameters. We report PPL and LongPPL (lower is better). Infini-Transformerâ€™s memory size is tied
with head dimensions, thus other memory models adjust their memory size to align with it.


**1x Params (100M)** **2x Params (200M)** **4x Params (400M)**
**Model**
**Mem.** **PPL** **LongPPL** **Mem.** **PPL** **LongPPL** **Mem.** **PPL** **LongPPL**


**Trans.++** 0 3.322 3.866 0 3.226 3.757 0 3.206 3.730
**MemTrans.** 0.8M 3.305 3.843 1.2M 3.213 3.715 1.6M 3.187 3.689
**InfiniTrans** 0.8M 2.950 2.736 1.2M 2.796 2.484 1.6M 2.772 2.410
**Melodi** 0.8M 2.940 2.980 1.2M 2.840 2.857 1.6M 2.771 2.719


**Elastic.uni** 0.8M 2.958 **2.560** 1.2M 2.824 **2.335** 1.6M 2.764 **2.238**
**Elastic.exp** 0.8M **2.915** 2.586 1.2M **2.775** 2.364 1.6M **2.737** 2.273


reinforcing the trade-off observed in the main results: uniform sampling better supports key-token
prediction that depends on long-range context.


**3.5. Injecting Bias into Memory Retrieval at Test-Time**


**Table 5** | Bias injection experiments on PG-19, Proof-Pile, and FineWeb-Edu. We experiment with 4
polynomial sampling strategies during memory retrieval. Each entry reports _PPL / LongPPL_ .


**Model** **PG-19** **Proof-Pile** **FineWeb-Edu**


**Elastic Memoryuni** 10.69 / 8.65 2.96 / **2.56** 12.42 / **10.92**
**Elastic Memoryuni-exp** 10.66 / 8.97 2.93 / 2.61 12.38 / 12.15
**Elastic Memoryexp** **10.65** / 8.89 **2.92** / 2.59 **12.32** / 11.93
**Elastic Memoryexp-uni** 10.70 / **8.60** 2.97 / 2.57 12.52 / 11.45


**Table 6** | Bias injection experiments on Proof-Pile when scaling model size or memory size. We
experiment with 4 polynomial sampling strategies during memory retrieval. Each entry reports _PPL /_
_LongPPL_ .


**Scaling Memory** **Scaling Model**
**Model** **1x**
**2x** **4x** **8x** **16x** **2x** **4x**


**Elastic Memoryuni** 2.96 / **2.56** 2.91 / **2.39** 2.86 / **2.28** 2.81 / **2.19** 2.78 / **2.14** 2.82 / **2.34** 2.76 / **2.24**
**Elastic Memoryuni-exp** 2.93 / 2.61 2.88 / 2.44 2.84 / 2.33 2.80 / 2.25 2.77 / 2.18 2.80 / 2.39 2.75 / 2.28
**Elastic Memoryexp** **2.92** / 2.59 **2.86** / 2.43 **2.82** / 2.31 **2.78** / 2.22 **2.75** / 2.15 **2.78** / 2.36 **2.74** / 2.27
**Elastic Memoryexp-uni** 2.97 / 2.57 2.91 / 2.42 2.86 / 2.30 2.82 / 2.21 2.79 / **2.14** 2.82 / 2.35 2.78 / 2.26


**Setting and Results.** A unique feature of Elastic Memory is the decoupling of its memory state
representation from the retrieval mechanism. We test the flexibility of this design by training a model
with one sampling strategy and evaluating it with another, without any retraining. The results are
presented in Table 5 and Table 6.


**PPL.** Test-time injection of exponential sampling can improve PPL, even when the model was
trained with uniform sampling (e.g., _Elastic Memoryuni-exp_ vs. _Elastic Memoryuni_ ). This suggests that
the learned memory state supports multiple retrieval biases, and that an exponential bias can be
beneficial for the standard perplexity metric at inference.


**LongPPL.** In contrast, LongPPL tends to favor uniform sampling at inference. This aligns with the

### 9


### Towards Compressive and Scalable Recurrent Memory

interpretation that exponential sampling emphasizes nearby context (improving average-token PPL),
whereas uniform sampling better preserves and retrieves signals from the distant past that matter for
key tokens in long contexts.


**3.6. Probing Memory Usage: Corrupted Local Context**


**Table 7** | Local context corruption experiments on Proof-Pile under different corruption rates. Each
entry reports _PPL / LongPPL_ .


**Corrupted Rate** **Hardly Corrupted Rate**
**Model** **Origin.**
**25%** **50%** **75%** **25%** **50%** **75%**


**Mem. Transformer** 3.31 / 3.84 3.31 / 3.85 3.32 / 3.85 **3.32** / 3.85 3.96 / 4.61 5.12 / 5.99 34.39 / 28.42
**Elastic Memoryuni** 2.96 / **2.56** 2.99 / **2.63** 3.02 / **2.69** **3.32** / 3.86 3.55 / **3.18** 4.55 / **4.08** **30.53** / **25.26**
**Elastic Memoryexp** **2.91** / 2.59 **2.93** / 2.66 **2.95** / **2.69** **3.32** / **3.84** **3.48** / 3.22 **4.37** / 4.10 30.57 / 25.27


**Setting and Results.** To verify that our model genuinely relies on its long-term memory, we conduct
an experiment where we intentionally corrupt the local context, thereby forcing the model to leverage
historical information. We replace a percentage of the Key and Value tokens within the current
block with random noise. We test two settings: â€œCorrupted,â€ where noise is injected only in the
memory-augmented layer, and â€œHardly Corrupted,â€ a more challenging setting where noise is injected
in all layers. As shown in Table 7, while all modelsâ€™ performance degrades under corruption, Elastic
Memory exhibits greater robustness, particularly in the more difficult â€œHardly Corruptedâ€ setting, in
terms of both PPL and LongPPL. This suggests that our memory mechanism provides a reliable source
of information that the model can effectively fall back on when local context is compromised.


**3.7. Speed Comparison**


**Table 8** | Speed comparison when scaling model or memory size. Test the average data volume per
second during training on Proof-Pile, normalized against Transformer++. Higher values are better.


**Scaling Memory** **Scaling Model**
**Model** **1x**
**2x** **4x** **8x** **16x** **2x** **4x**


**Transformer++** 1.000    -    -    -    - **0.880** **0.579**
**Mem. Transformer** **1.079** **1.074** **1.067** **1.056** **1.037** 0.853 0.565
**Infini-Transformer** 0.637     -     -     -     - 0.515 0.357
**Melodi** 0.548 0.580 0.580 0.572 0.534 0.529 0.366


**Elastic Memoryuni** 1.055 1.050 1.032 0.978 0.795 0.847 0.555
**Elastic Memoryexp** 1.057 1.053 1.033 0.976 0.794 0.850 0.557


**Setting and Results.** Finally, we evaluate the computational efficiency of Elastic Memory by measuring
its training throughput, normalized against the Transformer++ baseline. The results in Table 8
show that Elastic Memory maintains competitive training speed. At the 1x setting, its throughput is
close to both Transformer++ and Memorizing Transformer, and it remains significantly faster than
Infini-Transformer and Melodi. As memory capacity increases, throughput decreases, reflecting the
additional retrieval cost, but Elastic Memory remains competitive overall.

### 10


### Towards Compressive and Scalable Recurrent Memory **4. Related Work**

Our work builds upon a rich history of research in language model memory. We situate our contributions within three primary areas: external memory, associative memory, and summary tokens.


**External Memory.** A prominent line of work tackles fixed-context limits by augmenting Transformers
with an external memory decoupled from the parameters, enabling a potentially vast repository of
information. Transformer-XL (Dai et al., 2019) introduced segment-level recurrence, caching hidden
states from previous segments to form a sliding window of short-term memory and extend context
without recomputation. kNN-LM (Khandelwal et al., 2020) marked a shift to non-parametric memory:
the model retrieves similar instances from a large external datastore to augment predictions, yielding
long-term memory whose capacity scales with the datastore. Subsequent work refined this retrieval.
Memorizing Transformer (Wu et al., 2022) integrated kNN retrieval via a differentiable attention
layer, enabling end-to-end learning of when and how to use retrieved information. Recent efforts
target retrieval efficiency and quality: Focused Transformer (Tworkowski et al., 2023) mitigates
the â€œdistraction issueâ€â€”where irrelevant memories dilute contextâ€”using contrastive learning, and
LongMem (Wang et al., 2023) uses a decoupled side network to manage a memory bank while
keeping the main LLM frozen. Despite strong performance, retrieval remains the bottleneck: as
the store grows, low-latency, high-precision retrieval is hard, and maintaining and indexing large
memories incurs notable computational overhead.


**Associative Memory.** Another vein reframes self-attention as an associative memory to tame quadratic
compute and memory. Rooted in fast weight programming (Schmidhuber, 1992), the idea is to
generate context-dependent â€œfast weightsâ€ on the fly, compressing history into the network state.
DeltaNet (Schlag et al., 2021) shows linear attention as an efficient instantiation: memory accumulates
keyâ€“value outer products to approximate full attention with linear complexity. Gated DeltaNet
(Yang et al., 2025) adds LSTM-like gating to selectively forget outdated or irrelevant information.
Infini-Transformer (Munkhdalai et al., 2024) incorporates associative memory into vanilla attention
and uses the delta rule for more compressive updates, enabling infinitely long contexts with fixed
memoryâ€”promising for streaming. A core limitation, however, is capacity: the outer-product state
has fixed size (by model dimension and head count). As context grows, this constant-size memory
compresses more aggressively, increasing interference and degrading fine-grained retrieval, forming a
key bottleneck for truly scalable long-context modeling.


**Summary Tokens.** A complementary idea compresses history into a small set of summary tokens or
vectors, avoiding full-history retention. The condensed representation is passed between segments.
Block-Recurrent Transformers (Hutchins et al., 2022) add block-level recurrence with LSTM-style
gating, propagating a compressed state. Recurrent Memory Transformer (Bulatov et al., 2022)
appends learnable memory tokens per segment that serve as a dedicated â€œscratchpad,â€ whose final
states seed the next segment. AutoCompressor (Chevalier et al., 2023) learns fixed-size summary
vectors as soft prompts for past segments, conditioning future predictions. Melodi (Chen et al., 2025)
introduces hierarchical compression, producing short- and long-term summaries to reduce memory
while preserving multi-scale context. The main challenge is summary quality: compression creates an
information bottleneck, and effectiveness hinges on preserving salient information. What counts as
â€œsalientâ€ is task-dependentâ€”a summary tuned for one objective (e.g., perplexity) may not transfer to
others (e.g., QA)â€”hindering generalization.

### **5. Conclusion**


We present Elastic Memory, a memory design that brings together theory and practical scale for
long-context language models. Built on the HiPPO theory for online function approximation, it treats

### 11


### Towards Compressive and Scalable Recurrent Memory

memory as optimal compression of history, with a clear update rule and a flexible polynomial-sampling
retriever. It reaches state-of-the-art results on long-document benchmarks with zero extra parameters,
while using memory well and scaling efficiently. Bias-injection tests show the benefit of the decoupled
retriever, and context-corruption tests confirm real reliance on long-term history. These gains come
with competitive training throughput, making the method practical.

### **Acknowledgments**


This work is sponsored by the National Natural Science Foundation of China (NSFC) grant (No.
62576211) and the National Key Research and Development Program of China (No. 2023ZD0121402).

### **References**


Azerbayev, Z., Piotrowski, B., Schoelkopf, H., Ayers, E. W., Radev, D., and Avigad, J. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. _arXiv preprint arXiv:2302.12433_,
2023.


Bulatov, A., Kuratov, Y., and Burtsev, M. S. Recurrent memory transformer. In _Advances in Neural_
_Information Processing Systems (NeurIPS)_, volume 35, pp. 11079â€“11091, 2022.


Chen, Y., Hutchins, D., Jansen, A., Zhmoginov, A., Racz, D., and Andersen, J. MELODI: Exploring
memory compression for long contexts. In _International Conference on Learning Representations_
_(ICLR)_, 2025.


Chevalier, A., Wettig, A., Ajith, A., and Chen, D. Adapting language models to compress contexts. In
_Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)_,
pp. 3829â€“3846, 2023.


Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and Salakhutdinov, R. Transformer-XL: Attentive
language models beyond a fixed-length context. In _Proceedings of the 57th Annual Meeting of the_
_Association for Computational Linguistics (ACL)_, pp. 2978â€“2988, 2019.


Fang, L., Wang, Y., Liu, Z., Zhang, C., Jegelka, S., Gao, J., Ding, B., and Wang, Y. What is wrong
with perplexity for long-context language modeling? In _International Conference on Learning_
_Representations (ICLR)_, 2025. URL `[https://arxiv.org/abs/2410.23771](https://arxiv.org/abs/2410.23771)` . arXiv:2410.23771.


Gu, A., Dao, T., Ermon, S., Rudra, A., and RÃ©, C. HiPPO: Recurrent memory with optimal polynomial
projections. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 33, pp. 1474â€“
1487, 2020.


He, Z., Yang, M., Feng, M., Yin, J., Wang, X., Leng, J., and Lin, Z. Fourier transformer: Fast long range
modeling by removing sequence redundancy with FFT operator. _arXiv preprint arXiv:2305.15099_,
2023.


Hutchins, D., Schlag, I., Wu, Y., Dyer, E., and Neyshabur, B. Block-recurrent transformers. In _Advances_
_in Neural Information Processing Systems (NeurIPS)_, volume 35, pp. 33248â€“33261, 2022.


Kai, J., Zeng, B., Wang, Y., Bai, H., He, Z., Jiang, B., and Lin, Z. FreqKV: Frequency domain key-value
compression for efficient context window extension. _arXiv preprint arXiv:2505.00570_, 2025.


Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. Generalization through memorization: Nearest neighbor language models. In _International Conference on Learning Representations_
_(ICLR)_, 2020.

### 12


### Towards Compressive and Scalable Recurrent Memory

Meta AI. The llama 3 herd of models. _arXiv preprint arXiv:2407.21783_, 2024a. URL `[https:](https://arxiv.org/abs/2407.21783)`
`[//arxiv.org/abs/2407.21783](https://arxiv.org/abs/2407.21783)` .


Meta AI. Meta llama 3.1 8b model card. `[https://huggingface.co/meta-llama/Meta-Llama](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)`
`[-3.1-8B](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)`, 2024b.


Munkhdalai, T., Faruqui, M., and Gopal, S. Leave no context behind: Efficient infinite context
transformers with infini-attention. _arXiv preprint arXiv:2404.07143_, 2024.


Penedo, G., KydlÃ­Äek, H., Ben Allal, L., Lozhkov, A., Mitchell, M., Raffel, C., Von Werra, L., and Wolf,
T. The FineWeb datasets: Decanting the web for the finest text data at scale. In _Advances in Neural_
_Information Processing Systems (NeurIPS)_, volume 37, 2024. URL `[https://openreview.net/f](https://openreview.net/forum?id=n6SCkn2QaG)`
`[orum?id=n6SCkn2QaG](https://openreview.net/forum?id=n6SCkn2QaG)` .


Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., and Lillicrap, T. P. Compressive transformers
for long-range sequence modelling. In _International Conference on Learning Representations (ICLR)_,
2020. URL `[https://openreview.net/forum?id=SylKikSYDH](https://openreview.net/forum?id=SylKikSYDH)` .


Schlag, I., Irie, K., and Schmidhuber, J. Linear transformers are secretly fast weight programmers. In
_International Conference on Machine Learning (ICML)_, pp. 9355â€“9366. PMLR, 2021.


Schmidhuber, J. Learning to control fast-weight memories: An alternative to recurrent nets. _Neural_
_Computation_, 4(1):131â€“157, 1992.


Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and MiÅ‚oÅ›, P. Focused transformer: Contrastive training for context scaling. volume 36, pp. 42661â€“42688, 2023.


Wang, W., Dong, L., Cheng, H., Liu, X., Yan, X., Gao, J., and Wei, F. LongMem: Language models with
long-term memory. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 36,
2023.


Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. In _International_
_Conference on Learning Representations (ICLR)_, 2022.


Yang, S., Kautz, J., and Hatamizadeh, A. Gated delta networks: Improving Mamba2 with delta rule.
In _International Conference on Learning Representations (ICLR)_, 2025. arXiv:2412.06464.

### 13


### Towards Compressive and Scalable Recurrent Memory **A. Experimental Details**

**A.1. Training Infrastructure and Hyperparameters**


All experiments were conducted on a single compute node equipped with 8 NVIDIA A800 GPUs. We
provide the complete training configuration for reproducibility.


**Table 9** | Training hyperparameters used in all experiments.


**Hyperparameter** **Value**


**Optimizer** AdamW
**Learning Rate** 2e-3
**Adam** _ğ›½_ 1 0.9
**Adam** _ğ›½_ 2 0.95
**Adam** _ğœ–_ 1e-7
**Weight Decay** 0.1
**Max Gradient Norm** 1.0
**LR Scheduler** Cosine
**Warmup Ratio** 0.01
**Training Epochs** 20
**Batch Size per Device** 2
**Gradient Accumulation Steps** 1
**Context Length** 32,768
**Chunk Size** 2,048
**Precision** BF16
**Random Seed** 42


**Training Setup.** All models follow a unified Llama 3 architecture with modifications confined to the
attention or memory mechanisms. Each optimization step processes approximately 0.5M tokens,
with each sample containing 32,768 tokens. For memory-augmented layers, we input the full 32,768
tokens and process them in chunks of 2,048 tokens using the recurrent memory mechanism. All
attention modules use the FlashAttention2 implementation for efficiency. The complete training
hyperparameters are listed in Table 9.


**Table 10** | Raw training throughput (samples/second). Higher is better.


**Scaling Memory** **Scaling Model**
**Model** **1x**
**2x** **4x** **8x** **16x** **2x** **4x**


**Transformer++** 14.689     -     -     -     - **12.930** **8.500**
**Mem. Transformer** **15.844** **15.774** **15.677** **15.514** **15.237** 12.531 8.297
**Infini-Transformer** 9.359     -     -     -     - 7.572 5.249
**Melodi** 8.052 8.523 8.518 8.403 7.851 7.777 5.370


**Elastic Memoryuni** 15.503 15.420 15.153 14.370 11.673 12.445 8.152
**Elastic Memoryexp** 15.530 15.471 15.179 14.331 11.663 12.491 8.177


**Speed Measurement Methodology.** Table 8 reports training throughput normalized against Transformer++. We measure throughput using the `train_samples_per_second` metric reported by
Weights & Biases. The raw (unnormalized) training throughput values are provided in Table 10.


**Inference Throughput.** To provide a comprehensive efficiency comparison, we also report inference
throughput in Table 11, measured using the `eval/samples_per_second` metric from Weights &

### 14


### Towards Compressive and Scalable Recurrent Memory

**Table 11** | Raw inference throughput (samples/second). Higher is better.


**Scaling Memory** **Scaling Model**
**Model** **1x**
**2x** **4x** **8x** **16x** **2x** **4x**


**Transformer++** 6.212    -    -    -    - **6.379** **5.913**
**Mem. Transformer** **6.608** **6.544** **6.463** **6.523** **6.455** 6.287 5.873
**Infini-Transformer** 6.013     -     -     -     - 6.146 5.609
**Melodi** 5.552 5.558 5.646 5.738 5.653 5.698 5.078


**Elastic Memoryuni** 6.406 6.512 6.457 6.428 6.106 6.322 5.798
**Elastic Memoryexp** 6.498 6.480 6.444 6.367 6.126 6.338 5.848


Biases. Elastic Memory maintains an efficiency advantage during inference, though the gap with
baselines is narrower compared to training. We attribute this to the fact that Elastic Memory introduces
no additional learnable parameters compared to the base model, which simplifies the computational
graph during training and yields a more pronounced efficiency benefit. During inference, where
gradient computation is absent, this advantage is naturally reduced.


**Memory Size Calculation.** The memory size metrics (1x, 2x, ..., 16x) reported in our experiments
refer to the theoretical size of the recurrent memory module, as defined in Table 1. Specifically, the
baseline memory size (1x) is aligned with that of Infini-Transformer, whose memory capacity equals
_ğ‘‘_ key Ã— _ğ‘‘_ value Ã— _ğ»_ per layer. For Elastic Memory, memory size scales with the HiPPO dimension _ğ‘_ : at
1x, _ğ‘_ = 540; at 2x, _ğ‘_ = 1080; and so forth up to 16x with _ğ‘_ = 8640.


**A.2. Context Length Coverage**


A key distinction among memory models is the context length their memory can access:


  - **Memorizing Transformer** : Uses a fixed-size KV queue (FIFO). The memory size directly
determines context coverage. At 16x memory, it covers the full 32k context.

  - **Infini-Transformer, Melodi, Elastic Memory** : These compressive memory models can access
the _full 32k context_ at all memory sizes. The memory size determines compression fidelity, not
context coverage.


This distinction is crucial for interpreting Table 3: despite both accessing the full 32k context,
_Elastic Memoryexp_ at 1x achieves lower PPL than a 16x Memorizing Transformer, illustrating that
principled compression can be competitive even when raw context coverage is not the limiting factor.

### **B. Memory Reconstruction Quality**


To validate that Elastic Memory faithfully preserves historical information, we analyze the reconstruction quality of the HiPPO-compressed memory states from two perspectives: synthetic signal
reconstruction and language model Key/Value reconstruction.


**B.1. Synthetic Signal Reconstruction**


We first validate HiPPOâ€™s compression capability on synthetic signals with known ground truth. We
generate composite sine wave signals _ğ‘“_ ( _ğ‘¡_ ) = [ï¿½] _ğ‘–_ _[ğ´][ğ‘–]_ [sin][(][2] _[ğœ‹ğœ”][ğ‘–][ğ‘¡]_ [+] _[ğœ™][ğ‘–]_ [)][ with varying frequency compositions]
and compress them using HiPPO-LegS with different polynomial orders _ğ‘_ and discretization methods.
The compressed state is then used to reconstruct the original signal, and we measure reconstruction

### 15


### Towards Compressive and Scalable Recurrent Memory

MSE.


**Table 12** | HiPPO reconstruction quality on synthetic signals. Smooth signals (sine waves) achieve
near-perfect reconstruction, while random noise cannot be compressed effectively.


**Signal Type** **HiPPO Dim** _ğ‘_ **Sample Length** **Discretization** **Reconstruct MSE**


**1 sine wave** 32 1024 ZOH 1.2e-5
**3 sine waves** 32 1024 ZOH 2.3e-4
**5 sine waves** 32 1024 ZOH 9.8e-4


**5 sine waves** 32 1024 forward 3.3e-3
**5 sine waves** 32 1024 backward 2.9e-3
**5 sine waves** 32 1024 bilinear 4.8e-4


**random noise** 32 1024 ZOH 0.97
**random noise** 128 1024 ZOH 0.90
**random noise** 512 1024 ZOH 0.72


As shown in Table 12, HiPPO compression exhibits several properties:


  - **Faithful low-frequency preservation** : Smooth sine wave signals achieve near-perfect reconstruction (MSE âˆ¼ 10 [âˆ’][5] to 10 [âˆ’][4] ) even with small _ğ‘_ = 32.

  - **Robust degradation** : As signal complexity increases (more sine components), reconstruction
quality degrades smoothly rather than catastrophically.

  - **High-frequency noise are incompressible** : Random noise, which can be treated as containing
all frequencies, cannot be effectively compressed regardless of _ğ‘_ .


These properties validate that HiPPO provides a principled compression mechanism suitable for
preserving the smooth, low-frequency characteristics observed in language model hidden states.


**B.2. Language Model Key/Value Reconstruction**


We further analyze reconstruction quality on actual Key/Value sequences from our trained models.
We measure the Mean Squared Error (MSE) between the original Key/Value sequences and their
reconstructions from the compressed memory state.


**Table 13** | Reconstruction quality vs. performance on Proof-Pile. As HiPPO dimension _ğ‘_ increases,
reconstruction fidelity improves (lower MSE), corresponding to better language modeling performance
(lower PPL).


**Memory Size** **1x** **2x** **4x** **8x** **16x**


**HiPPO Dimension** _ğ‘_ 540 1080 2160 4320 8640
**Reconstruction MSE** 0.45 0.41 0.35 0.31 0.27
**PPL** 2.92 2.86 2.82 2.78 2.75


The results in Table 13 confirm that increasing _ğ‘_ monotonically improves reconstruction fidelity
without causing interference or overcompression. This validates that the polynomial basis provides
a well-behaved compression mechanism where additional capacity translates directly to improved
history representation.

### 16


### Towards Compressive and Scalable Recurrent Memory **C. Theoretical Grounding: Why HiPPO Works for Language Models**

A natural question arises: the HiPPO framework assumes continuous, smooth signals, yet language
model hidden states are discrete sequences. We address this apparent mismatch.


Recent empirical analyses reveal that the deep representations of language models exhibit significant smoothness and low-frequency characteristics in the sequence dimension. Kai et al. (2025)
analyzed the power spectrum of Llama-2-7Bâ€™s Key/Value representations using DCT transforms and
found that most energy concentrates in low-frequency components, with this concentration increasing in deeper layers. Similarly, He et al. (2023) observed analogous low-frequency dominance in
Transformer hidden states.


These findings suggest that deep Transformer representations, despite originating from discrete
tokens, exhibit the continuity properties that align with HiPPOâ€™s theoretical assumptions. Our design
choice to inject Elastic Memory at layer 9 (a deeper layer) is motivated by this observation. The strong
empirical performance across diverse datasets validates this theoretical alignment.

### **D. Discussion: Compression vs. Direct Sampling**


An intuitive alternative to our approach would be to directly sample tokens from the uncompressed
history (e.g., uniformly selecting one token per block). We argue that HiPPO compression provides
fundamental advantages over such direct sampling.


**Global Information Preservation.** When projecting history onto the HiPPO basis, the resulting state
**C** aggregates information from the _entire_ trajectory. This process acts as a spectral low-pass filter,
retaining the salient semantic structure while discarding high-frequency noise. In contrast, direct
sampling suffers from information erasure: any token falling between sample points is completely
lost (aliasing), creating blind spots in the modelâ€™s memory.


**Empirical Validation.** Our â€œCorrupted Contextâ€ experiments (Section 3.6) provide evidence for this
advantage. Under local context corruption, Elastic Memory exhibits greater robustness than baselines,
suggesting that the compressed representation provides a reliable information source that aggregates
neighborhood information rather than point samples.


**Efficiency Comparison.** The Memorizing Transformer represents an â€œupper boundâ€ of raw storage
(full FIFO queue). As shown in Table 3, _Elastic Memoryexp_ at 1x achieves lower PPL than a 16x
Memorizing Transformer, demonstrating that mathematically principled compression can be more
parameter-efficient than caching raw tokens.

### **E. Detailed Mathematical Derivations**


This appendix provides detailed mathematical derivations for readers interested in a deeper understanding of the HiPPO framework and our Elastic Memory formulation. We expand upon the
equations presented in Sections 2.2 and 2.3 of the main text.


**E.1. HiPPO-LegS State Space Derivation**


The HiPPO framework derives an Ordinary Differential Equation (ODE) that governs the evolution of
the polynomial coefficients. We provide the key steps here.

### 17


### Towards Compressive and Scalable Recurrent Memory

**Step 1: Defining the Optimization Objective.** At any time _ğ‘¡_, we seek a polynomial _ğ‘”_ [(] _[ğ‘¡]_ [)] ( _ğ‘¥_ ) =

- _ğ‘_ âˆ’1
_ğ‘›_ =0 _[ğ‘][ğ‘›]_ [(] _[ğ‘¡]_ [)] _[ğ‘”]_ _ğ‘›_ [(] _[ğ‘¡]_ [)] [(] _[ğ‘¥]_ [)][ that minimizes the approximation error:]



2

1

(11)
_ğ‘¡_ _[ğ‘‘ğ‘¥.]_







min
_ğ‘_ ( _ğ‘¡_ )



âˆ« _ğ‘¡_


0



_ğ‘“_ ( _ğ‘¥_ ) âˆ’



_ğ‘_ âˆ’1
âˆ‘ï¸

_ğ‘ğ‘›_ ( _ğ‘¡_ ) _ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [(] _[ğ‘¥]_ [)]

_ğ‘›_ =0



Since { _ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [}][ forms an orthonormal basis under the measure] _[ ğœ‡]_ [(] _[ğ‘¡]_ [)] [, the optimal coefficients are given by:]



_ğ‘ğ‘›_ ( _ğ‘¡_ ) = âŸ¨ _ğ‘“, ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [âŸ©] _ğœ‡_ [(] _[ğ‘¡]_ [)][ =] [1]

_ğ‘¡_



âˆ« _ğ‘¡_

_ğ‘“_ ( _ğ‘¥_ ) _ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [(] _[ğ‘¥]_ [)] _[ğ‘‘ğ‘¥.]_ (12)
0



**Step 2: Deriving the Time Derivative.** To obtain an incremental update rule, we differentiate _ğ‘ğ‘›_ ( _ğ‘¡_ )
with respect to _ğ‘¡_ . Using Leibnizâ€™s rule:



âˆ« _ğ‘¡_ 
_ğ‘“_ ( _ğ‘¥_ ) _ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [(] _[ğ‘¥]_ [)] _[ğ‘‘ğ‘¥]_
0



_ğ‘‘_ _[ğ‘‘]_
_ğ‘‘ğ‘¡_ _[ğ‘][ğ‘›]_ [(] _[ğ‘¡]_ [)][ =] _ğ‘‘ğ‘¡_




- 1


_ğ‘¡_



= âˆ’ [1]

_ğ‘¡_ [2]



_ğ‘¡_

_ğ‘“_ ( _ğ‘¥_ ) _ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [(] _[ğ‘¥]_ [)] _[ğ‘‘ğ‘¥]_ [+] [1]
0 _ğ‘¡_



âˆ« _ğ‘¡_




[1] _ğ‘›_ [(] _[ğ‘¡]_ [) +] [1]

_ğ‘¡_ _[ğ‘“]_ [(] _[ğ‘¡]_ [)] _[ğ‘”]_ [(] _[ğ‘¡]_ [)] _ğ‘¡_



âˆ« _ğ‘¡_



_ğ‘‘ğ‘¥._ (13)
_ğœ•ğ‘¡_



_ğ‘¡_



_ğ‘¡_

_ğ‘›_ [(] _[ğ‘¥]_ [)]
_ğ‘“_ ( _ğ‘¥_ ) _[ğœ•ğ‘”]_ [(] _[ğ‘¡]_ [)]
0 _ğœ•ğ‘¡_



The first term equals âˆ’ [1]



~~âˆš~~
_ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [(] _[ğ‘¡]_ [)][ =]



_ğ‘¡_ ~~âˆš~~

2 _ğ‘›_ + 1 _ğ‘ƒğ‘›_ (1) =



_ğ‘¡_ _[ğ‘][ğ‘›]_ [(] _[ğ‘¡]_ [)][. The second term involves evaluating the basis at the boundary, where]



2 _ğ‘›_ + 1 since _ğ‘ƒğ‘›_ (1) = 1 for all Legendre polynomials.



**Step 3: Computing the Basis Derivative.** The time derivative of the scaled Legendre basis is:



_ğœ•ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [(] _[ğ‘¥]_ [)] âˆš

=
_ğœ•ğ‘¡_


Substituting _ğ‘§_ = [2] _ğ‘¡_ _[ğ‘¥]_ [âˆ’] [1, we get:]



2 _ğ‘›_ + 1 Â· _ğ‘ƒğ‘›_ [â€²]




- 2 _ğ‘¥_



_ğ‘¥_ - 
       -       - [2] _[ğ‘¥]_
_ğ‘¡_ [âˆ’] [1] _ğ‘¡_ [2]



_ğ‘¡_ [2]







_._ (14)



_ğœ•ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [(] _[ğ‘¥]_ [)]

= âˆ’
_ğœ•ğ‘¡_



~~âˆš~~



( _ğ‘§_ + 1) _ğ‘ƒğ‘›_ [â€²] [(] _[ğ‘§]_ [)] _[.]_ (15)
_ğ‘¡_



2 _ğ‘›_ + 1



Using the Legendre polynomial derivative identity ( _ğ‘¥_ + 1) _ğ‘ƒğ‘›_ [â€²] [(] _[ğ‘¥]_ [)][ =] _[ ğ‘›ğ‘ƒ][ğ‘›]_ [(] _[ğ‘¥]_ [) + (][2] _[ğ‘›]_ [âˆ’] [1][)] _[ğ‘ƒ][ğ‘›]_ [âˆ’][1][(] _[ğ‘¥]_ [) + (][2] _[ğ‘›]_ [âˆ’]
5) _ğ‘ƒğ‘›_ âˆ’2( _ğ‘¥_ ) + Â· Â· Â·, the integral of the basis derivative contribution becomes:



1


_ğ‘¡_



_ğ‘¡_

_ğ‘›_ [(] _[ğ‘¥]_ [)]
_ğ‘“_ ( _ğ‘¥_ ) _[ğœ•ğ‘”]_ [(] _[ğ‘¡]_ [)]
0 _ğœ•ğ‘¡_



_ğ‘¡_



_ğ‘›_
âˆ‘ï¸

_ğ´_ Ëœ _ğ‘›ğ‘˜ğ‘ğ‘˜_ ( _ğ‘¡_ ) _,_ (16)

_ğ‘˜_ =0



âˆ« _ğ‘¡_




_[ğ‘¡]_ [(] _[ğ‘¥]_ [)]

_ğ‘‘ğ‘¥_ = âˆ’ [1]
_ğœ•ğ‘¡_ _ğ‘¡_



where _ğ´_ [Ëœ] _ğ‘›ğ‘˜_ captures only the contribution from the basis derivative:



(2 _ğ‘›_ + 1) [1][/][2] (2 _ğ‘˜_ + 1) [1][/][2] if _ğ‘›> ğ‘˜_

_ğ‘›_ if _ğ‘›_ = _ğ‘˜_
0 if _ğ‘›< ğ‘˜_



_ğ´_ Ëœ _ğ‘›ğ‘˜_ =



ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³



_._ (17)



Note that the diagonal element here is _ğ‘›_, not _ğ‘›_ + 1.


**Step 4: Combining All Contributions.** Recall that the first term from Step 2 contributes âˆ’ [1] _ğ‘¡_ _[ğ‘][ğ‘›]_ [(] _[ğ‘¡]_ [)][.]

When combined with the basis derivative contribution (which has diagonal element _ğ‘›_ ), the total
diagonal contribution becomes _ğ‘›_ + 1. Thus, the final HiPPO-LegS matrices are:



_ğ‘‘_
_ğ‘‘ğ‘¡_ _[ğ‘]_ [(] _[ğ‘¡]_ [)][ =][ âˆ’] [1] _ğ‘¡_




[1] [1]

_ğ‘¡_ **[A]** _[ğ‘]_ [(] _[ğ‘¡]_ [) +] _ğ‘¡_



(18)
_ğ‘¡_ **[B]** _[ ğ‘“]_ [(] _[ğ‘¡]_ [)] _[,]_


### 18


### Towards Compressive and Scalable Recurrent Memory

where the HiPPO-LegS matrices are given by:



_ğ´ğ‘›ğ‘˜_ =



(2 _ğ‘›_ + 1) [1][/][2] (2 _ğ‘˜_ + 1) [1][/][2] if _ğ‘›> ğ‘˜_

ï£±ï£´ï£´ï£´ï£²

_ğ‘›_ + 1 if _ğ‘›_ = _ğ‘˜_

ï£´ï£´ï£´ï£³0 if _ğ‘›< ğ‘˜_



_,_ _ğµğ‘›_ = (2 _ğ‘›_ + 1) [1][/][2] _._ (19)



Note that **A** is lower triangular, which reflects the causal nature of the memory system. The diagonal
element _ğ‘›_ + 1 arises from combining the measure derivative contribution (+1) with the basis derivative
contribution ( _ğ‘›_ ).


**E.2. Discretization via Zero-Order Hold**


To apply the continuous ODE to discrete sequences, we discretize using the Zero-Order Hold (ZOH)
method.


**ZOH Discretization.** For a continuous-time linear system ï¿½ _ğ‘_ ( _ğ‘¡_ ) = _ğ´ğ‘_ ( _ğ‘¡_ ) _ğ‘_ ( _ğ‘¡_ ) + _ğµğ‘_ ( _ğ‘¡_ ) _ğ‘“_ ( _ğ‘¡_ ) with time-varying
coefficients, the ZOH discretization assumes _ğ‘“_ ( _ğ‘¡_ ) is constant over each discrete interval [ _ğ‘¡ğ‘˜, ğ‘¡ğ‘˜_ +1). The
solution involves the state transition matrix Î¦( _ğ‘¡_ 2 _, ğ‘¡_ 1) = exp ï¿½âˆ« _ğ‘¡_ 1 _ğ‘¡_ 2 _[ğ´][ğ‘]_ [(] _[ğœ]_ [)] _[ğ‘‘ğœ]_ ï¿½:


âˆ« _ğ‘¡ğ‘˜_ +1
_ğ‘_ ( _ğ‘¡ğ‘˜_ +1) = Î¦( _ğ‘¡ğ‘˜_ +1 _, ğ‘¡ğ‘˜_ ) _ğ‘_ ( _ğ‘¡ğ‘˜_ ) + Î¦( _ğ‘¡ğ‘˜_ +1 _, ğœ_ ) _ğµğ‘_ ( _ğœ_ ) _ğ‘‘ğœ_               - _ğ‘“ğ‘˜._ (20)


_ğ‘¡ğ‘˜_



**Application to HiPPO-LegS.** For HiPPO-LegS where _ğ´ğ‘_ ( _ğ‘¡_ ) = âˆ’ [1]




[1] _ğ‘¡_ **[A]** [ and] _[ ğµ][ğ‘]_ [(] _[ğ‘¡]_ [)][ =] [1] _ğ‘¡_



**Application to HiPPO-LegS.** For HiPPO-LegS where _ğ´ğ‘_ ( _ğ‘¡_ ) = âˆ’ _ğ‘¡_ **[A]** [ and] _[ ğµ][ğ‘]_ [(] _[ğ‘¡]_ [)][ =] _ğ‘¡_ **[B]** [, we first compute]

the state transition matrix. With _ğ‘¡ğ‘˜_ = _ğ‘˜_ (unit time steps), we have:

âˆ« _ğ‘˜_ +1 âˆ« _ğ‘˜_ +1          - _ğ‘˜_          


_ğ‘˜_ +1 âˆ« _ğ‘˜_ +1

_ğ´ğ‘_ ( _ğœ_ ) _ğ‘‘ğœ_ =

_ğ‘˜_ _ğ‘˜_



_ğ‘˜_ +

  - [1]
_ğ‘˜_ _ğœ_




[1] - _ğ‘˜_ + 1

_ğœ_ **[A]** _[ ğ‘‘ğœ]_ [=][ âˆ’] **[A]** [ ln] _ğ‘˜_







_._ (21)



_ğ‘˜_



Therefore, the discrete state matrix is:


                        -                         - _ğ‘˜_ + 1
**A** Â¯ _ğ‘˜_ = exp           - **A** ln

_ğ‘˜_


For the input matrix, we compute:



ï¿½ï¿½ - _ğ‘˜_
=
_ğ‘˜_ + 1




- **A**
_._ (22)



_ğ‘˜_ +1 âˆ« _ğ‘˜_ +1

Î¦( _ğ‘˜_ + 1 _, ğœ_ ) _ğµğ‘_ ( _ğœ_ ) _ğ‘“ğ‘˜_ _ğ‘‘ğœ_ =

_ğ‘˜_ _ğ‘˜_



âˆ« _ğ‘˜_ +1
**B** Â¯ _ğ‘˜_ _ğ‘“ğ‘˜_ =




- **A**

   - [1] (23)

_ğœ_ **[B]** _[ ğ‘“][ğ‘˜]_ _[ğ‘‘ğœ.]_



_ğ‘˜_




- _ğœ_
_ğ‘˜_ + 1



Evaluating this integral:



1
**B** Â¯ _ğ‘˜_ _ğ‘“ğ‘˜_ =
( _ğ‘˜_ + 1) **[A]**



âˆ« _ğ‘˜_ +1

_ğœ_ **[A]** [âˆ’] **[I]** _ğ‘‘ğœ_   - **B** _ğ‘“ğ‘˜_

_ğ‘˜_



1
= ( _ğ‘˜_ + 1) **[A]** [Â·] **[ A]** [âˆ’][1][ ï¿½] ( _ğ‘˜_ + 1) **[A]** - _ğ‘˜_ **[A]** [ï¿½] **B** _ğ‘“ğ‘˜_




    = **A** [âˆ’][1]




  - _ğ‘˜_
**I** _ğ‘˜_ + 1



**B** _ğ‘“ğ‘˜._ (24)




- **A** [ï¿½]



Thus, the discrete input matrix is:
**B** Â¯ _ğ‘˜_ = **A** [âˆ’][1][ ï¿½] **I**               - **A** Â¯ _ğ‘˜_               - **B** _._ (25)


Note that this follows from the standard ZOH formula for time-varying systems, where the sign is
( **I** - **A** [Â¯] _ğ‘˜_ ) rather than ( **A** [Â¯] _ğ‘˜_ - **I** ) due to the negative sign in _ğ´ğ‘_ ( _ğ‘¡_ ) = âˆ’ [1] _ğ‘¡_ **[A]** [.]


These position-dependent matrices capture the time-varying dynamics of the HiPPO-LegS system.

### 19


### Towards Compressive and Scalable Recurrent Memory

**E.3. Block-Level Parallelization**


We now derive the parallel block update formulation used in Elastic Memory.


**Unrolling the Recurrence.** Starting from the discrete recurrence _ğ‘ğ‘˜_ = **A** [Â¯] _ğ‘˜ğ‘ğ‘˜_ âˆ’1 + **B** [Â¯] _ğ‘˜_ _ğ‘“ğ‘˜_, we unroll over a
block of _ğ¿_ steps:


_ğ‘_ 0 = **A** [Â¯] 0 _ğ‘_ âˆ’1 + **B** [Â¯] 0 _ğ‘“_ 0

_ğ‘_ 1 = **A** [Â¯] 1 _ğ‘_ 0 + **B** [Â¯] 1 _ğ‘“_ 1 = **A** [Â¯] 1 **A** [Â¯] 0 _ğ‘_ âˆ’1 + **A** [Â¯] 1 **B** [Â¯] 0 _ğ‘“_ 0 + **B** [Â¯] 1 _ğ‘“_ 1

_..._




 - _ğ‘—_ +1 
 - **A** Â¯ _ğ‘˜_


_ğ‘˜_ = _ğ¿_ âˆ’1







_ğ¿_ âˆ’1
âˆ‘ï¸


_ğ‘—_ =0



**B** Â¯ _ğ‘—_ _ğ‘“_ _ğ‘—._ (26)



_ğ‘ğ¿_ âˆ’1 =




- 0

 - **A** Â¯ _ğ‘˜_


_ğ‘˜_ = _ğ¿_ âˆ’1



_ğ‘_ âˆ’1 +



**Matrix Formulation.** Define the state transition matrix **P** = [ï¿½][0] _ğ‘˜_ = _ğ¿_ âˆ’1 **[A]** [Â¯] _[ğ‘˜]_ [and the HiPPO kernel][ Â¯] **[K]** [ âˆˆ] [â„] _[ğ‘]_ [Ã—] _[ğ¿]_



with columns:







**K** Â¯: _,ğ‘—_ =




- _ğ‘—_ +1

 - **A** Â¯ _ğ‘˜_


_ğ‘˜_ = _ğ¿_ âˆ’1



**B** Â¯ _ğ‘—._ (27)



The block update then becomes a single matrix operation:


**C** new = **PC** old + **KF** [Â¯] _,_ (28)


where **F** = [ _ğ‘“_ 0 _, ğ‘“_ 1 _, . . ., ğ‘“ğ¿_ âˆ’1] _[ğ‘‡]_ âˆˆ â„ _[ğ¿]_ [Ã—] _[ğ‘‘]_ is the input matrix.


**E.4. Polynomial Reconstruction**


The memory retrieval leverages the reconstruction property of polynomial approximation.


**Reconstruction Formula.** Given the coefficient vector _ğ‘_ ( _ğ‘¡_ ) âˆˆ â„ _[ğ‘]_ at time _ğ‘¡_, the approximated signal
value at any point _ğ‘¥_ âˆˆ[0 _, ğ‘¡_ ] is:



_ğ‘_ âˆ’1
âˆ‘ï¸



_ğ‘_ ~~âˆš~~
âˆ‘ï¸

_ğ‘ğ‘›_ ( _ğ‘¡_ )

_ğ‘›_ =0



2 _ğ‘›_ + 1 _ğ‘ƒğ‘›_



_ğ‘“_ Ë†( _ğ‘¥_ ) = _ğ‘”_ [(] _[ğ‘¡]_ [)] ( _ğ‘¥_ ) =



_ğ‘_ âˆ’1
âˆ‘ï¸

_ğ‘ğ‘›_ ( _ğ‘¡_ ) _ğ‘”ğ‘›_ [(] _[ğ‘¡]_ [)] [(] _[ğ‘¥]_ [)][ =]

_ğ‘›_ =0




- 2 _ğ‘¥_ 
_ğ‘¡_ [âˆ’] [1]



_._ (29)



**Reconstruction Matrix.** To retrieve _ğ¿_ mem sample points { _ğ‘¥_ 0 _, ğ‘¥_ 1 _, . . ., ğ‘¥ğ¿_ mem âˆ’1} simultaneously, we
construct the reconstruction matrix **R** âˆˆ â„ _[ğ¿]_ [mem] [Ã—] _[ğ‘]_ :



~~âˆš~~
_ğ‘…_ _ğ‘—ğ‘›_ = 2 _ğ‘›_ + 1 _ğ‘ƒğ‘›_




 - 2 _ğ‘¥_ _ğ‘—_ 
  - 1
_ğ‘¡_




 - 2 _ğ‘¥_ _ğ‘—_



_._ (30)



The reconstructed values are then obtained via matrix multiplication:


**F** Ë† = **RC** _,_ (31)


where **F** [Ë†] âˆˆ â„ _[ğ¿]_ [mem] [Ã—] _[ğ‘‘]_ contains the reconstructed feature vectors at each sample point.


**Sampling Strategies.** The choice of sample points { _ğ‘¥_ _ğ‘—_ } determines the reconstruction matrix and
thus the information retrieved:


 - **Uniform** : _ğ‘¥_ _ğ‘—_ = _ğ‘—_  - _ğ¿_ mem _ğ‘¡_ [for] _[ ğ‘—]_ [=][ 0] _[,]_ [ 1] _[, . . ., ğ¿]_ [mem][ âˆ’] [1.]

 - **Exponential** : _ğ‘¥_ _ğ‘—_ = _ğ‘¡_  - (1 âˆ’ _ğ›¼_ _[ğ¿]_ [mem] [âˆ’][1][âˆ’] _[ğ‘—]_ ) where _ğ›¼_ âˆˆ(0 _,_ 1) controls the decay rate.


Both strategies yield valid reconstructions; the choice depends on the desired attention pattern over
history.

### 20


