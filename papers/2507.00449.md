## **Overcoming Long-Context Limitations of State-Space** **Models via Context-Dependent Sparse Attention**

**Zhihao Zhan** **[1,2,]** _[â€ ]_ **, Jianan Zhao** **[1,2,]** _[â€ ]_ **, Zhaocheng Zhu** **[1,2]** **, Jian Tang** **[1,3,4,*]**

1Mila - QuÃ©bec AI Institute, 2University of MontrÃ©al 3HEC MontrÃ©al, 4CIFAR AI Chair

_â€ _ Equal contribution *Correspondence: tangjian@mila.quebec


**Abstract**


Efficient long-context modeling remains a critical challenge for natural language
processing (NLP), as the time complexity of the predominant Transformer architecture scales quadratically with the sequence length. While state-space models
(SSMs) offer alternative sub-quadratic solutions, they struggle to capture longrange dependencies effectively. In this work, we focus on analyzing and improving
the long-context modeling capabilities of SSMs. We show that the widely used
synthetic task, associative recall, which requires a model to recall a value associated with a single key without context, insufficiently represents the complexities
of real-world long-context modeling. To address this limitation, we extend the
associative recall to a novel synthetic task, _joint recall_, which requires a model to
recall the value associated with a key given in a specified context. Theoretically, we
prove that SSMs do not have the expressiveness to solve multi-query joint recall in
sub-quadratic time complexity. To resolve this issue, we propose a solution based
on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which has
the expressiveness to solve multi-query joint recall with sub-quadratic computation.
To bridge the gap between theoretical analysis and real-world applications, we
propose locality-sensitive Hashing Attention with sparse Key Selection (HAX),
which instantiates the theoretical solution and is further tailored to natural language
domains. Extensive experiments on both synthetic and real-world long-context
benchmarks show that HAX consistently outperforms SSM baselines and SSMs
integrated with context-independent sparse attention (CISA). Our code is available
at: `[https://github.com/DeepGraphLearning/HAX](https://github.com/DeepGraphLearning/HAX)` .


**1** **Introduction**


Long-context modeling is a central challenge in natural language processing (NLP), which underpins a
variety of applications, such as document summarization, question answering, and machine translation

[30]. Recent advances in large language models (LLMs) have broadened the landscape of long-context
modeling, enabling new capabilities such as autonomous agents, retrieval-augmented generation,
dialogue systems, and long-context reasoning [22]. This growing demand has spurred intensive
research into algorithms that can efficiently and effectively capture long-range dependencies [37].


Currently, the Transformer architecture [38] is the dominant paradigm in sequence modeling. However, its applicability to long sequences is fundamentally constrained by the required computation that
grows quadratically with the sequence length. This motivates the research direction for the invention
of efficient architectures.


Recently, state-space models (SSMs) [10, 16, 31] have emerged as a potential alternative solution,
offering sub-quadratic time complexity as well as comparable performance to Transformers on
short-context NLP tasks [14, 4]. However, empirical evidence suggests that SSMs are less effective


39th Conference on Neural Information Processing Systems (NeurIPS 2025).


_**Associative Recall**_
does _not_ specify context.


_**Joint Recall**_
specifies context.



âœ“ The oceans are blue, and snow is white.


White!
What color is snow?


âœ“ In the U.S., people pay with dollars and drive cars on the right side of the road.
âœ“ In the U.K., people pay with pounds and drive cars on the left side of the road.



Which side of the road

do people drive on

in the U.S.?



Right!



Figure 1: Comparison of joint recall and associative recall. Associative recall does not account for
context. _Joint recall_ extends associative recall by incorporating _context-dependency_ into key-value
associations. For example, while associative recall may map "pay with" to either "dollar" or "pound",
joint recall allows it to map to "dollar" in the U.S. and "pound" in the U.K., depending on context.
This makes joint recall a more realistic and rigorous synthetic task for both theoretical analysis and
empirical benchmark for long-context modeling.


than Transformers in capturing long-range dependencies [40]. Furthermore, theoretical analysis by

[19] demonstrates that SSMs are much less capable of handling long-context copying, due to the
limitations of architecture representation capacity.


In this work, we aim to better understand and improve the long-context modeling abilities of SSMs.
We first show that previous studies based on the widely used synthetic task, associative recall [2],
might be constrained by its limited capability to simulate natural language in-context dependencies.
To be specific, associative recall assumes that each key is uniquely associated with a value, regardless
of the surrounding context. However, natural language often defies this assumption: the same key
can correspond to different values depending on its context. Consider the example in Figure 1, when
asked on which side of the road people drive, the correct answer should depend on the country being
referenced. Without specifying whether the context is the US or the UK, the question becomes
ambiguous. This example highlights a critical shortcoming of associative recall: it lacks the capacity
to simulate context-dependent key-value association, which is very common in natural language.


To address this limitation, we extend the associative recall to a more general synthetic task, joint
recall. Unlike associative recall, joint recall requires the model to retrieve a value corresponding
to a key conditioned on a specified context. Theoretically, we prove that standard SSMs lack the
representational capacity to solve multi-query joint recall under sub-quadratic time complexity.


To overcome this expressiveness bottleneck, we propose to augment SSMs with Context-Dependent
Sparse Attention (CDSA), a class of sparse attention with sparse attention patterns that are conditioned
on the context representations. Locality-sensitive hashing (LSH) attention [20] exemplifies CDSA,
while context-independent sparse attention (CISA) includes sliding window attention, A-shaped
attention, and dilated attention [6]. Compared to CISA, CDSA enables dynamic content-dependent
routing of information, which is essential to efficiently solve the multi-query joint recall task. We
theoretically show that there exists a CDSA which, when integrated with SSMs, enables solving the
multi-query joint recall task in sub-quadratic time with respect to sequence length. Moreover, we
establish an expressiveness gap between SSMs integrated with CDSA and SSMs integrated with
CISA on multi-query joint recall.


Building upon this insight and to bridge the gap between theory and practice, we propose a novel
architecture: locality-sensitive Hashing Attention with sparse Key Selection (HAX). HAX improves
the expressiveness of LSH attention by incorporating our proposed Key Selection (KS) attention,
and is further integrated with state-of-the-art SSMs, Mamba and Mamba2 [14, 4], instantiating the
theoretically grounded solution. We validate the effectiveness of HAX through extensive experiments
on both synthetic and real-world long-context modeling benchmarks. The experiment results show
that HAX consistently outperforms SSM baselines as well as SSMs augmented with CISA. These
findings demonstrate that CDSA, when carefully integrated with SSMs, is a critical component in
unlocking their potential for long-context modeling.


2


Our main contributions are summarized as follows:


1. We introduce _joint recall_, a novel synthetic task that extends associative recall to contextdependent key-value association, which offers a new perspective for both theoretical analysis
and empirical benchmark for long-context modeling.
2. Through theoretical analysis on _joint recall_, we demonstrate that integrating state-space models
(SSMs) with context-dependent sparse attention (CDSA) has the expressiveness to solve multiquery joint recall with sub-quadratic computation.
3. Guided by this theoretical insight, we propose a novel architecture, HAX, based on SSM
integrated with CDSA, which consistently outperforms SSMs and SSMs integrated with contextindependent sparse attention (CISA) on both synthetic and real-world long-context benchmarks.


**2** **Preliminaries**


In this section, we introduce two prominent approaches for efficient architecture design: sparse
attention in Sec. 2.1 (exemplified by LSH attention in Sec. 2.2) and SSMs in Sec. 2.3. We also
introduce associative recall, a widely-used synthetic benchmark for long-context modeling, in Sec. 2.4.


**2.1** **Sparse Attention**


For a sequence of length _l_, we denote the attention scores of auto-regressive sequence modeling as:


**A** = Softmax( **M** _âŠ™_ **QK** _[âŠ¤]_ ) (1)


where **M** _âˆˆ{_ 0 _,_ 1 _}_ _[l][Ã—][l]_ is the auto-regressive mask, **Q** _,_ **K** _âˆˆ_ R _[l][Ã—][d]_, _d_ is the hidden dimension. In this
work, we define the attention scores for sparse attention as:


**A** = Softmax( **S** _âŠ™_ **M** _âŠ™_ **QK** _[âŠ¤]_ ) (2)


with **S** _âˆˆ{_ 0 _,_ 1 _}_ _[l][Ã—][l]_ representing the sparse attention pattern. We consider the sparsity constraint:


_âˆ¥_ **S** _âˆ¥_ 0 _â‰ª_ _l_ [2] (3)


To ensure per-step computational efficiency, we further tighten this constraint by requiring:


_âˆ€i, âˆ¥_ **S** _iâˆ¥_ 0 _â‰ª_ _l_ (4)


where **S** _i_ denotes the _i_ -th row of **S** . This implies that each query attends to at most _k_ keys, _k â‰ª_ _l_ .


**2.2** **Locality-Sensitive Hashing Attention**


Given that locality-sensitive hashing (LSH) attention represents one of the most effective inputdependent sparse attention for auto-regressive modeling [20], we reformulate a simple algorithm to
generate the sparse attention pattern of LSH. This algorithm accepts the query and key matrices **Q**
and **K** as input and outputs a binary sparse attention pattern **S** LSH. At each forward pass, **Q** and **K**
are first centralized and normalized to **Q** [Ëœ] and **K** [Ëœ], respectively:


**Q** Ëœ _i_ = normalize( **Q** _i âˆ’_ **Q** Â¯ _i_ ) _,_ **K** Ëœ _i_ = normalize( **K** _i âˆ’_ **K** Â¯ _i_ ) (5)


i.i.d.
Next, a random projection matrix **H** _âˆ¼N_ (0 _,_ 1) _âˆˆ_ R _[d][Ã—][h]_ is sampled to project the normalized
vectors **Q** [Ëœ] and **K** [Ëœ] into the hash space. Then, we consider two binning rules which assign each vector
**Q** Ëœ _i_ and Ëœ **K** _i_ to a hash bin: the argmax binning rule [20, 35] and the sign-bit binning rule [8, 5].


The argmax binning rule assigns each vector to the index of its most aligned column in **H** :

bin _Qi_ = argmax( **Q** [Ëœ] _i_ **H** ) _,_ bin _Ki_ = argmax( **K** [Ëœ] _i_ **H** ) (6)


The sign-bit binning rule constructs a binary hash code by computing the signs of the projected values
and interpreting it as a binary number:



_h_

- **1** [( **K** [Ëœ] _i_ **H** ) _j >_ 0] _Â·_ 2 _[h][âˆ’][j]_ (7)


_j_ =1



bin _Qi_ =



_h_

- **1** [( **Q** [Ëœ] _i_ **H** ) _j >_ 0] _Â·_ 2 _[h][âˆ’][j]_ _,_ bin _Ki_ =

_j_ =1


3


The argmax binning rule assigns vectors to _h_ bins, while the sign-bit binning rule assigns vectors to
2 _[h]_ bins. We will further discuss the relationship between these two binning strategies in Appendix A.
Based on the assigned bins, a preliminary sparse pattern **S** [Ëœ] LSH is constructed by allowing each query
to attend to all preceding keys within the same bin:
**S** ËœLSH _ij_ = **1** [bin _Qi_ = bin _Kj_ ] (8)
Finally, to satisfy the sparsity constraint defined in Eq. 4, a per-bin sliding window mask **M** LSH is
applied, so that each query only attends to at most _k_ nearest keys in the same bin:

**S** LSH = **M** LSH _âŠ™_ **S** [Ëœ] LSH (9)


**2.3** **Generalized State-Space Model**


Following the definitions introduced by [19], we formulate generalized state-space models as sequence
models defined by an update rule _u_ : _U Ã— V â†’U_ and an output function _r_ : _U â†’V_, where _V_
denotes the token vocabulary and _U_ represents the recurrent state. Let _U_ 0(âˆ…) _âˆˆU_ denote the initial
state. Given an input sequence _v_ 1 _, ..., vn âˆˆV_, for _i_ in _{_ 1 _...n}_, the state _Ui_ ( _v_ 1 _, ..., vi_ ) _âˆˆU_ and its
corresponding output _Ri_ ( _v_ 1 _, ..., vi_ ) _âˆˆV_ are defined recursively as:
_Ui_ ( _v_ 1 _, ..., vi_ ) = _u_ ( _Uiâˆ’_ 1( _v_ 1 _, ..., viâˆ’_ 1) _, vi_ ) (10)
_Ri_ ( _v_ 1 _, ..., vi_ ) = _r_ ( _Ui_ ( _v_ 1 _, ..., vi_ )) (11)


**2.4** **Associative Recall**


The associative recall task was originally introduced in [2]. [27] found that the LLM performance on
this task is strongly correlated with their in-context learning abilities . [1] extended associative recall
to the multi-query setting: a model is first given a sequence of associated key-value pairs, and then
required to recall each value when queried with the corresponding key. Associative recall has been
widely adopted as a synthetic benchmark for long-context modeling [4, 18].


**3** **Joint Recall**


We discuss the motivation and formulation of joint recall in Sec. 3.1 and Sec. 3.2, respectively, and
finally provide the theoretical results in Sec. 3.3.


**3.1** **Motivation**



The motivation behind proposing joint recall is to
overcome a key limitation of the setup of associative recall: each key corresponds to a single fixed
value. While this setup is well-suited for studying
the tasks that emphasize capturing stable lexical
patterns, such as sub-word units or fixed multiword expressions, it falls short in representing the
context-sensitive nature of meaning in natural language. Consider the following examples:


  - The legislative branch of the U.S. government is called Congress.

  - On Monday mornings, Alice studies math.




|Multi-Query Associative Recall recalls each value associated with a single key without context:|Col2|
|---|---|
|Multi-Query Associative Recall|Multi-Query Associative Recall|
|Input|a 5 b 2 c 3 d 1 e 4 | c ? e ? a ? d ? b ?|
|Output|3 4 5 1 2|


|Multi-Query Joint Recall recalls each value associated with a key given in a specified context:|Col2|
|---|---|
|Multi-Query Joint Recall|Multi-Query Joint Recall|
|Input|A a 3 b 2 B b 4 a 1 | B a ? b ? A b ? a ?|
|Output|1 4 2 3|



From a philosophical perspective, definitions are Figure 2: Comparison between synthetic
often constructed using genus keys and differentia _multi-query joint recall_ and associative recall.
context. In the first example, the value "Congress"
is identified with the genus key "the legislative branch" and the differentia context "of the U.S. government". The second example reflects a more daily scenario, where the value "math" is identifiable
only when all contextual elements, "Monday" and "morning", are considered together with the key
"Alice". These cases illustrate that accurate semantic interpretation in natural language often requires
integrating context and keys, suggesting that models must move beyond the simplistic one-to-one
mappings of associative recall to capture the compositional and context-dependent nature of meaning.
This observation motivates our introduction of a novel synthetic task, which we refer to as joint recall.


4


**3.2** **Formulation**


Associative recall requires a model to memorize _nk_ associated key-value pairs. Joint recall generalizes
this task: the model is required to memorize an _ncÃ—nk_ table of context-specific key-value associations,
in which _nk_ keys are associated with different values in each of the _nc_ contexts. Inspired by [1], we
also extend joint recall to a multi-query setting, requiring the model to recover the entire table instead
of a specific entry in the table.


Figure 2 illustrates multi-query joint recall with _nc_ = 2 and _nk_ = 2. Following the structure of
natural language, the sequentialized table input consists of _nc_ context blocks, each beginning with
a context token (e.g. uppercase letter in Figure 2), followed by _nk_ key-value pairs specific to that
context (e.g. lowercase-letter-digit pairs in Figure 2). Then, the model is asked to recall the associated
values given each context-key pair, under arbitrary permutations of the context and key ordering.


Appendix C further extends the joint recall formulation to multi-level context: for example, in the
sentence "On Monday mornings, Alice studies math.", â€œMondayâ€ and â€œmorningâ€ are contexts at
hierarchical levels. It also provides theoretical analyses grounded in this extended formulation.


**3.3** **Theoretical Analysis**


**3.3.1** **Categorization of sparse attention**


The sparse attention pattern **S** defined in Sec. 2.1 can be categorized as context-dependent or contextindependent, depending on whether it is predetermined or dynamically inferred from the context
representations. Context-independent sparse attention (CISA) patterns, such as sliding window
attention, A-shaped attention, and dilated attention [6], are fixed regardless of context. In contrast,
context-dependent sparse attention (CDSA) patterns, exemplified by LSH attention [20], adapt
according to the context representation. Appendix Figure 8 provides an illustration of both categories.


**3.3.2** **Limited expressiveness of SSMs**


As a corollary of Theorem 2.7 in [19], we demonstrate that solving the multi-query joint recall task
imposes a linear growth requirement on the state dimension of SSMs with respect to the number of
entries _n_ in the joint recall table, _n_ = _nc Ã— nk_ . Let _|U|_ be the number of distinct representations that
the recurrent state space _U_ can encode, for a state with _b_ bits of capacity, _|U|_ = 2 _[b]_ . We define the
uniform multi-query joint recall distribution as the distribution in which all values are sampled i.i.d.
from the uniform distribution over the token vocabulary _V_ . In this setting, we obtain the following:

**Corollary 3.1** (Limited expressiveness of SSMs) **.** _Under the uniform multi-query joint recall distri-_
_bution, for any n, a generalized state-space model defined in Sec. 2.3 incurs an error rate of at least_
_|U|_
1 _âˆ’_ _|V|_ _[n]_ _[.]_


_Remark 3.1._ To guarantee Pr[ _err_ ] = 0, it is necessary that the number of representable states satisfies
_|U| â‰¥|V|_ _[n]_ . Taking the logarithm of both sides yields the condition _b â‰¥_ _n_ log _|V|_ . This implies that
the state-space dimension of the model must grow linearly with the number of entries _n_ in the joint
recall table, highlighting a fundamental limitation of the representation capacity of SSMs.


**3.3.3** **Improved expressiveness of SSMs integrated with CDSA**


For SSMs integrated with sparse attention, we establish the following results:

**Proposition 3.2** (Improved expressiveness of SSMs integrated with CDSA) **.** _There exists a_ 2 _-layer_
_auto-regressive hybrid model consisting of an SSM layer followed by an LSH attention layer, which_
_can solve multi-query joint recall in O_ ( _n_ log [2] _n_ ) _time complexity with O_ (log _n_ ) _SSM state dimensions._


**Proposition 3.3** (Limited expressiveness of SSMs integrated with CISA) **.** _There does not exist a_
2 _-layer auto-regressive hybrid model consisting of an SSM layer followed by a CISA layer, which_
_can solve multi-query joint recall with o_ ( _n_ [2] ) _time complexity, since it requires at least O_ ( _[n]_ _k_ [)] _[ SSM]_

_state dimensions, k is the maximum number of keys that each query allowed to attend to in the sparse_
_attention module, as defined in Eq. 4._


_Remark 3.2._ Comparing **Proposition** 3.2 with **Proposition** 3.3, we see a clear representation capacity
gap between the SSMs integrated with CDSA and the SSMs integrated with CISA.


5


LSH attention Key Selection (KS) attention



1. Assign each query / key to a hash bin.
2. Calculate attention within each bin.


K bin id



1. Calculate a score for each key.
2. Queries attend to top-scored keys.

|Col1|5|8|3|1|4|6|9|2|7|
|---|---|---|---|---|---|---|---|---|---|
|ğ‘ºğŠğ’<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||
|ğ‘ºğŠğ’<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||
|ğ‘ºğŠğ’<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||
|ğ‘ºğŠğ’<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||
|ğ‘ºğŠğ’<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||
|ğ‘ºğŠğ’<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||
|ğ‘ºğŠğ’<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||



K idx

Memorizes important keys.









1
2
3
3
2
1
2
1
3









Q bin id






|Col1|2|1|3|1|1|3|2|2|2|
|---|---|---|---|---|---|---|---|---|---|
|ğ‘ºğ‹ğ’ğ‡<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||
|ğ‘ºğ‹ğ’ğ‡<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||
|ğ‘ºğ‹ğ’ğ‡<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||
|ğ‘ºğ‹ğ’ğ‡<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||
|ğ‘ºğ‹ğ’ğ‡<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||
|ğ‘ºğ‹ğ’ğ‡<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||
|ğ‘ºğ‹ğ’ğ‡<br>âˆˆ{0,1}ğ‘™Ã—ğ‘™||||||||||



K idx

Handles interactions in context.


|Col1|Col2|
|---|---|
|`Conv`<br>`Ïƒ`<br>`SSM`<br>`+`<br>`Ïƒ`<br>Ã—<br>`A`<br>`B C`<br>`X`<br>`Y`<br>**`HAX`**|`Conv`<br>`Ïƒ`<br>`SSM`<br>`+`<br>`Ïƒ`<br>Ã—<br>`A`<br>`B C`<br>`X`<br>`Y`<br>**`HAX`**|
|||





Integrating **HAX** with Mamba



Figure 3: Illustration of the HAX architecture, and Mamba [14] integrated with HAX.


_Remark 3.3._ In practice, with an appropriate constant _k_, integrating CISA with SSMs still provides
an advantage: unlike SSMs, which only have access to the last state representation, CISA layers can
attend to _k_ different state representations simultaneously, at a cost of _k_ times of computation budget.


Complete proofs are provided in the Appendix B.


**4** **Method**


Guided by the theoretical analysis in Sec. 3.3, we propose a new architecture, locality-sensitive
Hashing Attention with sparse Key Selection (HAX). HAX improves the expressiveness of LSH
attention by incorporating our proposed Key Selection (KS) attention, and is further integrated with
state-of-the-art SSMs, serving as an instantiation of context-dependent sparse attention (CDSA)
integrated with SSMs, thereby benefiting from the theoretical advantages discussed in Sec. 3.3.


In this section, we first discuss the expressiveness limitations of LSH attention in Sec. 4.1, and then
address these limitations by introducing our proposed key selection (KS) attention in Sec. 4.2. Finally,
Sec. 4.3 details the architecture of HAX as well as how HAX is integrated with state-of-the-art SSM
architectures, Mamba and Mamba2 [14, 4].


**4.1** **Limitation of LSH Attention**


In LLMs, certain keys (particularly those at the beginning of a sequence) often receive attention from
most queries, forming distinctive "vertical-stripe" attention patterns [39], as illustrated in Appendix
Figure 9. These globally attended keys play an essential role in instruction following, where the
model is expected to focus its attention on the instruction tokens [23].


Although LSH instantiates CDSA, as discussed in Sec. 2.2, it suffers from a key limitation: difficulty
in capturing "vertical-stripe" attention patterns. This arises because in each hashing round, every
key is mapped to a single bucket, and attention is constrained to occur only between queries and
keys within the same bucket. As a result, when many queries are forced to attend to a limited set of
key buckets, those buckets become overloaded, diminishing representation diversity and ultimately
degrading attention quality.


**4.2** **Key Selection (KS) Attention**


**Goals** . To address the limitation of LSH attention in capturing "vertical-stripe" attention patterns, we
propose to augment LSH attention by integrating it with a novel key selection (KS) attention module.
This module is designed to satisfy the following desirable properties:


1. "Vertical-stripe" capability: KS attention can express "vertical-stripe" attention patterns.
2. Auto-regressive compatibility: The computation of KS attention for the current token does not
depend on future queries or keys.
3. Context-dependent sparsity: KS sparse attention pattern is conditioned on the query and key
representations in context and satisfies Eq. 4.


6


**Modeling.** Taking the query and the key matrices **Q** and **K** as input, KS attention operates in two
phases. The first phase is key scoring, where an scoring module computes an importance score for
each key based on the key itself and all previous queries:
_xi_ = _fÎ¸_ ( **K** _i,_ **Q** 1 _...i_ ) (12)
The second phase is key selection: each query attends to the _k_ previous highest-scoring keys,
**S** KS _ij_ = **1** [ _xj âˆˆ_ Top- _k{x_ 1 _, ..., xi}_ ] (13)
With an ideal key scoring module that assigns the highest scores to the globally important keys, KS
attention effectively covers _k_ "vertical-stripes" within attention patterns.


For simplicity, we use a multilayer perceptron (MLP) as the key scoring network:

_fÎ¸_ ( **K** _i,_ **Q** 1 _..i_ ) â‰œ MLPï¿½ **K** _i,_ normalizeï¿½ [ï¿½] **Q** _p_ ï¿½ï¿½ (14)


1 _â‰¤pâ‰¤i_

**Training.** To train the scoring MLP, at each layer, we randomly sample _k_ candidate keys. Their
indices are denoted by _I_ . We compute the reference attention weights via a simple linear attention
module, and calculate a pairwise ranking loss between these reference weights and the predicted
scores. To be specific, we compute:
**A** _[â€²]_ = **Q K** [ _I_ ] _[âŠ¤]_ _,_ _y_ = _Ïƒ_ ( **A** _[â€²]_ ) _âŠ™_ **M** [ _I_ ] _,_ (15)
where **K** [ _I_ ] is the selected key representations, **M** [ _I_ ] is the auto-regressive mask restricted to those
positions, and _Ïƒ_ ( _Â·_ ) is the sigmoid function. With predicted scores _x âˆˆ_ R _[k]_ and target scores _y âˆˆ_ R _[k]_,
we construct pairwise logits and targets:



**P** _ij_ ( _x_ ) = _xi âˆ’_ _xj,_ **T** _ij_ ( _y_ ) =


and define the ranking loss:



ï£±
ï£²



1 if _yi > yj,_
0 _._ 5 if _yi_ = _yj,_ (16)
0 if _yi < yj._



ï£³



_L_ score( _x, y_ ) = [1]



BCE ( **P** _ij_ ( _x_ ) _,_ **T** _ij_ ( _y_ )) _,_ (17)

_i,j_




 _k_ [2]



where BCE( _Â·, Â·_ ) denotes binary cross-entropy. This objective encourages the scoring network to
assign higher scores to informative keys that receive higher attention weights. The final training
objective sums the ranking loss across layers with the auto-regressive language modeling loss _L_ LM:

_L_ = _L_ LM + _Î±_            - _L_ score (18)


layers

where _Î±_ is a scalar hyperparameter that balances the contribution of the ranking loss.


**4.3** **Hybrid Block Design**


Finally, we propose locality-sensitive Hashing Attention with sparse Key Selection (HAX), which
combines LSH and KS attention patterns:
**S** HAX = max _{_ **S** LSH _,_ **S** KS _} âˆˆ{_ 0 _,_ 1 _}_ _[l][Ã—][l]_ (19)



When _âˆ€i, âˆ¥_ **S** LSH _iâˆ¥_ 0 _â‰¤_ _[k]_




_[k]_ 2 _[,][ âˆ¥]_ **[S]** [KS] _[i][âˆ¥]_ [0] _[ â‰¤]_ _[k]_ 2



When _âˆ€i, âˆ¥_ **S** LSH _iâˆ¥_ 0 _â‰¤_ 2 _[,][ âˆ¥]_ **[S]** [KS] _[i][âˆ¥]_ [0] _[ â‰¤]_ 2 [, it satisfies]

_âˆ€i, âˆ¥_ **S** HAX _iâˆ¥_ 0 _â‰¤_ _k_ (20)
Intuitively, LSH and KS attention are complementary, each addressing the otherâ€™s limitations. LSH
attention routes each query to semantically similar keys through randomized hashing, offering flexible,
content-based interactions that KS attention alone lacks. In contrast, KS attention introduces broadcast
connections to a small set of globally important keys, such as instructions or formatting markers,
thereby recovering the â€œvertical-stripeâ€ patterns that LSH attention struggles to express. While LSH
attention promotes diverse contextual representations, mitigating risks of representation collapse, KS
attention sharpens focus by allocating attention weights to the most informative positions, enabling
stronger long-range control. Importantly, both mechanisms are inherently sparse, so their combination
introduces sub-quadratic computational cost.


Figure 3 illustrates Mamba-based and Mamba2-based HAX layer. The proposed hybrid sparse
attention layer mitigates the representation capacity limitations of SSMs by coupling them with a
parallel sparse attention branch. A parameterized gate rescales the sparse attention output before
fusion, which promotes stable optimization.


7


Figure 4: Validation _LLM_ during continual pre-training on
Mamba 130M models. Mamba integrated with HAX is the
only architecture that consistently exhibits a decreasing validation loss over the entire training process.


**5** **Experiments**



Table 1: Results of multiquery joint recall. Integrating
Mamba or Mamba2 with HAX
achieves the best performance,
which is in **bold** .


Mamba Mamba2


Base 16.3 36.6
+D 7.8 19.6
+SW 18.7 70.6
+SW+D 16.6 48.6
+A 16.4 49.3
+LSH 11.6 13.5
+KS _(ours)_ 36.6 60.1
+HAX _(ours)_ **38.0** **74.3**


Base (2 _Ã—_ dim) 34.5 59.2


Samba 6.3



We conduct experiments from two perspectives. First, we empirically verify the theoretical findings
presented in Sec. 3 on multi-query joint recall. Then, we demonstrate the effectiveness of the proposed
hybrid sparse architecture across synthetic and real-world NLP benchmarks.


**5.1** **Empirical Verification on Multi-Query Joint Recall**


**Data.** We construct a multi-query joint recall dataset in which the number of context blocks and the
number of keys per context are independently sampled from the range [5 _,_ 16], and the size of the
vocabulary is fixed at _|V|_ = 16. The dataset consists of 1.4M training examples, along with 14.4K
samples each for validation and testing.


**Baselines.** We adopt Mamba [14] and Mamba2 [4] as base architectures and evaluate various hybrid
sparse attention models built upon them, as illustrated in Figure 3. These include dilated attention (D),
sliding window attention (SW), a combination of sliding window and dilated attention (SW+D) [6],
A-shaped attention (A), locality-sensitive hashing (LSH) attention, our proposed key selection (KS)
attention, and HAX (a combination of LSH and KS attention). We also consider a Samba baseline.


**Setup.** For fair comparison, we fix the hidden size to 64 and fix _k_ = 64 defined in Eq. 4 (the
maximum number of keys each query can attend to) across all hybrid architectures. For variants
that integrate multiple sparse attention components, namely SW+D, A (consists of a SW component
and a sink attention component, a sink attention only attends to the first tokens in the sequence),
and HAX (LSH+KS), we allocate _k_ = 32 to each component, in order to maintain a global _k_ = 64.
We also consider a baseline that doubles the hidden size of SSMs to compensate for the absence of
hybrid sparse attention. We fix the number of layers to 2 for all models. We conduct experiments
using 3 different random seeds for each learning rate in _{_ 3e-3, 1e-3, 3e-4 _}_, and report the average
performance corresponding to the best-performing learning rate. For evaluation, we calculate mean
accuracy per-sample and average over the testset.


**Results.** As summarized in Table 1, compared to the Mamba and Mamba2 base architectures,
most hybrid sparse attention models improve performance. In particular, our proposed HAX model
consistently achieves the best performance, which underscores the effectiveness of our approach.


**5.2** **Continual Pre-training on Natural Language**


**Setup.** To evaluate our method on real-world long-context natural language data, we perform
continual pre-training based on the publicly released Mamba checkpoints [14]. As in section 5.1, we
augment the Mamba architecture with sparse attention, as illustrated in Figure 3. We include SSMs
integrated with CISA as baselines, along with ablated variants of our HAX model. We fix the sparsity
parameter _k_ = 128. For variants that integrate multiple sparse attention components, namely SW+D,
A, and HAX (LSH+KS), we set _k_ = 64 to each component, in order to maintain a global _k_ = 128.


8


Table 2: Ruler benchmark at 2K context length. We compare different sparse attention integrated with
Mamba 790M model, including CISA methods: dilated attention (D), sliding window attention (SW),
and their combination (SW+D), and A-shaped attention (A), and CDSA methods: LSH attention, and







|our proposed key selection (KS) attention and HAX. The best average performance is in bold|d.|
|---|---|
|Model<br>NIAHS1<br>NIAHS2<br>NIAHS3<br>NIAHMK1<br>NIAHMK2<br>NIAHMK3<br>NIAHMV<br>NIAHMQ<br>VT<br>CWE<br>FWE<br>QA1|Average|
|Mamba<br>100<br>95.2<br>86.8<br>34.4<br>2.6<br>0<br>32.4<br>28.35<br>2.76<br>1.4<br>100<br>25.2<br>+D<br>0<br>0<br>0<br>0<br>0<br>0<br>0<br>0<br>17.4<br>0<br>100<br>0<br>+SW<br>100<br>93.6<br>83.6<br>36.2<br>3.4<br>0<br>31.05<br>31.45<br>5.24<br>1.26<br>100<br>24.4<br>+SW+D<br>100<br>95<br>82.4<br>34<br>2.6<br>0<br>32.9<br>29.45<br>4.72<br>1.22<br>100<br>24.8<br>+A<br>100<br>93.2<br>81.4<br>34.4<br>2.6<br>0<br>31.7<br>27.1<br>4.4<br>1.26<br>100<br>26.8<br>+LSH<br>0<br>0<br>0<br>0<br>0<br>0<br>0<br>0<br>0.2<br>0.06<br>100<br>0<br>+KS_ (ours)_<br>100<br>94.4<br>84.4<br>33.6<br>1.8<br>1.8<br>37.25<br>33.85<br>4.04<br>1<br>100<br>25<br>+HAX_ (ours)_<br>100<br>98.8<br>89.6<br>45.4<br>3.8<br>0.2<br>35.5<br>28.2<br>2.04<br>1.64<br>100<br>29.6|42.43<br>9.78<br>42.52<br>42.26<br>41.91<br>8.36<br>43.10<br>**44.57**|


Table 3: LongBench English tasks. We compare different sparse attention integrated with Mamba







|790M model, including CISA and CDSA methods. The best average performance is in bold.|Col2|
|---|---|
|Model<br>2WikiMQA<br>GovReport<br>HotPotQA<br>LCC<br>MultiNews<br>MultiFQA<br>MuSiQue<br>NQA<br>PsgCnt<br>PsgRet<br>Qasper<br>QMSum<br>Repobench<br>SamSum<br>Trec<br>TriviaQA|Averag|
|Mamba<br>13.39<br>20.3<br>8.37<br>39.51<br>20.36<br>27.62<br>3.89<br>8.35<br>1.04<br>1.5<br>10.69<br>20.36<br>38.47<br>20.15<br>45<br>37.59<br>+D<br>0<br>0.21<br>0<br>2.87<br>0.26<br>0<br>0<br>0<br>0<br>0<br>0<br>0.72<br>3.46<br>0.7<br>0<br>0<br>+SW<br>13.81<br>20.58<br>9.25<br>38.06<br>20.64<br>27.55<br>5.15<br>8.12<br>1.2<br>1.5<br>10.55<br>19.57<br>38.38<br>22.42<br>43<br>36.55<br>+SW+D<br>13.6<br>20.39<br>8.55<br>39<br>20.74<br>27.55<br>4.33<br>7.98<br>0.83<br>1.56<br>10.17<br>19.73<br>38.84<br>22.49<br>43<br>37.02<br>+A<br>13.63<br>20.77<br>8.55<br>38.59<br>20.2<br>27.31<br>4.56<br>7.66<br>0.27<br>1.74<br>11.32<br>20.09<br>38.5<br>20.73<br>44.5<br>37.16<br>+LSH<br>1.66<br>1.49<br>1.5<br>7.74<br>0.98<br>2.32<br>0.96<br>0.87<br>0.1<br>0<br>2.3<br>5.05<br>7.13<br>3.89<br>0<br>0.9<br>+KS_ (ours)_<br>12.93<br>20.93<br>9.34<br>38.22<br>20.83<br>26.56<br>4.34<br>7.46<br>0.27<br>1.67<br>11.65<br>19.35<br>38.17<br>22.97<br>45<br>37.52<br>+HAX_ (ours)_<br>12.73<br>20.64<br>8.79<br>39.3<br>21.79<br>28.31<br>3.95<br>7.64<br>0.83<br>2<br>11.94<br>19.63<br>39.07<br>21.04<br>43<br>38.47|19.79<br>0.51<br>19.77<br>19.74<br>19.72<br>2.31<br>19.83<br>**19.95**|


Table 4: Performance of extrapolation from 2K to 4K context length on Ruler benchmark. We
compare different sparse attention integrated with Mamba 790M model, including CISA and CDSA







|methods as in Table 2. The best average performance is in bold.|Col2|
|---|---|
|Model<br>NIAHS1<br>NIAHS2<br>NIAHS3<br>NIAHMK1<br>NIAHMK2<br>NIAHMK3<br>NIAHMV<br>NIAHMQ<br>VT<br>CWE<br>FWE<br>QA1<br>QA2|Average|
|Mamba<br>100<br>19.2<br>9.2<br>5.2<br>0.6<br>0<br>0.65<br>1.85<br>5.72<br>4.3<br>100<br>26.2<br>78.6<br>+D<br>0<br>0<br>0<br>0<br>0<br>0<br>0<br>0<br>65.2<br>0<br>100<br>0<br>73<br>+SW<br>100<br>18.2<br>5<br>5.6<br>0.2<br>0.2<br>1.75<br>2.1<br>4.4<br>4.78<br>100<br>22.4<br>78.8<br>+SW+D<br>100<br>20.2<br>8.2<br>6.2<br>0.4<br>0.2<br>1.45<br>1.25<br>4.36<br>4.32<br>100<br>24.6<br>79<br>+A<br>100<br>19.8<br>7.2<br>6.6<br>1.2<br>0.2<br>1.3<br>0.8<br>4.36<br>3.24<br>100<br>24.2<br>79<br>+LSH<br>0<br>0<br>0<br>0<br>0<br>0<br>0<br>0<br>0<br>0.04<br>100<br>0.2<br>73<br>+KS_ (ours)_<br>99.8<br>20.2<br>7.8<br>8.2<br>1<br>0.2<br>1.2<br>1.85<br>9.32<br>2.86<br>100<br>22.6<br>78.6<br>+HAX_ (ours)_<br>100<br>23<br>11<br>9<br>0.8<br>0<br>3.4<br>2.75<br>6.36<br>2.54<br>100<br>29<br>78.8|27.04<br>18.32<br>26.42<br>26.94<br>26.76<br>13.33<br>27.20<br>**28.20**|


Overall, our HAX adds only about 1% additional parameters to the released Mamba architecture. All
models are continually pre-trained for 50K steps with a context length of 2K.


**Validation Loss during Continual Pre-Training.** Figure 4 shows the validation loss _LLM_ of 130M
model continual pre-training on The Pile [11]. As observed, the Mamba base architecture and all
baseline variants exhibit either training instability or plateau early in the training process. In contrast,
our proposed HAX model is the only architecture that shows a consistent decline in validation loss
throughout the training process, indicating improved stability and sustained learning.


**Ruler and LongBench Evaluation.** Tables 2 and 3 present the downstream performance for
the Mamba 790M model after continual pre-training for 50K steps on TxT-360 [36] followed by
instruction fine-tuning for 10K steps on UltraChat [7] following Mamba-Chat [24]. Ruler [18] is a
synthetic long-context NLP benchmark designed to assess model performance on tasks including
retrieval, multi-hop reasoning, aggregation, and question answering. LongBench [3] comprises
real-world NLP long-context tasks, including question answering, summarization, few-shot learning,
retrieval, aggregation, and code completion. The evaluation results, summarized in Tables 2 and 3,
show that among all hybrid sparse attention variants, our proposed HAX model is the only one that
outperforms the Mamba baseline by a significant margin on average.


**Extrapolation Evaluation.** We additionally evaluate the Mamba 790M models on the Ruler [18]
benchmark at a 4K context length, despite being continually pre-trained and instruction fine-tuned
solely with a 2K context length. As summarized in Table 4, our proposed HAX model consistently
outperforms all baselines, underscoring its robustness for context length extrapolation.


9


**6** **Related Work**


**6.1** **State-Space Models**


State-space models (SSMs) originated in control theory, exemplified by damped mass-spring systems

[29]. HiPPO [15] was one of the first efforts to adapt SSMs for machine learning applications. LSSL

[17] unified convolutional neural networks (CNNs), recurrent neural networks (RNNs), and ordinary
differential equations (ODEs) under the SSM framework, enabling their implementation within deep
neural networks. H3 [10] integrated SSM layers with short convolutional filters to enhance sequence
modeling. Mamba [14] advanced this line of work by making all SSM parameters _input-dependent_,
which significantly increases the representation capacity of SSMs. Mamba2 [4] further improved the
architecture and established connections between SSMs and Transformer attention.


**6.2** **Analysis on State-Space Models**


Recent empirical studies have shown that SSM long-context modeling performance often lags
behind that of Transformer architectures [40]. [19] demonstrated that even solving simple tasks like
copying requires SSM state dimensions to grow linearly with the sequence length. Furthermore, [25]
established that the expressiveness of SSMs is bounded by the complexity class TC [0] . [34] further
showed that SSMs and Transformers capture overlapping yet distinct subsets of TC [0], providing a
theoretical basis for developing hybrid models that combine the strengths of both architectures.


**6.3** **Context-Dependent Sparse Attention**


Locality-sensitive hashing (LSH) attention [20] stands as one of the most widely adopted forms of
context-dependent sparse attention. Follow-up LSH attention variants can be broadly categorized by
their binning rules: the argmax binning rule is adopted in [20, 35], while the sign-bit binning rule is
used in [8, 5]. [28] introduces a Triton kernel that accelerates hash-based sparse attention. Recently,

[43] proposed native sparse attention, a new context-dependent sparse attention that outperforms
Transformers training from scratch, highlighting its potential for efficient long-context modeling.


**6.4** **Hybrid Architectures**


Several works have explored architectures that mix a large proportion of SSM layers with a small
number of full attention layers, and have reported performance surpassing that of standard Transformers [40]. The effectiveness of such hybrid architectures has been further validated at billion-parameter
scale [21]. In parallel, researchers have also investigated the design of hybrid sparse attention models

[33, 32, 9, 26, 44], which offer sub-quadratic computational complexity, providing a promising
direction for efficient long-context modeling.


**7** **Conclusion**


In this work, we introduce _joint recall_, a novel synthetic task that generalizes associative recall to
context-dependent key-value retrieval. Theoretically, we show that both SSMs and SSMs integrated
with context-independent sparse attention (CISA) could not solve multi-query joint recall within subquadratic time complexity, while integrating SSMs with context-dependent sparse attention (CDSA)
overcomes this limitation. Guided by this insight, we propose to integrate state-of-the-art SSMs with
a novel CDSA, locality-sensitive Hashing Attention with sparse Key Selection (HAX). Experiment
results confirm that HAX achieves improved training stability and consistent performance gains
across synthetic and real-world long-context NLP benchmarks. The _joint recall_ task therefore offers a
unified theoretical lens and empirical yardstick for long-context modeling, while HAX demonstrates
the power of theory-driven architecture design. These results highlight the importance of aligning
model design with expressiveness improvements, and demonstrate that combining efficient sequence
models with CDSA is a promising direction for scalable long-context modeling.


10


**Acknowledge**


We would like to thank Jingyang Yuan, Wanru Zhao, Meng Qu, Zewen Chi for their insightful
discussion on language model pre-training. We also thank the anonymous reviewers, AC, SAC, and
PC for their time, thoughtful feedback, and constructive engagement throughout the review process.


11


**References**


[1] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri
Rudra, and Christopher RÃ©. Zoology: Measuring and improving recall in efficient language
models. In _The Twelveth International Conference on Learning Representations_, 2024.


[2] Jimmy Ba, Geoffrey E. Hinton, Volodymyr Mnih, Joel Z. Leibo, and Catalin Ionescu. Using
fast weights to attend to the recent past. In _The Twenty-ninth Annual Conference on Neural_
_Information Processing Systems_, 2016.


[3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao
Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for
long context understanding. In _Proceedings of the 62nd Annual Meeting of the Association for_
_Computational Linguistics_, 2024.


[4] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms
through structured state space duality. _arXiv preprint arXiv: 2405.21060_, 2024.


[5] Aditya Desai, Shuo Yang, Alejandro Cuadron, Matei Zaharia, Joseph E. Gonzalez, and Ion
Stoica. Hashattention: Semantic sparsity for faster inference. In _The Forty-second International_
_Conference on Machine Learning_, 2025.


[6] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu
Wei. Longnet: Scaling transformers to 1,000,000,000 tokens, 2023.


[7] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong
Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional
conversations, 2023.


[8] Ning Ding, Yehui Tang, Haochen Qin, Zhenli Zhou, Chao Xu, Lin Li, Kai Han, Liao Heng, and
Yunhe Wang. Memoryformer: Minimize transformer computation by removing fully-connected
layers. In _The Thirty-eighth Annual Conference on Neural Information Processing Systems_,
2024.


[9] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, ZIJIA CHEN, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan Celine
Lin, Jan Kautz, and Pavlo Molchanov. Hymba: A hybrid-head architecture for small language
models. In _The Thirteenth International Conference on Learning Representations_, 2025.


[10] Daniel Y. Fu, Tri Dao, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher RÃ©.
Hungry hungry hippos: Towards language modeling with state space models. In _The Eleventh_
_International Conference on Learning Representations_, 2023.


[11] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:
An 800gb dataset of diverse text for language modeling, 2021.


[12] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles
Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noacâ€™h, Haonan Li, Kyle McDonell, Niklas
Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,
Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework
for few-shot language model evaluation, 2024.


[13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela
Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem
Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson,
Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,
Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret,
Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius,
Daniel Song, et al. The llama 3 herd of models, 2024.


[14] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In
_The First Conference on Language Modeling_, 2024.


12


[15] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher RRÃ©. Hippo: Recurrent
memory with optimal polynomial projections. In _The Thirty-fourth Annual Conference on_
_Neural Information Processing Systems_, 2020.


[16] Albert Gu, Karan Goel, and Christopher RÃ©. Efficiently modeling long sequences with structured
state spaces. In _The Tenth International Conference on Learning Representations_, 2022.


[17] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher RChristopher RRÃ©. Combining recurrent, convolutional, and continuous-time models
with linear state-space layers. In _The Thirty-fifth Annual Conference on Neural Information_
_Processing Systems_, 2021.


[18] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia,
Yang Zhang, and Boris Ginsburg. Ruler: Whatâ€™s the real context size of your long-context
language models? In _The First Conference on Language Modeling_, 2024.


[19] Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. Repeat after me:
Transformers are better than state space models at copying. In _The Forty-first International_
_Conference on Machine Learning_, 2024.


[20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In
_The Eighth International Conference on Learning Representations_, 2020.


[21] Barak Lenz, Opher Lieber, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg,
Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor
Muhlgay, Dor Zimberg, Edden M. Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez
Schwartz, Gal Cohen, Gal Shachaf, Haim Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay
Dalmedigos, Jhonathan Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman,
Mor Zusman, Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, et al. Jamba: Hybrid
transformer-mamba language models. In _The Thirteenth International Conference on Learning_
_Representations_, 2025.


[22] Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang,
Chenchen Zhang, Ge Zhang, Jiebin Zhang, Yuanxing Zhang, Zhuo Chen, Hangyu Guo, Shilong
Li, Ziqiang Liu, Yong Shan, Yifan Song, Jiayi Tian, Wenhao Wu, Zhejian Zhou, Ruijie Zhu,
Junlan Feng, Yang Gao, Shizhu He, Zhoujun Li, Tianyu Liu, Fanyu Meng, Wenbo Su, Yingshui
Tan, Zili Wang, Jian Yang, Wei Ye, Bo Zheng, Wangchunshu Zhou, Wenhao Huang, Sujian Li,
and Zhaoxiang Zhang. A comprehensive survey on long context language modeling, 2025.


[23] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,
and Percy Liang. Lost in the middle: How language models use long contexts. _Transactions of_
_the Association for Computational Linguistics_, 2024.


[24] Justus Mattern and Konstantin Hohr. Mamba-chat. GitHub, 2023.


[25] William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space
models. In _The Forty-first International Conference on Machine Learning_, 2024.


[26] Elvis Nunez, Luca Zancato, Benjamin Bowman, Aditya Golatkar, Wei Xia, and Stefano Soatto.
Expansion span: Combining fading memory and retrieval in hybrid state space models, 2024.


[27] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain,
Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson
Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,
Sam McCandlish, and Chris Olah. In-context learning and induction heads, 2022.


[28] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and FranÃ§ois Fleuret. Faster causal attention
over large sequences through sparse flash attention. In _The Thirty-seventh Annual Conference_
_on Neural Information Processing Systems_, 2023.


[29] Badri Narayana Patro and Vijay Srinivas Agneeswaran. Mamba-360: Survey of state space
models as transformer alternative for long sequence modelling: Methods, applications, and
challenges, 2024.


13


[30] Saurav Pawar, S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Aman Chadha,
and Amitava Das. The what, why, and how of context length extension techniques in large
language models â€“ a detailed survey, 2024.


[31] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua
Bengio, Stefano Ermon, and Christopher RÃ©. Hyena hierarchy: Towards larger convolutional
language models. In _The Fortieth International Conference on Machine Learning_, 2023.


[32] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba:
Simple hybrid state space models for efficient unlimited context language modeling. In _The_
_Thirteenth International Conference on Learning Representations_, 2025.


[33] Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and Cheng Xiang
Zhai. Sparse modular activation for efficient sequence modeling. In _The Thirty-seventh Annual_
_Conference on Neural Information Processing Systems_, 2023.


[34] Yash Sarrof, Yana Veitsman, and Michael Hahn. The expressive capacity of state space models:
A formal language perspective. In _The Thirty-eighth Annual Conference on Neural Information_
_Processing Systems_, 2024.


[35] Zhiqing Sun, Yiming Yang, and Shinjae Yoo. Sparse attention with learning to hash. In _The_
_Tenth International Conference on Learning Representations_, 2022.


[36] Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, Bhaskar Rao,
Linghao Jin, Huijuan Wang, Zhoujun Cheng, Suqi Sun, Cun Mu, Victor Miller, Xuezhe Ma,
Yue Peng, Zhengzhong Liu, and Eric P. Xing. Txt360: A top-quality llm pre-training dataset
requires the perfect blend, 2024.


[37] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey,
2022.


[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _The Thirtieth Annual_
_Conference on Neural Information Processing Systems_, 2017.


[39] Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language
model. In _Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting_
_Neural Networks for NLP_, 2019.


[40] Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao,
Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika
Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, and Bryan Catanzaro. An empirical study
of mamba-based language models, 2024.


[41] Wikipedia. Harry Potter. `[https://en.wikipedia.org/wiki/Harry_Potter](https://en.wikipedia.org/wiki/Harry_Potter)`, 2025.


[42] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations
of linear attention mechanism, 2024.


[43] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda
Xie, Yuxing Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. Native sparse attention: Hardware-aligned and natively
trainable sparse attention. In _Proceedings of the 63rd Annual Meeting of the Association for_
_Computational Linguistics_, 2025.


[44] Luca Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin
Bowman, Matthew Trager, Alessandro Achille, and Stefano Soatto. Bâ€™MOJO: Hybrid state
space realizations of foundation models with eidetic and fading memory. In _The Thirty-eighth_
_Annual Conference on Neural Information Processing Systems_, 2024.


14


Information





Auto-Regressive Model
Input


|C K V K V<br>1 11 11 12 12<br>C K V K V<br>2 21 21 22 22<br>â€¦<br>C K V K V<br>5 51 51 52 52|K<br>11|V<br>11|K<br>12|V<br>12|
|---|---|---|---|---|
|C1 K11 V11 K12 V12<br>C2 K21 V21 K22 V22<br>â€¦<br>C5 K51 V51 K52 V52|K21|V21|K22|V22|
|C1 K11 V11 K12 V12<br>C2 K21 V21 K22 V22<br>â€¦<br>C5 K51 V51 K52 V52|K51|V51|K52|V52|








|sep<br>C K V K V<br>2 22 22 21 21<br>C K V K V<br>5 52 52 51 51<br>C K V K V<br>1 11 11 12 12|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|sep<br>C2 K22 V22 K21 V21<br>C5 K52 V52 K51 V51<br>C1 K11 V11 K12 V12|K22|V22|K21|V21|
|sep<br>C2 K22 V22 K21 V21<br>C5 K52 V52 K51 V51<br>C1 K11 V11 K12 V12|K52|V52|K51|V51|
|sep<br>C2 K22 V22 K21 V21<br>C5 K52 V52 K51 V51<br>C1 K11 V11 K12 V12|K11|V11|K12|V12|


|Col1|Col2|Col3|Col4|
|---|---|---|---|
||V22||V21|
||V52||V51|
||V11||V12|



Figure 5: Input and output components of the auto-regressive multi-query joint recall task. The input
sequence is further divided into an information component and an inquiry component.


**A** **Relationship Between the Argmax and Sign-Bit LSH Binning Rules**


In this section, we show how the sign-bit LSH binning rule (Eq. 7) can be interpreted as an argmax
LSH binning rule (Eq. 6) applied to an expanded projection matrix with 2 _[h]_ columns. We first
construct the expanded matrix, and then prove the equivalence.


**Expanding the projection matrix.** Let the original random projection be **H** = [ **H** 1 _, ...,_ **H** _h_ ] _âˆˆ_
i.i.d.
R _[d][Ã—][h]_ _,_ **H** _j_ _âˆ¼N_ (0 _,_ 1) _._ Define a codebook of 2 _[h]_ signed prototypes

_B_ =      - **Bb** = [ï¿½] _j_ _[h]_ =1 _[b][j]_ **[H]** _[j]_ ï¿½ï¿½ï¿½ **b** = ( _b_ 1 _, . . ., bh_ ) _âˆˆ{âˆ’_ 1 _,_ +1 _}h_      - _âŠ‚_ R _[d]_ _._ (21)


Stacking all **Bb** as columns yields the implicit matrix **H** [Ëœ] _âˆˆ_ R _[d][Ã—]_ [2] _[h]_ _._

**Equivalence of the two binning rules.** For a normalized query vector **Q** [Ëœ] _i_, we define its sign
projection as **s** = sign( **Q** [Ëœ] _[âŠ¤]_ _i_ **[H]** [)] _[ âˆˆ{âˆ’]_ [1] _[,]_ [ +1] _[}][h][.]_ [ The inner product of][ Ëœ] **[Q]** _[i]_ [ and a prototype] **[ B][b]** _[ âˆˆB]_ [ is]



_âŸ¨_ **Q** [Ëœ] _i,_ **Bb** _âŸ©_ =



_h_

- _bj âŸ¨_ **Q** [Ëœ] _i,_ **H** _jâŸ©._ (22)


_j_ =1



Because every term with _bj Ì¸_ = _sj_ flips the sign of the positive quantity _|âŸ¨_ **Q** [Ëœ] _i,_ **H** _jâŸ©|_, we have the strict
inequality _âŸ¨_ **Q** [Ëœ] _i,_ **Bs** _âŸ©_ _> âŸ¨_ **Q** [Ëœ] _i,_ **Bb** _âŸ©_ for all **b** _Ì¸_ = **s** . Hence


argmax **b** _âˆˆ{âˆ’_ 1 _,_ +1 _}hâŸ¨_ **Q** [Ëœ] _i,_ **Bb** _âŸ©_ = **s** = bin [(sign)] _Qi_ _[,]_ (23)


The sign-bit assignment is exactly the argmax rule applied to **H** [Ëœ] . An identical argument holds for
keys **K** [Ëœ] _j_ . Thus, the sign-bit method equals the argmax method with 2 _[h]_ (expanded) columns.


**B** **Theoretical Proof**


Multi-query joint recall requires models to recall an _nc Ã— nk_ table of context-specific key-value
associations, in which _nk_ keys are associated with different values in each of the _nc_ contexts, with
_n_ = _nc Ã— nk_ being the total number of entries. For clarity, we introduce some additional notations
for multi-query joint recall in the auto-regressive setting, as illustrated in Appendix Figure 5. The
input sequence is divided into an information component and an inquiry component. The information
component provides the context-specific key-value associations. The inquiry component permutes
the order of context and keys in the information component, and the model is required to predict the
corresponding values given each key under every specified context.


**B.1** **Proof of Corollary 3.1**


**Corollary 3.1** (Limited expressiveness of SSMs) **.** _Under the uniform multi-query joint recall distri-_
_bution, for any n, a generalized state-space model defined in Sec. 2.3 incurs an error rate of at least_
_|U|_
1 _âˆ’_ _|V|_ _[n]_ _[.]_


15


SSM



Information Inquiry



Figure 6: Comparison between SSM, hybrid dilated attention model and hybrid locality-sensitive
hashing (LSH) attention model on joint recall. By selectively bypassing irrelevant context, sparse
attention alleviates memory overload in SSM layers and enhances the hybrid modelâ€™s capability to
retrieve relevant information.


The intuition behind the proof of Corollary 3.1 is straightforward: the number of possible joint recall
data instances that a model can accurately represent is fundamentally limited by the capacity of its
recurrent state. Since all information from the input sequence must be encoded into a fixed recurrent
state following the context during processing, the total number of distinguishable output is bounded
by the representation capacity of the state space _|U|_ . Consequently, if the size of the output space
_|V|_ _[n]_ exceeds _|U|_, the model inevitably incurs non-negligible error.


As a direct consequence of Theorem 2.7 in [19], we adopt their proof strategy. We reformulate
Lemma D.1 in [19] as the following Lemma B.1. Let _m_ denote the index of the last token in the
information component. Then, for any fixed permutation _P_ of the context and keys in the inquiry
component, the following Lemma B.1 holds:


**Lemma B.1.** _Let M be a fixed-state generalized SSM that maps the joint recall input space X to the_
_output space V_ _[n]_ _under any fixed permutation P of the context and keys in the inquiry component._
_Then there exists a function G_ : _U â†’V_ _[n]_ _such that for all inputs_ **x** _âˆˆX_ _, the model output satisfies_
_M_ ( **x** ) = _G_ ( _Um_ ( **x** )) _, Um is defined in Eq.10._


Following [19], we bound the error of the model by comparing the number of possible model states
to the number of distinct input instances.


_Proof._


1 _âˆ’_ Pr[ _err_ ] = Pr[ _M_ ( **x** ) = **y** _|_ **y** _âˆˆV_ _[n]_ ] (24)

= 1         - **1** [ _M_ ( **x** ) = **y** ] (25)
_|V|_ _[n]_

**y** _âˆˆV_ _[n]_



= 1 _|V|_ _[n]_

**y** _âˆˆV_ _[n]_




- **1** [ _G_ ( **u** ) = **y** ] _Â·_ **1** [ _Um_ ( **x** ) = **u** ] (26)


**u** _âˆˆU_



_â‰¤_ _|V|_ 1 _[n]_         - **1** [ _Um_ ( **x** ) = **u** ] (27)

**u** _âˆˆU_

_â‰¤_ _[|U|]_ (28)

_|V|_ _[n]_


To guarantee Pr[ _err_ ] = 0, it is necessary that the number of representable states satisfies _|U| â‰¥|V|_ _[n]_ .
Taking the logarithm of both sides yields the condition _b â‰¥_ _n_ log _|V|_ . This implies that the state-space
dimension of the model must grow linearly with the number of entries _n_ in the joint recall table,
highlighting a fundamental limitation of the representation capacity of SSMs.


In contrast, as illustrated in Figure 6, hybrid sparse attention models mitigate this limitation by
enabling information to propagate through multiple parallel paths, thereby alleviating the bottleneck
imposed by sequential state updates.


16


**B.2** **Proof of Proposition 3.2**


**Proposition 3.2** (Improved expressiveness of SSMs integrated with CDSA) **.** _There exists a_ 2 _-layer_
_auto-regressive hybrid model consisting of an SSM layer followed by an LSH attention layer, which_
_can solve multi-query joint recall in O_ ( _n_ log [2] _n_ ) _time complexity with O_ (log _n_ ) _SSM state dimensions._


_Proof._ We prove by construction. In the first layer, we expect the SSM state concatenates each value
token representation with its associated key token representation and context token representation. To
be specific, we expect the SSM state representation at each value token to be


[ **c** _,_ **k** _,_ **v** _, is_ _ _v_ ] _âˆˆU_


where **c** is the representation of the current associated context token, **k** is the representation of the
current associated key token, and **v** is the representation of the nearest value token. _is_ _ _v_ is a sign
indicator ( _âˆ’_ 1 or 1) that specifies whether the current token is a value token.


To achieve this, we first construct each vector **c**, **k** and **v** be a distinct _b_ -dimensional vector with
unit norm without zero entries, i.e., _âˆ¥_ **c** _âˆ¥_ 2 = 1, _âˆ¥_ **k** _âˆ¥_ 2 = 1, _âˆ¥_ **v** _âˆ¥_ 2 = 1, _âˆ€j,_ **c** _j Ì¸_ = 0 _,_ **k** _j Ì¸_ = 0 _,_ **v** _j Ì¸_ = 0.
Since the number of distinct vectors that can be drawn from the unit sphere grows exponentially with
dimensionality, _O_ (log _n_ ) embedding dimensions are sufficient to ensure that all representations are
distinguishable. Then we define an embedding space in which context and value tokens are mapped
to structured representations. Specifically, a context token is embedded as


**e** = [ **c** _,_ **0** _,_ **0** _, âˆ’_ 1]


where **c** is the constructed representation of this context token on the unit sphere, and the final
coordinate is set to _âˆ’_ 1 to indicate that the current token is not a value. Similarly, a key token is
embedded as


**e** = [ **0** _,_ **k** _,_ **0** _, âˆ’_ 1]


and a value token is embedded as


**e** = [ **0** _,_ **0** _,_ **v** _,_ 1]


where **k** and **v** are the key and value representations from the unit sphere, respectively, and the final
coordinate is set to 1 only when the current token is a value token. Following Eq. 10, we define the
update rule of the generalized SSM as follows:


_Ui_ = _u_ ( _Uiâˆ’_ 1 _,_ **e** ) = _Uiâˆ’_ 1 _âŠ™_ **1** [ **e** _j_ = 0] + **e** _âŠ™_ **1** [ **e** _j Ì¸_ = 0] (29)
_Ri_ = _r_ ( _Ui_ ) = _Ui_ (30)


where **e** is the current input embedding and **e** _j_ refers to its _j_ -th dimension. The update rule operates as
a conditional overwrite: if a position does not carry information (i.e., the corresponding dimension in
**e** is **0** ), the previous state is preserved; otherwise, it is updated with the current embedding. Following
this update rule, the SSM state at each value token in the information component takes the form


[ **c** _,_ **k** _,_ **v** _,_ 1]


while the SSM state at each key token takes the form


[ **c** _,_ **k** _,_ ? _, âˆ’_ 1]


In the second layer, LSH attention operates on the SSM state [ **c** _,_ **k** _,_ **v** _, is_ _ _v_ ] _âˆˆU_, using [ **c** _,_ **k** _,_ **0** _, is_ _ _v_ ]
as the LSH attention key representation, [ **c** _,_ **k** _,_ **0** _,_ 1] as the LSH attention query representation, and

[ **0** _,_ **0** _,_ **v** _,_ 1] as the LSH attention value representation. This design ensures that value tokens in the
information component and key tokens in the inquiry component that share the same context and
key (i.e., identical **c** and **k** representations in the SSM state) will always be assigned to the same
hash bin. With a sufficient number of, e.g. _O_ ( _n_ ) hash bins, which can be efficiently constructed
using sign-bit binning rule with _O_ (log _n_ ) random projection vectors, values associated with each key
in every specified context are reliably retrievable by LSH attention. This step is the bottleneck of
computation with _O_ ( _n_ log [2] _n_ ) time complexity.


17


1st-level

context


In the Spring Semester,


In the Fall Semester,



2nd-level

context


on Monday



mornings,


afternoons,



3rd-level



4th-level

context



value
context context



Alice studies math.
Bob studies physics.
â€¦



on Tuesday
on Wednesday
â€¦


on Monday
â€¦



Figure 7: An example of multi-level context in natural language.


**B.3** **Proof of Proposition 3.3**


**Proposition 3.3** (Limited expressiveness of SSMs integrated with CISA) **.** _There does not exist a_
2 _-layer auto-regressive hybrid model consisting of an SSM layer followed by a CISA layer, which_
_can solve multi-query joint recall with o_ ( _n_ [2] ) _time complexity, since it requires at least O_ ( _[n]_ _k_ [)] _[ SSM]_

_state dimensions, k is the maximum number of keys that each query allowed to attend to in the sparse_
_attention module, as defined in Eq. 4._


_Proof._ Consider a key given in the inquiry component of the auto-regressive joint recall task. The
model is required to output the associated value token when this key token is provided as input.
Taking the query representation from this key token, the sparse attention can attend to at most _k_
key representations from previous tokens, where the key representations are calculated based on the
SSM state representations of the first layer. To solve the joint recall task, these _k_ key representations
being attended must collectively encode the full information component. Since the full information
component length is _O_ ( _n_ ), by Corollary 3.1, _k_ state representations of the generalized SSM in the
first layer must use at least _O_ ( _[n]_ _k_ [)][ dimensions to collectively store the information component. Thus,]

the per-key computational cost required by the second-layer sparse attention is _O_ ( _k Â·_ _[n]_ _k_ [) =] _[ O]_ [(] _[n]_ [)][,]

and therefore the total time complexity is _O_ ( _n_ [2] ).


Comparing **Proposition** 3.2 with **Proposition** 3.3, we see a clear representation capacity gap between
the SSMs integrated with CDSA and the SSMs integrated with CISA. In practice, however, with
an appropriate constant _k_, integrating CISA with SSMs still provides an advantage: unlike SSMs,
which only have access to the last state representation, CISA layers can attend to _k_ different state
representations simultaneously, at a cost of _k_ times of computation budget.


**C** **Extending Joint Recall to Multi-level Context**


As illustrated in Figure 7, in many cases, natural language contexts exhibit hierarchical dependencies.
This motivates us to extend joint recall to the multi-level context setting, in which we regard the keys
as the last level of context.


**C.1** **Formulation**


Given _w_ different levels of context vocabulary _C_ 1 _, C_ 2 _, ..., Cw_ and token vocabulary _V_, multi-level
context joint recall requires a model to recover the mapping _C_ 1 _Ã— C_ 2 _Ã— ... Ã— Cw â†’V_ . The context
of multi-level context joint recall is hierarchically structured analogously to natural languages. It is


18


divided into _|C_ 1 _|_ first-level blocks, where each first-level block begins with a token from the first-level
context vocabulary _C_ 1. Each first-level block is further divided into sub-blocks beginning with a
second-level context token from _C_ 2, and this recursive sub-division continues up to the _w_ -th level.
The last-level block consists of a _w_ -th level context token followed by a value token from _V_ . Note
that associative recall is a special case of multi-level joint recall with _w_ = 1, and joint recall is
a special case of multi-level joint recall with _w_ = 2. We similarly define multi-query multi-level
context joint recall, where the model is required to recall all _n_ = _|C_ 1 _| Ã— |C_ 2 _| Ã— ... Ã— |Cw|_ entries in
the full context-value supertable.


**C.2** **Expressiveness of SSMs Integrated with CDSA on Multi-Level Context Joint Recall**


On multi-query multi-level context joint recall, both Corollary 3.1 and Proposition 3.3 continue to
hold under the same assumptions. We now extend Proposition 3.2 to the following Proposition C.1,
which demonstrates that SSMs integrated with CDSA remain expressive even in the presence of _w_
levels of hierarchical contexts.


**Proposition C.1** (Expressiveness of SSMs integrated with CDSA on multi-level context joint recall) **.**
_There exists a_ 2 _-layer auto-regressive hybrid model consisting of an SSM layer followed by an LSH_
_attention layer, which can solve multi-query multi-level context joint recall in O_ ( _wn_ log [2] _n_ ) _time_
_complexity with O_ ( _w_ log _n_ ) _SSM state dimensions._


_Proof._ Similar to Proposition 3.2, we prove by construction. In the first layer, we hope the SSM state
to consist of the context and value representations


[ **z** 1 _,_ **z** 2 _, ...,_ **z** _w,_ **v** _, is_ _ _v_ ] _âˆˆU_


where **z** _i_ denotes the representation of the nearest _i_ -th level context token, **v** represents the nearest
value token, and _is_ _ _v_ is a sign indicator ( _âˆ’_ 1 or 1) that specifies if the current token is a value token.


To achieve this, we similarly construct each vector **z** _i_ and **v** to be a distinct _b_ -dimensional vector with
unit norm without zero entries, i.e., _âˆ¥_ **z** _iâˆ¥_ 2 = 1, _âˆ¥_ **v** _âˆ¥_ 2 = 1, _âˆ€j,_ **z** _ij Ì¸_ = 0 _,_ **v** _j Ì¸_ = 0. Since the number
of distinct vectors that can be drawn from the unit sphere grows exponentially with dimensionality,
_O_ (log _n_ ) embedding dimensions are sufficient to ensure that all representations are distinguishable.
Consequently, the total size of the SSM state is _O_ ( _w_ log _n_ ).


Then we define an embedding space in which context and value tokens are mapped to structured
representations. Specifically, a _i_ -th level context token is embedded as


**e** = [ **0** _, ...,_ **0** _,_ **z** _i,_ **0** _, ..., âˆ’_ 1]


where **z** _i_ is the context token representation, and the final coordinate is set to _âˆ’_ 1 to indicate that the
token is not a value. Similarly, a value token is embedded as


**e** = [ **0** _,_ **0** _, ...,_ **0** _,_ **v** _,_ 1]


where **v** is the value token representation and the final coordinate is set to 1 to mark it as a value
token. We keep the update rule of the generalized SSM as in Eq. 29:


_Ui_ = _u_ ( _Uiâˆ’_ 1 _,_ **e** ) = _Uiâˆ’_ 1 _âŠ™_ **1** [ **e** _j_ = 0] + **e** _âŠ™_ **1** [ **e** _j Ì¸_ = 0] (31)
_Ri_ = _r_ ( _Ui_ ) = _Ui_ (32)


which operates as a conditional overwrite. Following this update rule, the SSM state at each value
token takes the form


[ **z** 1 _,_ **z** 2 _, ...,_ **z** _w,_ **v** _,_ 1]


where **z** 1 _,_ **z** 2 _, ...,_ **z** _w_ are the _w_ levels of context representations of the current token, **v** is the value
representation, and the final dimension is set to 1 to indicate that the current token is a value token.


In the second layer, LSH attention operates on the SSM state [ **z** 1 _,_ **z** 2 _, ...,_ **z** _w,_ **v** _, is_ _ _v_ ], using

[ **z** 1 _,_ **z** 2 _, ...,_ **z** _w,_ **0** _, is_ _ _v_ ] as the LSH attention key representation, [ **z** 1 _,_ **z** 2 _, ...,_ **z** _w,_ **0** _,_ 1] as the LSH
attention query representation, and [ **0** _,_ **0** _, ...,_ **v** _,_ 1] as the LSH attention value representation. This
design ensures that tokens that share the same context (i.e., identical [ **z** 1 _,_ **z** 2 _, ...,_ **z** _w_ ] in the SSM state)
will always be assigned to the same hash bin. With a sufficient number of, e.g. _O_ ( _n_ ) hash bins,
which can be efficiently constructed using sign-bit binning rule with _O_ (log _n_ ) random projection


19


vectors, value representations associated with identical key combinations in the context are reliably
retrievable by LSH attention. This step is the bottleneck of computation with _O_ ( _wn_ log [2] _n_ ) time
complexity.


This construction establishes that a 2-layer hybrid model consisting of a generalized SSM followed
by LSH attention can solve multi-query multi-level joint recall efficiently, with sub-quadratic time
complexity and sub-linear state complexity with respect to the input sequence length.


**D** **Experiment Details**


**D.1** **Empirical Verification on Joint Recall**


For all models, we fix the number of layers to 2, set the hidden size to 64, and use _k_ = 64. For
variants that integrate multiple sparse attention components, namely SW+D, A (consists of a SW
component and a sink attention component, a sink attention always attends to the first _k_ tokens in the
sequence only), and HAX (LSH+KS), we allocate _k_ = 32 to each component, in order to maintain a
global _k_ = 64. For both LSH and HAX (LSH+KS), we adopt the sign-bit binning strategy (Eq. 7)
with _h_ = 8, and refresh the random hashing matrix at each training step. To account for the absence
of sparse attention mechanisms, we additionally evaluate Mamba and Mamba2 baselines with their
hidden size doubled to 128, ensuring a fair comparison. Additionally, we include a Samba baseline,
consisting of 2 Mamba layers and 2 sliding window attention layers. For Samba, the hidden size and
sliding window width ( _k_ ) are both set to 64. We use AdamW optimizer. All models are trained for
400,000 steps with a batch size of 64. Our implementation is based on Flash-Linear-Attention [42].


**D.2** **Continual Pre-training on Natural Language**


For all experiments, we fix the sparsity parameter _k_ = 128. For variants that integrate multiple sparse
attention components, namely SW+D, A, and HAX (LSH+KS), we set _k_ = 64 to each component,
in order to maintain a global _k_ = 128. For both LSH and HAX (LSH+KS), we adopt the argmax
binning strategy (Eq. 6) with _h_ = _k_ . We resample the random hashing matrix at each training step
and fix a random hashing matrix during evaluation.


To enhance the long-context modeling capability, we filter samples to retain only those with tokenized
lengths of at least 2,048 tokens. At the beginning of continual pre-training, the **K** and **Q** projection
weights are initialized using the parameters of the **B** and **C** projections from Mamba, respectively,
based on state-space duality [4]. During continual pretraining, we use a cosine schedule with a
maximum learning rate of 3e-4 for 130M models and 1.5e-4 for 790M models, and a minimum
learning rate of 1e-5. A warm-up phase of 200 steps with a learning rate of 0 followed by 800 steps
of linear warm-up precedes the cosine schedule. For instruction tuning, we also apply a 200-step 0
learning rate phase followed by 800 steps of linear warm-up, after which the learning rate remains
constant at 3e-6. For both continual pre-training and instruction-tuning, we use AdamW optimizer
and train with a global batch size of 64 and a context length of 2K.


**E** **Additional Experiments**


**E.1** **Short-context Modeling Benchmark**


We follow Mamba [14] to evaluate the zero-shot short context modeling performance of the continually pre-trained models on the LM evaluation harness benchmark from EleutherAI [12]. Our
results in Table 5 show that continual pre-training on long sequences will not lead to a significant
performance drop on short context benchmarks, where the Mamba w/o continual pre-training results
are copied from the Mamba paper [14].


20


Table 5: LM evaluation harness benchmark for continually pre-trained Mamba models. We compare
different sparse attention integrated with Mamba, including CISA and CDSA methods as in Table 2.


LambdaPPL WinoGrande PIQA LambdaAcc HellaSwag ARC-E ARC-C AverageAcc


Mamba 15.58 51.8 63.8 44.3 35.3 47.8 24.4 44.6
+D 15.75 52.8 64.4 43.9 35.3 47.7 24.1 44.7
+SW 15.47 52.8 63.8 44.3 35.2 47.9 24.7 44.8
+SW+D 15.35 53.0 64.0 44.7 35.2 47.6 24.4 44.8
+A 15.65 53.0 64.0 44.2 35.3 47.7 24.7 44.8
+LSH 15.73 52.6 64.3 43.8 35.3 47.7 24.0 44.6
+KS _(ours)_ 15.86 52.4 63.7 44.1 35.2 47.9 24.5 44.6
+HAX _(ours)_ 15.62 52.5 63.9 44.3 35.3 47.9 24.5 44.7
w/o training 16.07 51.9 64.5 44.3 35.3 48.0 24.3 44.7


Sliding Window Attention A-shaped Attention Dilated Attention Locality-Sensitive
Hashing Attention


Figure 8: Examples for input-dependent and input-independent sparse attention patterns.


Figure 9: An example of the "vertical-stripe" attention pattern in LLM. We input the first paragraph
of the Wikipedia term "Harry Potter" [41] into the Llama 3.2 1B model [13] and visualize the log
attention probabilities of the last head in the first layer. The input text is: â€œHarry Potter is a series of
seven fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young
wizard, Harry Potter, and his friends, Ron Weasley and Hermione Granger, all of whom are students
at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harryâ€™s conflict with
Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing
body known as the Ministry of Magic, and subjugate all wizards and Muggles (non-magical people).â€


21


