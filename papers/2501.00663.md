### Titans: Learning to Memorize at Test Time

Ali Behrouzâ€ , Peilin Zhongâ€ , and Vahab Mirrokniâ€ 


â€ Google Research


{alibehrouz, peilinz, mirrokni}@google.com


**Abstract**


Over more than a decade there has been an extensive research effort of how effectively utilize recurrent models and
attentions. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows
attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling
of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new
neural long-term memory module that learns to memorize historical context and helps an attention to attend to the
current context while utilizing long past information. We show that this neural memory has the advantage of a fast
parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its
limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its
ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce
a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate
memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics,
and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models.
They further can _effectively_ scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks
compared to baselines.


#### **1 Introduction**



â€œThe true art of memory is the art of attention!"




                                                  - Samuel Johnson, 1787


ransformers, pure attention-based architectures (Vaswani et al. 2017), have been firmly established as state-ofthe-art models in sequence modeling, mainly due to their in-context learning and ability to learn at scale (Kaplan
# T et al. 2020). The primary building blocks of Transformersâ€“attention modulesâ€”function as associative memory



ransformers, pure attention-based architectures (Vaswani et al. 2017), have been firmly established as state-ofthe-art models in sequence modeling, mainly due to their in-context learning and ability to learn at scale (Kaplan
# T et al. 2020). The primary building blocks of Transformersâ€“attention modulesâ€”function as associative memory

blocks (Bietti et al. 2024), where they learn to store key-value associations and retrieve them by computing pairwise
similarity between queries (i.e., search signals) and keys (i.e., contexts). Accordingly, by design, the output of a Transformer
is exclusively conditioned on the direct dependencies of tokens in the _current_ context window. This accurate modeling of
dependencies, however, comes with quadratic time and memory complexity in terms of the context length. In complex
real-world tasks (e.g., language modeling (N. F. Liu et al. 2024), video understanding (C.-Y. Wu et al. 2019), long-term time
series forecasting (H. Zhou et al. 2021)), the context window can become extremely large, making the applicability of
Transformers challenging in these downstream tasks.


To overcome the scalability issue of Transformers, recent studies aim to design different variants of linear Transformers (Kacham, Mirrokni, and P. Zhong 2024; Katharopoulos et al. 2020; S. Yang, B. Wang, Shen, et al. 2024), where softmax is
replaced by a kernel function in the attention (see Â§2.1 for details), resulting in a significant drop in memory consumption.
Despite efficiency and the ability to scale to longer context, linear Transformers do not show competitive performance
compared to Transformers as the kernel trick makes the model a linear recurrent network, in which the data is compressed
into a matrix-valued states (Katharopoulos et al. 2020). This, however, brings a contradictory fact about linear recurrent (or
linear Transformers) models: On one hand, we use these linear models to enhance scalability and efficiency (linear vs.
quadratic complexity), whose advantages is appeared for very long context; On the other hand, a very long context cannot
be properly compressed in a small vector-valued or matrix-valued states (S. Wang 2024).


1


Furthermore, beyond efficiency, most existing architecturesâ€“ranging from Hopfield Networks (Hopfield 1982) to LSTMs (JÃ¼rgen Schmidhuber and Hochreiter 1997) and Transformers (Vaswani et al. 2017)â€“face challenges when dealing with generalization, length extrapolation, and/or reasoning (Anil et al. 2022; Qin, Y. Zhong, and Deng 2024), all of which are inseparable
parts of many hard real-world tasks. Although these architectures draw inspiration from the human brain, each of which
are missing: (1) a crucial component for learning processâ€”such as short-term memory, long-term memory, meta-memory,
attending to current context, etc. (Cowan 2008); (2) how these components are interconnected systems that can operate
independently; and/or (3) the ability to _actively_ learn from data and memorize the abstraction of past history. We argue
that in an effective learning paradigm, similar to human brain, there are _distinct_ yet interconnected modules, each of which
is responsible for a component crucial to the learning process.


**Memory Perspective**


Memory is a fundamental mental process and is an inseparable component of human learning (Terry 2017). Without
a properly functioning memory system, humans and animals would be restricted to basic reflexes and stereotyped
behaviors. Accordingly, memory has been the inspiration for many seminal research in machine learning literature; e.g.,
Hopfield Networks (Hopfield 1982), LSTMs (JÃ¼rgen Schmidhuber and Hochreiter 1997), and Transformers (Vaswani et al.
2017).


Taking inspiration from the common definitions of memory and learning in neuropsychology literature (Okano, Hirano,
and Balaban 2000), most existing architectures consider memory as a neural update caused by an input, and define learning
as a process for acquiring effective and useful memory, given an objective. In this perspective, Recurrent Neural Networks
(RNNs) (Williams and Zipser 1989) can be defined as models with a vector-valued memory module M (also called hidden
state) with two main steps: Given a new input _ğ‘¥ğ‘¡_ at time _ğ‘¡_, the model (1) updates the memory using a function _ğ‘“_ (M _ğ‘¡_ âˆ’1 _,ğ‘¥ğ‘¡_ )
(with compression); and (2) retrieves the corresponding memory of input using a function _ğ‘”_ (M _ğ‘¡,ğ‘¥ğ‘¡_ ) (see Â§2.1 for details).
Similarly, Transformers can be seen as architectures with a growing memory and two similar steps. That is, the pair of key
and value matrices acts as the modelâ€™s memory, and the model: (1) updates the memory by appending the key and value to
the memory (without compression), and (2) retrieves query vectorsâ€™ corresponding memory by finding the similarity of
query and key vectors, which is then used to weight the value vectors for the output.


This perspective, can help us better understand existing paradigms, their critical differences, and design more effective
architectures. For example, the main difference between Transformers (Vaswani et al. 2017) and _linear_ Transformers (Katharopoulos et al. 2020) is the memory structure as well as the memory updating step, in which linear Transformers
compress the historical data into a fixed-size matrix-valued memory while Transformers keep all historical data (within
the context length) without any compression. While both linear Transformers and linear RNNs (including state space
models) compress the information in memory update step, the critical difference lies in the structure of the memory,
where linear RNNs (vs. linear Transformers) use a vector-valued memory (vs. matrix-valued memory). Therefore, this
perspective motivates us to ask: **(Q1)** What constitute a good structure for the memory? **(Q2)** What is a proper memory
update mechanism? and **(Q3)** What is a good memory retrieval process?


Revisiting our understanding of human memory, it is neither a unitary process nor it serves a single function (Cowan
2008). In fact, memory is a confederation of systemsâ€“e.g., short-term, working, and long-term memoryâ€“each serving a
different function with different neural structures, and each capable of operating independently (Willingham 1997). This
fact motivates us to ask: **(Q4)** How to design an efficient architecture that incorporates different interconnected memory
modules. Finally, storing a memory is a neural process that requires to encode and store the abstraction of the past. It can
be over-simplification to assume a single vector or a matrix, whose parameters are encoding the data in a linear manner,
are enough for storing long-term history. **(Q5)** Is a deep memory module needed to effectively store/remember long
past?


**Contributions and Roadmap**


In this paper, we aim to answer the above five questions by designing a long-term neural memory module, that can
efficiently and effectively learn to memorize at test time. Building upon its design, we discuss how it can be incorporated
into an architecture.


**Neural Memory** (Â§3). We present a (deep) neural long-term memory that (as a meta in-context model) learns how to
memorize/store the data into its parameters at test time. Inspired by human long-term memory system (Mandler 2014),


2


we design this memory module so an event that violates the expectations (being surprising) is more memorable. To this
end, we measure the surprise of an input with the gradient of the neural network with respect to the input in _associative_
_memory loss_ (see Â§3.1 for details). To better handle the limited memory, we present a decaying mechanism that consider the
proportion of memory size and the amount of data surprise, resulting in better memory management. We show that this
decay mechanism is in fact the generalization of forgetting mechanism in modern recurrent models (Dao and Gu 2024; Gu
and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024). Interestingly, we find that this mechanism is equivalent to optimizing
a meta neural network with mini-batch gradient descent, momentum, and weight decay. Building upon tensorizing
mini-batch gradient descent to use more matmul operations (Yu Sun et al. 2024), we present a fast and parallelizable
algorithm to train our deep neural long-term memory.


**Titans Architectures** (Â§4). After designing the long-term neural memory, an important remaining question is how to
effectively and efficiently incorporate memory into a deep learning architecture. We present Titans, a family of deep models
that consists of three hyper-heads: (1) Core: this module consists of the short-term memory, and is responsible for the main
flow of processing the data (we use attention with limited window size); (2) Long-term Memory: this branch is our neural
long-term memory module that is responsible to store/remember long past; (3) Persistent Memory: this is a set of learnable
but date-independent parameters that encodes the knowledge about a task. Finally, as a proof of concept, we present three
variants of Titans, in which we incorporate memory as: (i) a context, (ii) a layer, and (iii) a gated branch.


**Experimental Results** (Â§5). We perform experimental evaluations on language modeling, commonsense reasoning, recallintensive, needle in haystack, time series forecasting, and DNA modeling tasks. We observe that our Titan architecture
outperforms all modern recurrent models as well as their hybrid variants (combining with sliding-window attention) across
a comprehensive set of benchmarks. Furthermore, Titans outperforms Transformers with the same context window, and
show competitive performance with Transformers that use the entire context. This results are achieved while, contrary to
Transformers, Titans scale to larger than 2M context window size.

#### **2 Preliminaries**


n this section, we discuss the notation and some background concepts that we use though the paper. We let
_ğ‘¥_ âˆˆ R _[ğ‘]_ [Ã—] _[ğ‘‘]_ [in] be the input, M be a neural network (neural memory module), Q _,_ K _,_ V be the query, key and value
# I of the attention mechanism, and M be the attention mask. When segmenting the sequence, we use S [(] [ğ‘–] [)] to refer to

the _ğ‘–_ -th segment. Through the paper, we abuse the notation and use subscripts to refer to a specific element of a matrix,
vector, or segments. For example, we let S [(] _ğ‘—_ _[ğ‘–]_ [)] be the _ğ‘—_ -th token in the _ğ‘–_ -th segment. The only exception is subscripts with _ğ‘¡_,
which we reserved to index recurrence over time, or the state of a neural network at time _ğ‘¡_ . Given a neural network N and
a data sample _ğ‘¥_, we use N ( _ğ‘¥_ ) (resp. N [âˆ—] ( _ğ‘¥_ )) to refer to the forward pass with (resp. without) weight adjustment. Also, we
abuse the notation and use N [(] _[ğ‘˜]_ [)] to refer to the _ğ‘˜_ -th layer of the neural network. In the following, we first, discuss the
backgrounds for attention and its efficient variants followed by a review of modern linear RNNs. Finally, we discuss a
memory perspective of these architectures that motivates us to design Titans.


**2.1** **Backgrounds**


**Attention.** Transformers (Vaswani et al. 2017) as the de facto backbone for many deep learning models are based on
attention mechanism. Given input _ğ‘¥_ âˆˆ R _[ğ‘]_ [Ã—] _[ğ‘‘]_ [in], causal attention computes output y âˆˆ R _[ğ‘]_ [Ã—] _[ğ‘‘]_ [in] based on softmax over input
dependent key, value, and query matrices:


Q = _ğ‘¥_ WQ _,_ K = _ğ‘¥_ WK _,_ V = _ğ‘¥_ WV _,_ (1)




   -    exp Q _ğ‘–_ [âŠ¤][K] _[ğ‘—]_ [/âˆš] _[ğ‘‘]_ [in] V _ğ‘—_
_,_ (2)

~~ï¿½~~ _ğ‘–â„“_ =1 [exp] ~~ï¿½~~ Q _ğ‘–_ [âŠ¤][K] _[â„“]_ [/] ~~[âˆš]~~ _[ğ‘‘]_ [in] ~~ï¿½~~



y _ğ‘–_ =



_ğ‘–_
âˆ‘ï¸


_ğ‘—_ =1



where WQ _,_ WK _,_ and WV âˆˆ R _[ğ‘‘]_ [in] [Ã—] _[ğ‘‘]_ [in] are learnable parameters. Despite the power and effectiveness in recall, transformers
need at least _ğ‘_ Ã— _ğ‘‘_ operators to calculate the output, resulting in larger memory consumption and lower-throughput for
longer sequences.


**Efficient Attentions.** To improve the memory consumption and throughput of softmax attention for longer sequences,
various studies focused on I/O aware implementations of attention (Dao 2024; Dao, D. Fu, et al. 2022), designing more


3


efficient attention mechanisms by sparsifying the attention matrix (B. Chen et al. 2021; Choromanski et al. 2021; Dai et al.
2019), approximating the softmax (Arora et al. 2024), or developing kernel-based (linear) attentions (Aksenov et al. 2024;
Kacham, Mirrokni, and P. Zhong 2024; Schlag, Irie, and JÃ¼rgen Schmidhuber 2021; S. Yang, B. Wang, Shen, et al. 2024). In
this part, we focus on the later, i.e., linear attentions, where the softmax in standard attention is replaced with an alternative
kernel function _ğœ™_ ( _., ._ ), such that _ğœ™_ ( _ğ‘¥,ğ‘¦_ ) = _ğœ™_ ( _ğ‘¥_ ) _ğœ™_ ( _ğ‘¦_ ). Accordingly, the attention can be written as:



_ğ‘–_
âˆ‘ï¸


_ğ‘—_ =1



_ğœ™_ ( _ğ‘„ğ‘–_ ) [âŠ¤] _ğœ™_ ( _ğ¾ğ‘—_ ) _ğœ™_ ( _ğ‘„ğ‘–_ ) [âŠ¤] [ï¿½] _[ğ‘–]_ _ğ‘—_ =1 _[ğœ™]_ [(] _[ğ¾][ğ‘—]_ [)] _[ğ‘‰][ğ‘—]_

~~ï¿½~~ _ğ‘–â„“_ =1 _[ğœ™]_ [(] _[ğ‘„][ğ‘–]_ [)][âŠ¤] _[ğœ™]_ [(] _[ğ¾][â„“]_ [)] _ğ‘‰ğ‘—_ = _ğœ™_ ( _ğ‘„ğ‘–_ ) [âŠ¤] ~~[ï¿½]~~ _[ğ‘–]_ _â„“_ =1 _[ğœ™]_ [(] _[ğ¾][â„“]_ [)] _,_ (3)



_ğœ™_ ( _ğ‘„ğ‘–_ [âŠ¤] _[ğ¾][ğ‘—]_ [)]

~~ï¿½~~ _ğ‘–_ _ğ‘‰ğ‘—_ =
_â„“_ =1 _[ğœ™]_ [(] _[ğ‘„]_ _ğ‘–_ [âŠ¤] _[ğ¾][â„“]_ [)]



y _ğ‘–_ =



_ğ‘–_
âˆ‘ï¸


_ğ‘—_ =1



resulting in a higher-throughput as terms [ï¿½] _[ğ‘–]_ _ğ‘—_ =1 _[ğœ™]_ [(] _[ğ¾][ğ‘—]_ [)][ and][ ï¿½] _[ğ‘–]_ _â„“_ =1 _[ğœ™]_ [(] _[ğ¾][â„“]_ [)][ are re-using in each step. When choosing the kernel]
as identity matrix (Yutao Sun et al. 2023), the above formulation can also be written in a recurrent format:


M _ğ‘¡_ = M _ğ‘¡_ âˆ’1 + _ğ¾ğ‘¡_ [âŠ¤] _[ğ‘‰][ğ‘¡]_ _[,]_ (4)

y _ğ‘¡_ = _ğ‘„ğ‘¡_ M _ğ‘¡_ _,_ (5)


which allows efficient inference for linear attentions.


**Modern Linear Models and Their Memory Perspective.** As discussed earlier, one can define learning as a process for
acquiring effective and useful memory. Building upon this, one can see the hidden state of Recurrent Neural Networks
(RNNs) as a memory unit, which the model aims to compress the information into. Accordingly, in a general form of
recurrent neural network, the hidden state can be treated as a memory unit and the recurrence process can be split into the
_read_ and _write_ operations in the memory unit. That is, we let _ğ‘¥_ âˆˆ R _[ğ‘]_ [Ã—] _[ğ‘‘]_ [in] be the input, M âˆˆ R _[ğ‘‘]_ is the memory unit, and
y âˆˆ R _[ğ‘‘]_ [in] is the output, then the general form of the recurrent neural network is defined as:


M _ğ‘¡_ = _ğ‘“_ (M _ğ‘¡_ âˆ’1 _,ğ‘¥ğ‘¡_ ) _,_ Write Operation (6)

y _ğ‘¡_ = _ğ‘”_ (M _ğ‘¡,ğ‘¥ğ‘¡_ ) _,_ Read Operation (7)


where _ğ‘“_ ( _., ._ ) is the _read_ and _ğ‘”_ ( _., ._ ) is the _write_ corresponding functions. Note that here the subscript of M _ğ‘¡_ shows the state
of the memory at time _ğ‘¡_ .


In this perspective, the recurrence formula of linear Transformers (see Equation 4) is equivalent to additively compress
and write keys and values, ( _ğ¾ğ‘¡,ğ‘‰ğ‘¡_ ), into a matrix-valued memory unit M _ğ‘¡_ . Therefore, when dealing with long context
data, this additive nature of the process results in memory overflow, significantly damaging the performance of the model.
To address this, studies have focused on two promising directions: (1) Adding forget mechanism: several studies have
presented adaptive (data-dependent) forgetting gate mechanisms for linear models, where it can erase the memory when it
is needed. As examples of such models, we refer to GLA (S. Yang, B. Wang, Shen, et al. 2024), LRU (Orvieto et al. 2023),
Griffin (De et al. 2024), xLSTM (Beck et al. 2024), and Mamba2 (Dao and Gu 2024), which the later is also connected to the
discretized version of traditional state space models (Gu and Dao 2024).(2) Improving the write operation: To overcome the
additive nature of memory write operation in traditional recurrent models, Widrow and Hoff (1988) presented Delta Rule,
in which before adding a memory (i.e., a pair of key and value), the model first removes its past value. To enhance the
parallelizable training and scaling, S. Yang, B. Wang, Yu Zhang, et al. (2024) present a fast paralellizable algorithm. Finally,
very recently, S. Yang, Kautz, and Hatamizadeh (2024) improved the DeltaNets by adding a forget gate.


**Memory Modules.** Memory has always been one of the core parts of the neural network designs (Graves, Wayne,
and Danihelka 2014; JH Schmidhuber 1992; JÃ¼rgen Schmidhuber and Hochreiter 1997; J. Zhang et al. 2024). The idea of
seeing linear layers as the key-value (associative) memory system backs to fast weight programs, in which dynamic fast
programs are incorporated into recurrent neural networks to serve as writable memory (JH Schmidhuber 1992). The two
learning rules of Hebbian (Hebb 2005) and delta (Prados and Kak 1989) are the most popular learning rules for fast weight
programs, which have been extensively explored in various studies (Irie, Schlag, et al. 2021; Munkhdalai, Sordoni, et al.
2019; Munkhdalai and H. Yu 2017; Schlag, Irie, and JÃ¼rgen Schmidhuber 2021; JH Schmidhuber 1992; S. Yang, Kautz, and
Hatamizadeh 2024; S. Yang, B. Wang, Yu Zhang, et al. 2024). All these models, however, are based on momentary surprise,
missing the token flow in the sequences (see Section 3.1), and most of them lacks a forgetting gate, resulting in a poor
memory management.


We further discuss the connection of our architectures with recent models in Appendix C. Additional related work are
discussed in Appendix A.


4


#### **3 Learning to Memorize at Test Time**


    - overcome the lack of long-term memory and to enable the model to learn, forget, and retrieve information, in
this section, we present a neural long-term memory module, which is a meta models that learns to memorize at
# T test time. In Section 3.1, we first discuss the motivation and the design of the neural memory. In Section 3.2, we

discuss how our architecture design can benefit from a fast and parallelizable training. Finally, in Section 3.3, we augment
our architecture using persistent memory module, in which we use learnable but data-independent parameters to learn
meta information about the task.


**3.1** **Long-term Memory**


To design a neural long-term memory module, we need a model that can encode the abstraction of the past history into its
parameters. An example of this can be LLMs that are shown to be memorizing their training data (Leybzon and Kervadec
2024; Schwarzschild et al. 2024; Staab et al. 2024). Therefore, a simple idea is to train a neural network and expect it to
memorize its training data. Memorization, however, has almost always been known as an undesirable phenomena in
neural networks as it limits the model generalization (Bayat et al. 2024), causes privacy concerns (Staab et al. 2024), and
so results in poor performance at test time. Moreover, the memorization of the training data might not be helpful at test
time, in which the data might be out-of-distribution. We argue that, we need an online meta-model that learns how to
memorize/forget the data at test time. In this setup, the model is learning a function that is capable of memorization, but it
is not overfitting to the training data, resulting in a better generalization at test time.


**Learning Process and Surprise Metric.** The key idea to train a long-term memory is to treat its training as an online
learning problem, in which we aim to compress the past information _ğ‘¥_ 1 _, . . .,ğ‘¥ğ‘¡_ âˆ’1 into the parameters of our long-term
neural memory module M _ğ‘¡_ . As discussed earlier, an event that violates the expectations (i.e., is surprising) is more
memorable for humans (Mandler 2014). Inspired by this, a simple definition of surprise for a model can be its gradient with
respect to the input. The larger the gradient is, the more different the input data is from the past data. Accordingly, using
this surprise score, we can update the memory as:



M _ğ‘¡_ = M _ğ‘¡_ âˆ’1 âˆ’ _ğœƒğ‘¡_ âˆ‡ _â„“_ (M _ğ‘¡_ âˆ’1; _ğ‘¥ğ‘¡_ )
**ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½** **ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½**

      - ï¿½ï¿½      Surprise



_._ (8)



This surprise metric, however, can result in missing important information that comes after a big surprising moment.
That is, the gradient can become extremely small after several surprising steps, leading to stocking in a flat area (i.e., local
minima), and missing information about some parts of the sequence. From the human memory perspective, an event might
not consistently surprise us through a long-period of time although it is memorable. The reason is that the initial moment
is surprising enough to get our attention through a long time frame, leading to memorizing the entire time frame. To
improve the above surprise metric (Equation 8), we break the surprise metric into (1) _past surprise_, which measures the
surprise amount of a very recent past; and (2) _momentary surprise_, which measures the surprise of incoming data:


M _ğ‘¡_ = M _ğ‘¡_ âˆ’1 + _ğ‘†ğ‘¡,_ (9)



_._ (10)



_ğ‘†ğ‘¡_ = _ğœ‚ğ‘¡_ _ğ‘†ğ‘¡_ âˆ’1

ï¿½ï¿½ï¿½ï¿½
Past Surprise




- _ğœƒğ‘¡_ âˆ‡ _â„“_ ( _ğ‘€ğ‘¡_ âˆ’1; _ğ‘¥ğ‘¡_ )
**ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½** **ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½**

  - ï¿½ï¿½   Momentary Surprise



Interestingly, this formulation is similar to gradient descent with momentum, where _ğ‘†ğ‘¡_ is the momentum element. Therefore,
the momentum here act as a memory of surprise across time (sequence length). In this formulation, the term _ğœ‚ğ‘¡_ is a
data-dependent surprise decay (a function of _ğ‘¥ğ‘¡_ ), controlling how surprise decays over time, and the term _ğœƒğ‘¡_ is controlling
how much of momentary surprise should be incorporated into the final surprise metric in a data-dependent manner. This
data-dependency is particularly important in this design: While surprise of previous tokens might be needed to affect
the surprise of the next token, it is mostly valid if all tokens are relevant and are in the same context. Accordingly, a
data-dependent _ğœ‚_ can control if memory needs to: (1) ignore the last surprise by setting _ğœ‚ğ‘¡_ â†’ 0 (possibly due to the change
of context), or (2) fully incorporate the last surprise by setting _ğœ‚ğ‘¡_ â†’ 1 (possibly as the token is highly relevant to its recent
past tokens).


**Objective.** Our above surprise metric is based on a loss function _â„“_ ( _._ ; _._ ), which is the objective that our memory is learning
to act as it at test time. That is, our memory module is a meta model that learns a function based on the loss function _â„“_ ( _._ ; _._ ).


5


In this work, we focus on _associative memory_, in which we aim to store the past data as the pairs of keys and values. Given
_ğ‘¥ğ‘¡_, similar to Transformers (Vaswani et al. 2017), we use two linear layers to project _ğ‘¥ğ‘¡_ into a key and value:


k _ğ‘¡_ = _ğ‘¥ğ‘¡ğ‘Šğ¾,_ v _ğ‘¡_ = _ğ‘¥ğ‘¡ğ‘Šğ‘‰_ _,_ (11)


where _ğ‘Šğ¾_ and _ğ‘Šğ‘‰_ âˆˆ R _[ğ‘‘]_ [in] [Ã—] _[ğ‘‘]_ [in] . Next, we expect our memory module to learn the associations between keys and values. To
this end, we define the loss as follows:


_â„“_ (M _ğ‘¡_ âˆ’1; _ğ‘¥ğ‘¡_ ) = âˆ¥M _ğ‘¡_ âˆ’1 (k _ğ‘¡_ ) âˆ’ v _ğ‘¡_ âˆ¥ [2] 2 (12)


By optimizing the above loss function in the inner-loop of our meta model (memory), the model learns how to memorize
the mapping between keys and values at test time. Note that, similar to meta-learning models (Nichol 2018; Zintgraf et al.
2019), training of the memory is in the inner-loop, and so parameters _ğ‘Šğ¾_ and _ğ‘Šğ‘‰_ are hyperparameters in the above loss
function. Accordingly, in the inner loop, we optimize Mâ€™s weights, while in the outer-loop, we optimize other parameters
of the entire architecture.


**Forgetting Mechanism.** When dealing with very large sequences (e.g., millions of tokens), it is crucial to manage which
past information should be forgottenâ€“even with a deep or a very large matrix-valued memory. To this end, we use an
adaptive forgetting mechanism that allows the memory to forget the information that is not needed anymore, resulting in
better managing the memoryâ€™s limited capacity. That is, given the next token _ğ‘¥ğ‘¡_, we modify the update rule as:


M _ğ‘¡_ = (1 âˆ’ _ğ›¼ğ‘¡_ )M _ğ‘¡_ âˆ’1 + _ğ‘†ğ‘¡,_ (13)

_ğ‘†ğ‘¡_ = _ğœ‚ğ‘¡ğ‘†ğ‘¡_ âˆ’1 âˆ’ _ğœƒğ‘¡_ âˆ‡ _â„“_ ( _ğ‘€ğ‘¡_ âˆ’1; _ğ‘¥ğ‘¡_ ) _,_ (14)


where _ğ›¼ğ‘¡_ âˆˆ[0 _,_ 1] is the gating mechanism that flexibly controls the memory; i.e., decides how much information should be
forgotten. For example, it can update the memory without affecting the past abstraction by letting _ğ›¼ğ‘¡_ â†’ 0, and can clear
the entire memory by letting _ğ›¼ğ‘¡_ â†’ 1. Later in this section, we show that this weight decay mechanism is closely related to
the gating mechanism in modern RNNs (Dao and Gu 2024; Orvieto et al. 2023).


**Memory Architecture.** In this paper, we focus on simple MLPs with _ğ¿_ M â‰¥ 1 layers as the architecture of our long-term
memory. The main reason behind this choice is that we want to focus on better motivating the design of the long-term
memory and ways that it can be incorporated into an architecture. However, our formulation and architectural design
opens a new research direction to design neural architectures that are more effective and efficient in memorization of data.
Recently, there has been a promising line of work to design such architectures (Berges et al. 2024; Cetin et al. 2024; J. Zhang
et al. 2024), which incorporating them into our framework (i.e., replacing simple MLPs with such architectures) can be an
interesting future work.


When using vector-valued or matrix-valued memory (De et al. 2024; Orvieto et al. 2023; S. Yang, B. Wang, Shen, et
al. 2024), the memory module is compressing the past data and fit it into a line. That is, from the meta learning or
online learning perspective (Yu Sun et al. 2024), using a matrix-valued memory M = _ğ‘Š_ âˆˆ R _[ğ‘‘]_ [in] [Ã—] _[ğ‘‘]_ [in] is equivalent to
optimize _â„“_ ( _ğ‘Šğ‘¡_ âˆ’1; _ğ‘¥ğ‘¡_ ) = âˆ¥ _ğ‘Šğ‘¡_ âˆ’1k _ğ‘¡_ - v _ğ‘¡_ âˆ¥ [2] 2 [, which is an online linear regression objective and so the optimal solution assumes]
the underlying dependency of historical data is linear. On the other hand, we argue that deep memory modules (i.e.,
_ğ¿_ M â‰¥ 2) . Aligning with the theoretical results that MLPs with at least two layers are strictly more expressive than linear
models (Hornik, Stinchcombe, and White 1989), in Section 5.5, we show that deep memory modules are more effective in
practice.


**Retrieving a Memory.** In the above, we discuss how one can design and train a long-term memory module that learns to
memorize at test time. A key remaining question is: _How one can retrieve information from the memory?_ We simply use the
forward pass without weight update (i.e., inference) to retrieve a memory correspond to a query. Formally, given an input
_ğ‘¥ğ‘¡_, we use a linear layer _ğ‘Šğ‘„_ to project the input, i.e., q _ğ‘¡_ = _ğ‘¥ğ‘¡ğ‘Šğ‘„_ and retrieve the corresponding (or useful) information
from the memory _ğ‘¦ğ‘¡_ by:


_ğ‘¦ğ‘¡_ = M [âˆ—] (q _ğ‘¡_ ) _._ (15)


6


Figure 1: The illustration of how the training of neural memory can be done in parallel and using matmuls.


**3.2** **How to Parallelize the Long-term Memory Training**


As discussed above, the design of our long-term memory module is equivalent to training a meta model by optimizing
associative memory loss function _â„“_ (M _ğ‘¡_ âˆ’1; _ğ‘¥ğ‘¡_ ) = âˆ¥M _ğ‘¡_ âˆ’1 (k _ğ‘¡_ ) âˆ’ v _ğ‘¡_ âˆ¥ [2] 2 [using gradient descent with momentum and weight]
decay. Therefore, in theory, the training of long-term memory module requires O ( _ğ‘_ ) FLOPs, where _ğ‘_ is the sequence
length. However, in practice, we need to parallelize the training process and to fully take advantage of hardware accelerators
(e.g., TPUs, GPUs), we need to tensorize the process and use more matmuls.


Next, we show that calculating the weights in the inner loop with mini-batch gradient descent, data-dependent learning
rate, and weight decay can be reformulated so that it uses only matmuls and sum. We build upon the work of Yu Sun et al.
(2024) that shows forward pass of a model optimizing with the mini-batch gradient descent (with constant learning rate)
can be calculated using matmuls. We can split the sequence into chunks of size _ğ‘_ â‰¥ 1, and write the mini-batch gradient
descent as:



M _ğ‘¡_ = (1 âˆ’ _ğ›¼ğ‘¡_ )M _ğ‘¡_ âˆ’1 âˆ’ _ğœƒğ‘¡_ âˆ‡ _â„“_ (M _ğ‘¡_ âˆ’1; _ğ‘¥ğ‘¡_ ) = _ğ›½ğ‘¡_ M0 âˆ’



_ğ‘¡_
âˆ‘ï¸ _ğ›½ğ‘¡_

_ğœƒğ‘–_ âˆ‡ _â„“_ (M _ğ‘¡_ [â€²] ; _ğ‘¥ğ‘–_ ) _,_ (16)
_ğ‘–_ =1 _ğ›½ğ‘–_



where _ğ‘¡_ [â€²] = _ğ‘¡_ - mod( _ğ‘¡,ğ‘_ ), and _ğ›½ğ‘–_ = [ï¿½] _[ğ‘–]_ _ğ‘—_ =1 [(][1][ âˆ’] _[ğ›¼]_ _[ğ‘—]_ [)][. For the sake of simplicity, we focus on the first chunk, i.e.,] _[ ğ‘¡]_ [=] _[ ğ‘]_ [and so]
_ğ‘¡_ [â€²] = 0. Also, we explain the process for the case that M _ğ‘¡_ = _ğ‘Šğ‘¡_ is linear. The process for MLPs with _ğ‘ğ‘_ â‰¥ 2 is similar. Using
our loss function, we have:



_ğ‘_

âˆ‡ _â„“_ ( _ğ‘Š_ 0; _ğ‘¥ğ‘–_ ) = Î˜ _ğ‘_ B _ğ‘_ ( _ğ‘Š_ 0 _ğ‘‹_  - _ğ‘‹_ ) _ğ‘‹_ [âŠ¤] _,_ (17)
_ğ›½ğ‘–_



âˆ‡ _â„“_ ( _ğ‘Š_ 0; _ğ‘¥ğ‘¡_ ) = ( _ğ‘Š_ 0 _ğ‘¥ğ‘¡_ - _ğ‘¥ğ‘¡_ ) _ğ‘¥ğ‘¡_ [âŠ¤] [â‡’]



_ğ‘_
âˆ‘ï¸



âˆ‘ï¸ _ğ›½ğ‘_

_ğœƒğ‘–_
_ğ‘–_ =1 _ğ›½ğ‘–_



where Î˜ _ğ‘_ = diag [ï¿½ï¿½] _ğœƒ_ 1 _ğœƒ_ 2 _. . ._ _ğœƒğ‘_ ï¿½ï¿½ and B _ğ‘_ is defined analogously on _ğ›½ğ›½ğ‘ğ‘–_ [s. Note that, we do not need to store all][ Î˜] _[ğ‘˜ğ‘]_ [and]

B _ğ‘˜ğ‘_ for _ğ‘˜_ = 1 _, . . ., ğ‘_ / _ğ‘_, instead, we store these matrices for each chunk, resulting in using less memory. Next, we extend
this representation so we can also incorporate the momentum term. In a chunk wise gradient descent with momentum, if
we look at the momentum term, we have:


_ğ‘†ğ‘¡_ = _ğœ‚ğ‘¡ğ‘†ğ‘¡_ âˆ’1 âˆ’ _ğœƒğ‘¡_ _ğ‘¢ğ‘¡,_ (18)


where _ğ‘¢ğ‘¡_ = âˆ‡ _â„“_ ( _ğ‘€ğ‘¡_ [â€²] ; _ğ‘¥ğ‘¡_ ). Note that, we can compute all _ğ‘¢ğ‘¡_ at the same time, and so Equation 18 is a linear recurrence
with _ğ‘¢ğ‘¡_ as an input, _ğ‘†ğ‘¡_ as the hidden state, and _ğœ‚ğ‘¡_ as input-dependent transition value. Accordingly, we can use parallel
associative scan (J. T. Smith, Warrington, and Linderman 2023) to calculate _ğ‘†ğ‘¡_ s in this chunk.


**Parameters as the Function of Chunks.** Instead of making parameters like _ğ›¼ğ‘¡,ğœƒğ‘¡_, and _ğœ‚ğ‘¡_ input-dependent (i.e., a function
of token _ğ‘¥ğ‘¡_ ), we can make them functions of their chunk. Despite losing expressive power, this formulation can help to
make the training even faster. In this case, we are using the same value for each of _ğ›¼_, _ğœƒ_, and _ğœ‚_ in each chunk. Accordingly,
in Equation 17, we can store Î˜ using a single scaler. Similarly we can make Equation 18 faster. That is, when _ğœ‚_ and _ğœƒ_ are
learnable but time-invariant inside each chunk, this equation becomes a linear time-invariant system (LTI), which can be
computed by a global convolution (Gu, Goel, and Re 2022). In our experiments, we make these parameters as the functions
of tokens. However, such simplifications (i.e., as the function of chunks) can be the interest of future work to training
larger models in more efficient manner.


7


Figure 2: **Memory as a Context (MAC) Architecture.** This architecture includes three branches of (1) core, (2) contextual
(long-term) memory, and (3) persistent memory. The core branch concatenates the _corresponding_ long-term and persistent
memories with the input sequence. Next, attention performs on the sequence and decides what part of the information
should store in the long-term memory. At the test time, parameters corresponds to contextual memory are still learning,
parameters corresponds to the core branch are responsible for in-context learning, and parameters of persistent memory
are responsible to store the knowledge about tasks and so are fixed.


**3.3** **Persistent Memory**


Our long-term memory can also be seen as a contextual memory, meaning that the output is fully depend on the context.
Therefore, in addition to our long-term memory, we also use a set of learnable but input-independent parameters to act as
task-related memory. This type of memory has been referred to as persistent or meta-memory in the literature (X. Dong
et al. 2024; Sukhbaatar, Grave, et al. 2019). Given _ğ‘ğ‘_ â‰¥ 1, we use learnable parameters _ğ‘ƒ_ = - _ğ‘_ 1 _ğ‘_ 2 _. . ._ _ğ‘ğ‘ğ‘_ - and
append it to the start of our sequence: i.e., given a context window size of _ğ‘_, we modify the input as:


_ğ‘¥_ new =                    - _ğ‘_ 1 _ğ‘_ 2 _. . ._ _ğ‘ğ‘ğ‘_                    - || _ğ‘¥,_ (19)


where || is concatenation. Next, we discuss the motivation of persistent memory from three perspective:


**Memory Perspective.** As discussed earlier, our neural long-term memory is a contextual memory, in which all parameters
are input-dependent. An effective memory system, however, also needs input-independent parameters to store the
abstraction of the task knowledge. That is, mastering a task requires the memorization of the knowledge that how the task
can be done, and these parameters are responsible for storing such knowledge.


**Feedforward Network Perspective.** In the Transformer architectures, there are fully connected layers after the attention
module, which are shown to be similar to attention weights but with data-independent parameters. That is, Sukhbaatar,
Grave, et al. (2019) showed that replacing the ReLU in fully connected layers with Softmax can results in an attention-like
weights, in which weights are data-independent:


_ğ¹ğ¹ğ‘_ ( _ğ‘¥_ ) = _ğ‘Šğ‘‰_ Softmax ( _ğ‘Šğ¾ğ‘¥_ ) _._ (20)


In fact, _ğ‘Šğ¾_ and _ğ‘Šğ‘‰_ are acting similar to _ğ¾_ and _ğ‘‰_ matrices in attention module when they are input-independent. The
persistent memory weights are expected to have the same functionality, meaning that using them in the first part of the
sequence leads to having input-independent attention weights (Sukhbaatar, Grave, et al. 2019).


**Technical Perspective.** Attention with causal mask has implicit bias toward initial tokens in the sequence, and so attention
weights are almost always highly active for initial tokens, resulting in performance damage. From the technical perspective,
these learnable parameters at the start of the sequence can mitigate such effect by redistributing the attention weights
more effectively (Han et al. 2024; Xiao et al. 2024).


8


(a) **Memory as a Context (MAC).** We segment the sequence
and use full causal attention in each window. Again, the first
_ğ‘ğ‘_ tokens are persistent memory and the next _ğ‘ğ‘™_ are long-term
memory tokens



(b) **Memory as Gating (MAG).** We use sliding window attention
(SWA) as a short-term memory and our neural memory module
as a long-term memory, combining by a gating.



Figure 3: Attention masks for different variants of Titans.

#### **4 How to Incorporate Memory?**


n important question that remained unanswered is: How one can effectively and efficiently incorporate the
designed neural memory into a deep learning architecture? As discussed earlier, from a memory perspective,
# A the pair of K and V matrices in transformers can be interpreted as an associative memory block. Due to their

accurate modeling of dependencies and so their limited context window, we interpret them as short-term memory modules,
attending to the _current_ context window size. On the other hand, our neural memory with the ability to continuously
learn from data and store it in its weights can play the role of a a long-term memory. In this section, we aim to answer
the above question by proposing three different variants of Titans. Later in our experiments, we show that each of these
variants has its own advantages/disadvantages and also can show a trade-off between the efficiency and effectiveness in
very long-contexts.


**4.1** **Memory as a Context**


In the first architecture design (see Figure 2), we treat the memory as a context to the current information. That is, given
a long sequence _ğ‘¥_ âˆˆ R _[ğ‘]_ [Ã—] _[ğ‘‘]_ [in], we first chunk the sequence into fixed-size segments S [(] _[ğ‘–]_ [)] for _ğ‘–_ = 1 _, . . ., ğ‘_ / _ğ¶_ . Given the
incoming segment S [(] _[ğ‘¡]_ [)], we consider it as the current context and its past segment as the historical information. Therefore,
let M _ğ‘¡_ âˆ’1 be the state of long-term memory before segment S [(] _[ğ‘¡]_ [)], we use the input context as the query to the memory
M _ğ‘¡_ âˆ’1 to retrieve the corresponding information from the long-term memory. That is, we retrieve the past information that
corresponds to S [(] _[ğ‘¡]_ [)] as:


_â„ğ‘¡_ = M _ğ‘¡_ [âˆ—] âˆ’1 [(][q] _[ğ‘¡]_ [)] _[,]_ (21)


where q _ğ‘¡_ = S [(] _[ğ‘¡]_ [)] _ğ‘Šğ‘„_ . Next, we use this historical information along with our persistent memory parameters as the input
sequence to the attention module:


SËœ ( _ğ‘¡_ ) =                                 - _ğ‘_ 1 _ğ‘_ 2 _. . ._ _ğ‘ğ‘ğ‘_                                 - || _â„ğ‘¡_ || S [(] _[ğ‘¡]_ [)] _,_ (22)

_ğ‘¦ğ‘¡_ = Attn ï¿½SËœ ( _ğ‘¡_ ) [ï¿½] _._ (23)


The structure of the attention map over the entire sequence is shown in Figure 3a. We then use _ğ‘¦ğ‘¡_ to update the long-term
memory module for the next segment and the final output:


M _ğ‘¡_ = M _ğ‘¡_ âˆ’1 ( _ğ‘¦ğ‘¡_ ) _,_ (24)

_ğ‘œğ‘¡_ = _ğ‘¦ğ‘¡_ âŠ—M _ğ‘¡_ [âˆ—] [(] _[ğ‘¦][ğ‘¡]_ [)] _[ .]_ (25)


Note that, in the above, we are updating the weight of M _ğ‘¡_ âˆ’1 through forward pass.


This architecture has two key advantages: (1) Attention by having both historical and current context, has the ability to
decides whether given the current data, the long-term memory information is needed. (2) The attention module helps


9


Figure 4: **Memory as a Gate (MAG) Architecture.** This architecture, similarly, has the three branches of (1) core, (2)
contextual memory, and (3) persistent memory. It, however, incorporates only persistent memory into the context and
combine memory with the core branch using a gating mechanism. At test time, the behavior is the same as Figure 2.


the long-term memory to store only useful information from the current context. That is, not all tokens in each segment
are useful and memorizing all of them can result in memory overflow. Therefore, attention is helping the memory to
understand which information is useful, better managing the memory capacity. (3) At test time: (i) persistent memory
parameters are fixed as they encodes the knowledge about the task, which should not be changed; (ii) the attention module
weights are in-context learner; and (iii) the long-term memory module is still learning (memorizing) the information at test
time. That is, we update the weights of the neural memory even at test time as weights are encoding the abstraction of
long past.


**4.2** **Gated Memory**


In the next variant (see Figure 4), in one branch, we directly use the input data to update the long-term memory, and in the
second branch, we use a sliding window attention (SWA):


_ğ‘¥_ Ëœ =                    - _ğ‘_ 1 _ğ‘_ 2 _. . ._ _ğ‘ğ‘ğ‘_                    - || _ğ‘¥,_ (26)

_ğ‘¦_ = SW-Attn [âˆ—] ( Ëœ _ğ‘¥_ ) _,_ (27)

_ğ‘œ_ = _ğ‘¦_ âŠ—M( Ëœ _ğ‘¥_ ) _,_ (28)


where SW-Attn [âˆ—] is sliding window attention with prefix (see Figure 3b). Note that, contrary to the previous design, we are
not segmenting the input data. Also, we abuse the notation and use M( _ğ‘¥_ ) to refer to the final output of the memory after
all recursion over the tokens of the sequence. In the above equation, âŠ— can be any non-linear gating. In our experiments,
we normalize the outputs _ğ‘¦_ and M( Ëœ _ğ‘¥_ ) using learnable vector-valued weights, followed by a non-linearity _ğœ_ ( _._ ).


The overall attention mask of this design is shown in Figure 3b. In this design, sliding window attention is act as a precise
short-term memory, while the neural memory module is acting as a fading memory for the model. This architecture design
can also be seen as a multi-head architecture where the structure of heads are different (X. Dong et al. 2024).


**4.3** **Memory as a Layer**


The last variant uses the neural Memory As a Layer (MAL) of a deep neural network (see Figure 5). This architecture
design is more common in the literature, where the hybrid models stack recurrent models with full or sliding window
attentions. Given input _ğ‘¥_, we have:


_ğ‘¥_ Ëœ =                    - _ğ‘_ 1 _ğ‘_ 2 _. . ._ _ğ‘ğ‘ğ‘_                    - || _ğ‘¥,_ (29)

_ğ‘¦_ = M( Ëœ _ğ‘¥_ ) _,_ (30)


_ğ‘œ_ = SW-Attn ( _ğ‘¦_ ) _,_ (31)


10


Figure 5: **Memory as a Layer (MAL) Architecture.** In this architecture, the memory layer is responsible to compress the
past and current context before the attention module.


where SW-Attn is sliding window attention. The main drawback of this design is that the power of the model is limited by
each of the layers and so it cannot take advantage of the complementary data processing of attention and neural memory
module. In our experiments, for evaluating memory in this design, we use a similar architecture as H3 (D. Y. Fu et al. 2023),
where we replace the the sequence model with our neural memory module (LMM).


**Memory Without Attention.** Although in the above, we discussed MAL as the combination of LMMs and attention in
a sequential manner, one simple variant of MAL is to treat LMM as a sequence model without any attention. From the
memory perspective, as discussed in Section 1, we expect each part of the memory system to work independently, even if
other components are disturbed. Therefore, a long-term memory module should still be a powerful model even without
short-term memory (i.e., attention). We refer to this variant as LMM or Titans (LMM) in our experiments. We provide
additional discussions on the connection of Titans and other modern recurrent models in Appendix C.


**4.4** **Architectural Details**


For the sake of simplicity and presentation, we avoid discussing the implementation details like using residual connection,
gating with linear layer, and normalization. In all blocks, we use residual connections. In our implementation, we use
SiLU(.) activation (Elfwing, Uchibe, and Doya 2018) as the non-linear activation for computing query, key, and values and
normalize queries and keys using _â„“_ 2-norm.


**Convolution.** Following the recent modern linear recurrent models (Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh
2024), we incorporate a 1D depthwise-separable convolution layer after each of the query, key, and value projections.
While not significantly affect the performance, these 1D convolutions have shown performance improvement and are also
computationally efficient.


**Gating.** We also follow the recent architectures that use normalization and gating with a linear layer before the final
output projection (Mehta et al. 2023).


**Theorem 4.1.** _Contrary to Transformers, diagonal linear recurrent models, and DeltaNet, all of which are limited to_ TC [0] _(Merrill,_
_Petty, and Sabharwal 2024), Titans are capable of solving problems beyond_ TC [0] _, meaning that Titans are theoretically more_
_expressive than Transformers and most modern linear recurrent models in state tracking tasks._

#### **5 Experiments**


ext, we evaluate the performance of Titans and its variants in language modeling, commonsense reasoning, needle
in haystack, DNA modeling, and time series forecasting tasks [1] . In more details, in this section, we answer the
# N following empirical questions: (1) How do Titans perform compared to baselines in downstream tasks? (see Â§5.2,


1In the first version of the work, we aim to provide insights/evidences about why the learning paradigms of Titans are effective. We are working on
finalizing the results of larger models and will report them in the next version.


11


Â§5.6, and Â§5.7); (2) What is the actual context length of Titans? (see Â§5.3 and Â§5.4); (3) How do Titans scale with respect to
context length? (see Â§5.8); (4) How the depth of memory can affect both performance and efficiency? (see Â§5.5); and (5)
What is the contribution of each Titansâ€™ component in its performance? (see Â§5.9).


**5.1** **Experimental Setup**


**Models.** In our experiments, we focus on the three variants of Titans, which we refer to as: Titans with (1) Memory as a
Context (MAC), (2) Memory as a Gate (MAG), and (3) Memory as a Layer (MAL) as well as (4) neural memory module
alone. The reason behind using our long-term memory as a separate module is based on our definition of learning. As
discussed in Section 1, we define learning a process for acquiring effective and useful memory. Accordingly, we expect our
long-term memory to effectively learn from data, even without attention. For each of these models, we consider four scales
with: (i) 170M, (ii) 340M, (iii) 400M, and (iv) 760M parameters. While the first three are trained on 15B tokens sampled
from FineWeb-Edu dataset (Penedo et al. 2024), the last one is trained on 30B tokens from the same dataset.


**Baselines.** We compare our models with the state-of-the-art linear recurrent models, Transformers, and hybrid models
(recurrent + attention). More specifically in language tasks, we compare with Transformer++ (Touvron et al. 2023),
RetNet (Yutao Sun et al. 2023), Gated Linear Attention (GLA) (S. Yang, B. Wang, Shen, et al. 2024), Mamba (Gu and Dao
2024), Mamba2 (Dao and Gu 2024), DeltaNet (S. Yang, B. Wang, Yu Zhang, et al. 2024), TTT (Yu Sun et al. 2024), and Gated
DeltaNet (S. Yang, Kautz, and Hatamizadeh 2024). In needle in haystack tasks, we also compare with GPT4 (Achiam et al.
2023), Llama3 with RAG (Touvron et al. 2023), RecurrentGemma2-9B (Botev et al. 2024), and Mistral (Jiang et al. 2023)
models, all of which are provided in the benchmark (Yuri Kuratov et al. 2024). In time series tasks, we compare with
Mamba-based (Behrouz, Santacatterina, and Zabih 2024), Transformer-based (Y. Liu et al. 2023; Nie et al. 2022; Yunhao
Zhang and Yan 2023), and linear models (Das et al. 2023; Z. Li et al. 2023; H. Wu et al. 2023; Zeng et al. 2023).


**Training.** In the training, we follow the training procedure of S. Yang, Kautz, and Hatamizadeh (2024), and use LLama 2
tokenizer with a vocabulary size of 32K and use training length of 4K tokens. We employ AdamW optimizer with learning
rate of 4 _ğ‘’_ -4 with cosine annealing schedule with batch size of 0.5M tokens, and weight decay of 0 _._ 1.


**5.2** **Language Modeling**


We first focus on the perplexity in language modeling and also commonsense reasoning tasks. The results for Titansâ€™
variants and also baselines with three different sizes of 340M, 400M, and 760M are reported in Table 1. Among non-hybrid
models, including Transformer++, our neural memory module achieves the best performance in both perplexity and
accuracy measures. Comparing our neural memory module and TTT, which is also a gradient-based recurrent model can
show us the importance of our weight decay as well as the momentum. As discussed earlier, the weight decay can be
interpreted as a gating mechanism to forget the past data, when it is needed. Also, momentum can help us better manage
the memory by providing additional memory for the surprise metric. While some baselines also take advantage of gating
mechanism, e.g., Mamba, Mamba2, and Gated DeltaNet, the superior performance of our neural memory module shows
the importance of both our surprise mechanism and having deep and non-linear memory. We further discuss the later in
Section 5.5.


Comparing the hybrid models, we found that all three variants of Titans (MAC, MAG, and MAL) outperform both Samba
(Mamba + attention) and Gated DeltaNet-H2 (Gated DeltaNet + atttention). We attribute the superior performance of Titans
(MAL) to the power of neural memory module as the architecture design and used attention are all the same. Comparing
Titans (MAG) and (MAC), we find that while their performance are close, MAC performs better when dealing with longer
dependencies in the data. Interestingly, both MAG and MAC outperform MAL variant, which due to using the same
modules, we attribute this to the architecture design of these models. This finding is particularly important as the current
hybrid models (except Hymba (X. Dong et al. 2024)) in the literature are using MAL-style combination of recurrent models
and attention.


**5.3** **Needle in a Haystack**


Scaling a model to longer context window is not always equivalent to being effective for very long sequences (Hsieh
et al. 2024). The needle-in-a-haystack (NIAH) task is designed to measure the actual effective context length of models.
In this task, we evaluate the model on retrieving a piece of information (i.e., the â€œneedleâ€) from long distractor texts (i.e.,


12


Table 1: Performance of Titans and recurrent- and Transformer-based baselines on language modeling and common-sense
reasoning tasks. Hybrid models are marked with [âˆ—] . The best results among simple and hybrid models are highlighted.


**Model** **Wiki.** **LMB.** **LMB.** **PIQA** **Hella.** **Wino.** **ARC-e** **ARC-c** **SIQA** **BoolQ** **Avg.**
ppl â†“ ppl â†“ acc â†‘ acc â†‘ acc_n â†‘ acc â†‘ acc â†‘ acc_n â†‘ acc â†‘ acc â†‘ â†‘

|Col1|Col2|340M params / 15B tokens|
|---|---|---|
|Transformer++<br>RetNet<br>GLA<br>Mamba<br>DeltaNet<br>TTT<br>Gated DeltaNet|31.52<br>41.08<br>32.50<br>49.73<br>28.51<br>43.02<br>30.83<br>40.21<br>28.65<br>47.30<br>27.44<br>34.19<br>27.01<br>30.94|30.76<br>62.98<br>34.76<br>50.53<br>45.21<br>24.05<br>36.81<br>58.24<br>42.92<br>28.24<br>62.61<br>34.15<br>50.91<br>44.27<br>23.62<br>36.79<br>59.72<br>42.54<br>28.73<br>64.05<br>35.96<br>50.00<br>54.19<br>24.29<br>37.13<br>58.39<br>44.09<br>29.94<br>63.79<br>35.88<br>49.82<br>49.24<br>24.56<br>35.41<br>60.07<br>43.59<br>28.43<br>63.52<br>35.95<br>49.63<br>52.68<br>25.37<br>37.96<br>58.79<br>44.04<br>30.06<br>63.97<br>35.71<br>50.08<br>53.01<br>26.11<br>37.32<br>59.83<br>44.51<br>34.11<br>63.08<br>38.12<br>51.60<br>55.28<br>26.77<br>34.89<br>59.54<br>45.42|
|Titans (LMM)<br>Titans (MAC)âˆ—<br>Titans (MAG)âˆ—<br>Titans (MAL)âˆ—|26.18<br>29.97<br>25.43<br>28.13<br>25.07<br>28.72<br>24.69<br>28.80|34.98<br>64.73<br>39.61<br>51.85<br>55.60<br>28.14<br>34.52<br>59.99<br>46.17<br>36.00<br>65.32<br>40.35<br>51.21<br>58.17<br>29.00<br>38.63<br>60.18<br>47.36<br>36.71<br>64.88<br>40.56<br>52.49<br>57.72<br>28.16<br>39.75<br>60.01<br>47.54<br>35.74<br>64.97<br>39.44<br>51.97<br>56.58<br>28.21<br>38.14<br>57.32<br>46.55|


|Col1|Col2|400M params / 15B tokens|
|---|---|---|
|Transformer++<br>RetNet<br>HGRN2<br>GLA<br>Mamba<br>Mamba2<br>DeltaNet<br>TTT<br>Gated DeltaNet<br>Sambaâˆ—<br>Gated DeltaNet-H2âˆ—|30.63<br>37.37<br>29.92<br>46.83<br>32.33<br>47.14<br>27.96<br>36.66<br>29.22<br>39.88<br>26.34<br>33.19<br>27.69<br>44.04<br>26.11<br>31.52<br>25.47<br>29.24<br>25.32<br>29.47<br>24.19<br>28.09|29.64<br>64.27<br>37.72<br>51.53<br>54.95<br>27.36<br>38.07<br>61.59<br>45.64<br>29.16<br>65.23<br>36.97<br>51.85<br>56.01<br>27.55<br>37.30<br>59.66<br>45.47<br>26.12<br>64.52<br>35.45<br>52.24<br>55.97<br>25.51<br>37.35<br>59.02<br>44.52<br>27.86<br>65.94<br>37.41<br>49.56<br>56.01<br>26.36<br>38.94<br>59.84<br>45.24<br>29.82<br>65.72<br>37.93<br>50.11<br>58.37<br>26.70<br>37.76<br>61.13<br>45.94<br>32.03<br>65.77<br>39.73<br>52.48<br>59.00<br>27.64<br>37.92<br>60.72<br>46.91<br>29.96<br>64.52<br>37.03<br>50.82<br>56.77<br>27.13<br>38.22<br>60.09<br>45.57<br>33.25<br>65.70<br>39.11<br>51.68<br>58.04<br>28.99<br>38.26<br>59.87<br>46.86<br>34.40<br>65.94<br>40.46<br>51.46<br>59.80<br>28.58<br>37.43<br>60.03<br>47.26<br>36.86<br>66.09<br>39.24<br>51.45<br>60.12<br>27.20<br>38.68<br>58.22<br>47.23<br>36.77<br>66.43<br>40.79<br>52.17<br>59.55<br>29.09<br>39.04<br>58.56<br>47.69|
|Titans (LMM)<br>Titans (MAC)âˆ—<br>Titans (MAG)âˆ—<br>Titans (MAL)âˆ—|25.03<br>28.99<br>25.61<br>27.73<br>23.59<br>27.81<br>23.93<br>27.89|35.21<br>65.85<br>40.91<br>52.19<br>59.97<br>29.20<br>38.74<br>60.85<br>47.83<br>36.92<br>66.39<br>41.18<br>52.80<br>60.24<br>29.69<br>40.07<br>61.93<br>48.65<br>37.24<br>66.80<br>40.92<br>53.21<br>60.01<br>29.45<br>39.91<br>61.28<br>48.60<br>36.84<br>66.29<br>40.74<br>52.26<br>59.85<br>29.71<br>38.92<br>58.40<br>47.87|


|Col1|Col2|760M params / 30B tokens|
|---|---|---|
|Transformer++<br>RetNet<br>Mamba<br>Mamba2<br>DeltaNet<br>TTT<br>Gated DeltaNet<br>Sambaâˆ—<br>Gated DeltaNet-H2âˆ—|25.21<br>27.64<br>26.08<br>24.45<br>28.12<br>23.96<br>22.94<br>28.37<br>24.37<br>24.60<br>24.17<br>23.51<br>21.18<br>22.09<br>20.63<br>22.71<br>19.88<br>20.83|35.78<br>66.92<br>42.19<br>51.95<br>60.38<br>32.46<br>39.51<br>60.37<br>48.69<br>34.51<br>67.19<br>41.63<br>52.09<br>63.17<br>32.78<br>38.36<br>57.92<br>48.46<br>32.80<br>66.04<br>39.15<br>52.38<br>61.49<br>30.34<br>37.96<br>57.62<br>47.22<br>33.54<br>67.90<br>42.71<br>49.77<br>63.48<br>31.09<br>40.06<br>58.15<br>48.34<br>37.06<br>66.93<br>41.98<br>50.65<br>64.87<br>31.39<br>39.88<br>59.02<br>48.97<br>34.74<br>67.25<br>43.92<br>50.99<br>64.53<br>33.81<br>40.16<br>59.58<br>47.32<br>35.54<br>68.01<br>44.95<br>50.73<br>66.87<br>33.09<br>39.21<br>59.14<br>49.69<br>39.72<br>69.19<br>47.35<br>52.01<br>66.92<br>33.20<br>38.98<br>61.24<br>51.08<br>39.18<br>68.95<br>48.22<br>52.57<br>67.01<br>35.49<br>39.39<br>61.11<br>51.49|
|Titans (LMM)<br>Titans (MAC)<br>Titans (MAG)<br>Titans (MAL)|20.04<br>21.96<br>19.93<br>20.12<br>18.61<br>19.86<br>19.07<br>20.33|37.40<br>69.28<br>48.46<br>52.27<br>66.31<br>35.84<br>40.13<br>62.76<br>51.56<br>39.62<br>70.46<br>49.01<br>53.18<br>67.86<br>36.01<br>41.87<br>62.05<br>52.51<br>40.98<br>70.25<br>48.94<br>52.89<br>68.23<br>36.19<br>40.38<br>62.11<br>52.50<br>40.05<br>69.99<br>48.82<br>53.02<br>67.54<br>35.65<br>30.98<br>61.72<br>50.97|



the â€œhaystackâ€). In this part, we use Single NIAH (S-NIAH) task from RULER benchmark (Hsieh et al. 2024) and evaluate
Titans and baselines on sequences with length 2K, 4K, 8K, and 16K. The results are reported in Table 2. Neural Memory
module achieves the best results compare to baselines in all three tasks. We attribute this superior performance to three
key differences of Titans with existing sequence models: (1) Compared to TTT, our Neural Memory can better handle the
memory capacity by using momentum and also the forgetting mechanism (i.e., weight decay). Therefore, with increasing
the sequence length, the performance of Neural Memory does not drop and show a consistent trend; (2) Compared to
Mamba2, which has the gating (forgetting) mechanism, Titans have deep non-linear memory, resulting in better memory
management. Also, contrary to our neural memory and DeltaNet, Mamba2 is not capable of removing a memory and so


13


Table 2: Performance of Titans and baselines on S-NIAH task from RULER benchmark. The best results among simple


and hybrid models are highlighted.


**S-NIAH-PK** **S-NIAH-N** **S-NIAH-W**
Model

2K 4K 8K 16K 2K 4K 8K 16K 2K 4K 8K 16K


TTT 98.4 98.8 98.0 88.4 60.2 36.6 10.2 4.4 78.8 28.0 4.4 0.0
Mamba2 98.6 61.4 31.0 5.4 98.4 55.8 14.2 0.0 42.2 4.2 0.0 0.0


(a) Few-shot Setup (b) Fine-Tuning Setup


Figure 6: Performance of Titans and baselines on BABILong benchmark. Titans (MAC) outperforms all baselines, including
extremely large models, e.g., GPT4.


we can see a significant drop in performance when increasing the sequence length; (3) Compared to DeltaNet, although it
is capable of removing memory using delta rule, it cannot erase the memory, lacking forgetting mechanism. Finally, As
expected we can see on par or better results when using Titans variants, where the best results correspond to MAC.


**5.4** **BABILong Benchmark**


In the previous section we discussed the results on a simple NIAH tasks where a single needle needs to be retrieved.
Although Titans showed better performance compared to baselines, their true advantage over very long sequences is still
hidden. To this end, in this section, we use a harder task from BABILong benchmark (Yuri Kuratov et al. 2024), in which
the model needs to reason across facts distributed in extremely long documents. We follow the original experimental setup
and training process in the benchmark. There are two settings: (1) Few-shot setting, in which we use large pre-trained
models, and (2) fine-tuning setting, where we fine-tune the MAC variant of Titans to compare it with other fine-tuned
baselines. The results for few-shot setting are reported in Figure 6a. In this setup, we can see Titans outperform all
baselinesâ€“i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al.
2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o-mini (Achiam et al. 2023). These
results are achieved while Titans (MAC) is having much less number of parameters than baselines.


In the fine-tuning setup, we compare the small fine-tuned version of Titans (MAC) with: (i) the fine-tuned version of small
models (almost the same number of parameters as Titans) such as Mamba (Gu and Dao 2024), RMT (Bulatov, Yury Kuratov,
and Burtsev 2022), (ii) large models with Retrieval-Augmented Generation (RAG) (P. Lewis et al. 2020) such as Llama3.18B (Touvron et al. 2023), and (iii) extremely large models such as GPT-4 (Achiam et al. 2023), GPT4o-mini, Qwen2.5-72B (A.
Yang et al. 2024), and Llama3.1-70B (Touvron et al. 2023). Baseline results are reported by (Yuri Kuratov et al. 2024). The
results of Titans and baselines are reported in Figure 6b. Titans outperform all models even extremely large models like
GPT4. Also, compared to Transformer-based with memory models like RMT, Titans show better performance mainly due
to their powerful memory. That is, RMT compress the historical data into 16 size vector-valued memory, while Titans with
in-context online memory learner are capable of encoding the past into the parameters of the model. Interestingly, even


14


(a) 170M Parameters (b) 360M Parameters (c) 760M Parameters


Figure 7: The effect of memory depth on the perplexity. Deeper long-term memory results in better scaling in longer
sequences.


Table 3: Performance on long-term forecasting. The best results are highlighted .


**Neural Memory** **Simba** **iTransformer** **RLinear** **PatchTST** **Crossformer** **TiDE** **TimesNet** **DLinear**


MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE


augmenting Llama3.1-8B model with RAG performs worse than Titans with about Ã—70 less parameters.


**5.5** **The Effect of Deep Memory**


In this section, we evaluate the effect of deep memory in both wall-clock training time and model performance [2] . To this
end, we focus on different variants of our neural memory module, where _ğ¿_ M = 1 _,_ 2 _,_ 3 _,_ 4. We also use Mamba as a baseline
for the model performance. For a fair comparison, we use the same training process for all models and train them on a
subset of the Pile dataset (L. Gao et al. 2020).


We report the perplexity of our models and baselines as the function of the sequence length in Figure 7. Interestingly, with
the increase of memory depth, _ğ¿_ M, the model can achieve better perplexity over all sequence length. Also, deeper memory
modules are more robust to the sequence length when the model has less number of parameters. With the increase of the
number of parameters, all models show better performance on longer sequences.


We also evaluate the effect of memory depth ( _ğ¿_ M = 1 _,_ 2 _,_ 3 _,_ 4) on the training
throughput. We report the training throughput (the number of tokens per
second) as the function of sequence length in Figure 8. All models scale linearly
with respect to the context length (i.e., constant trend in the number of tokens
per second with respect to sequence length). Also, by increasing the memory
depth, as expected, we can see a linear trend that a deeper memory results in
a slower training. Therefore, it is not always efficient to use deeper memory
modules, showing a trade-off between effectiveness and efficiency.



**5.6** **Time Series Forecasting**



Figure 8: The effect of memory depth on
training throughput



To show the effectiveness of our memory module in a broader tasks, we also evaluate its performance in time series
forecasting tasks. To this end, we use Simba framework (Patro and Agneeswaran 2024) for time series forecasting, and


2Note that, in this experiment, we only focus on the neural memory module to evaluate the effect of memory depth in the memorization process.
Combining neural memory with attention as we do in Titans variants, can additionally enhance the performance of the model over long sequences.


15


Table 4: Downstream evaluation of pre-trained DNA models on GenomicsBenchmarks (GreÅ¡ovÃ¡ et al. 2023). We report
top-1 classification accuracy (%).


Model Enhancer Cohn Enhancer Ens Human Reg. Non-TATA Promoters Human OCR Ens.


CNN 69.5 68.9 93.3 84.6 68.0
DNABERT 74.0 85.7 88.1 85.6 75.1
GPT 70.5 83.5 91.5 87.7 73.0
HyenaDNA 74.2 89.2 93.8 96.6 80.9


Transformer++ 73.4 89.5 89.9 94.4 79.5
Mamba 73.0        -        - 96.6        

replace its Mamba module with our neural memory. We report the results on common time series forecasting benchmark
datasetsâ€“ETT, ECL, Traffic, and Weather (H. Zhou et al. 2021). The results are reported in Table 3. Our neural memory
module is outperforming all baselines, including Mamba-based, linear-based, and Transformer-based architectures.


**5.7** **DNA Modeling**


In order to understand the capability of Titans beyond natural language, we further evaluate the performance of our
neural memory module on DNA modeling tasks. To this end, we evaluate pre-trained models on the downstream tasks
in GenomicsBenchmarks (GreÅ¡ovÃ¡ et al. 2023). We follow the same experimental setups from Nguyen et al. (2024), and
re-use the reported results of baselines by Arora et al. (2024). The performance of Titans (LMM) and baselines are reported
in Table 4. We find that LMM is competitive with state-of-the-art architectures across different downstream genomics
tasks.


**5.8** **Efficiency**



In this part, we compare the efficiency of our neural memory as well as Titans
with state-of-the-art sequence models. The training throughput of models for
different sequence length Ã— batch size are reported in Figure 9. Comparing
recurrent models, including our neural memory module, we can see our memory
module is slightly slower than Mamba2 and Gated DeltaNet, mainly due to: (1)
having deep memory and more expressive transition process (memory update),
and (2) highly optimized kernel in the implementation of Mamba2. Interestingly,
Titans (MAL) are faster than baselines as well as the memory module. The
main reason for this better throughput is the highly optimized kernel of FlashAttention (Dao 2024), which is used for implementing SWA and full attention
module in Titans.


**5.9** **Ablation Study**



Figure 9: Training throughput comparison of Titans and baselines.



Finally, we perform ablation studies on the different architectural choices in Titans. We consider our neural memory
module as a base model and then changing one component at a time: (1) replacing deep memory with linear memory,
removing (2) convolution, (3) momentum in the surprise measure, (4) weight decay (or forgot mechanism), and (5) persistent
memory. The results are reported in Table 5. All components of neural memory design are positively contributing to its
performance, where the greatest contribution comes from weight decay, momentum, convolution, and persistent memory,
respectively.


**The Effect of Architectural Design.** To evaluate the effect of architecture design, we compare the performance of three
represented variants of Titans in three aspects of (i) language modeling, (ii) commen-sense reasoning, and (iii) long context
NIAH (BABILong) tasks. The results are reported in Table 5. We find that MAC and MAG have close performance in
language modeling and common-sense reasoning tasks, while MAC achieve significantly better performance in long-context
NIAH. Both of these models achieve better performance than MAL. These results along with Figure 9, show a trade-off
between fast training and more expressive design.


16


Table 5: Ablation Study on Titans. All components of Titans are positively contributing to its performance.


Language Modeling Reasoning Long Context
Model
ppl â†“ acc â†‘ acc â†‘


+Attn (MAL) 25.91 47.87 96.91


Linear Memory 28.49 46.97 85.34
w/o Convolution 28.73 45.82 90.28
w/o Momentum 28.98 45.49 87.12
w/o Weight Decay 29.04 45.11 85.60
w/o Persistent Memory 27.63 46.35 92.49

#### **6 Conclusion**


In this paper, we present a neural long-term memory that, as a meta in-context learner, learns to memorize at test time.
The neural memory module is a recurrent model in nature, and is adaptively memorizing tokens that are more surprising
or are close to surprising tokens. Comparing to modern recurrent models, it has more expressive memory update and
storing mechanism. Using this memory, we present Titans architectures, and its three variants, in which we suggest to
incorporate the memory module as (1) a context, (2) gating, and (3) a layer. Our experimental evaluation on diverse tasks
tasks validate that Titans are more effective than Transformers and recent modern linear recurrent models, specifically for
long context. That is, Titans can scale to larger than 2M context window size with better accuracy than baselines.


Titans are implemented in Pytorch and JAX and we intend to make the code we used to train and evaluate our models
available soon.


17


#### **References**


[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. â€œGpt-4 technical reportâ€. In: _arXiv preprint_
_arXiv:2303.08774_ (2023).

[2] Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, and
Daniil Gavrilov. â€œLinear Transformers with Learnable Kernel Functions are Better In-Context Modelsâ€. In: _arXiv_
_preprint arXiv:2402.10644_ (2024).

[3] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan
Shillingford, and Nando De Freitas. â€œLearning to learn by gradient descent by gradient descentâ€. In: _Advances in_
_neural information processing systems_ 29 (2016).

[4] Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone,
Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. â€œExploring length generalization in large language modelsâ€. In:
_Advances in Neural Information Processing Systems_ 35 (2022), pp. 38546â€“38556.

[5] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, James Zou, Atri Rudra, and Christopher Re. â€œSimple linear attention language models balance the recall-throughput tradeoffâ€. In: _Forty-first International_
_Conference on Machine Learning_ [. 2024. url: https://openreview.net/forum?id=e93ffDcpH3.](https://openreview.net/forum?id=e93ffDcpH3)

[6] Dzmitry Bahdanau. â€œNeural machine translation by jointly learning to align and translateâ€. In: _arXiv preprint_
_arXiv:1409.0473_ (2014).

[7] Reza Bayat, Mohammad Pezeshki, Elvis Dohmatob, David Lopez-Paz, and Pascal Vincent. â€œThe Pitfalls of Memorization: When Memorization Hurts Generalizationâ€. In: _arXiv preprint arXiv:2412.07684_ (2024).

[8] Maximilian Beck, Korbinian PÃ¶ppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp,
GÃ¼nter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. â€œxLSTM: Extended Long Short-Term Memoryâ€. In:
_arXiv preprint arXiv:2405.04517_ (2024).

[9] Ali Behrouz, Michele Santacatterina, and Ramin Zabih. â€œMambamixer: Efficient selective state space models with
dual token and channel selectionâ€. In: _arXiv preprint arXiv:2403.19888_ (2024).

[10] Vincent-Pierre Berges, Barlas OÄŸuz, Daniel Haziza, Wen-tau Yih, Luke Zettlemoyer, and Gargi Gosh. â€œMemory
Layers at Scaleâ€. In: _arXiv preprint arXiv:2412.09764_ (2024).

[11] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. â€œBirth of a transformer: A
memory viewpointâ€. In: _Advances in Neural Information Processing Systems_ 36 (2024).

[12] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. â€œPiqa: Reasoning about physical commonsense in
natural languageâ€. In: _Proceedings of the AAAI conference on artificial intelligence_ . Vol. 34. 05. 2020, pp. 7432â€“7439.

[13] Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard
Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, et al. â€œRecurrentGemma: Moving Past Transformers
for Efficient Open Language Modelsâ€. In: _arXiv preprint arXiv:2404.07839_ (2024).

[14] LÃ©on Bottou and Vladimir Vapnik. â€œLocal learning algorithmsâ€. In: _Neural computation_ 4.6 (1992), pp. 888â€“900.

[15] Aydar Bulatov, Yuri Kuratov, Yermek Kapushev, and Mikhail S Burtsev. â€œScaling transformer to 1m tokens and
beyond with rmtâ€. In: _arXiv preprint arXiv:2304.11062_ (2023).

[16] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. â€œRecurrent memory transformerâ€. In: _Advances in Neural_
_Information Processing Systems_ 35 (2022), pp. 11079â€“11091.

[17] Edoardo Cetin, Qi Sun, Tianyu Zhao, and Yujin Tang. â€œAn Evolved Universal Transformer Memoryâ€. In: _arXiv_
_preprint arXiv:2410.13166_ (2024).

[18] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher RÃ©. â€œScatterbrain: Unifying sparse and
low-rank attentionâ€. In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 17413â€“17426.

[19] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos,
Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and
Adrian Weller. â€œRethinking Attention with Performersâ€. In: _International Conference on Learning Representations_ .
[2021. url: https://openreview.net/forum?id=Ua6zuk0WRH.](https://openreview.net/forum?id=Ua6zuk0WRH)

[20] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
â€œBoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questionsâ€. In: _Proceedings of the 2019 Conference_
_of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,_
_Volume 1 (Long and Short Papers)_ . Ed. by Jill Burstein, Christy Doran, and Thamar Solorio. Minneapolis, Minnesota:
[Association for Computational Linguistics, June 2019, pp. 2924â€“2936. doi: 10.18653/v1/N19-1300. url: https:](https://doi.org/10.18653/v1/N19-1300)
[//aclanthology.org/N19-1300/.](https://aclanthology.org/N19-1300/)


18


[21] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
â€œThink you have solved question answering? try arc, the ai2 reasoning challengeâ€. In: _arXiv preprint arXiv:1803.05457_
(2018).

[22] Nelson Cowan. â€œWhat are the differences between long-term, short-term, and working memory?â€ In: _Progress in_
_brain research_ 169 (2008), pp. 323â€“338.

[23] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. â€œTransformerXL: Attentive Language Models beyond a Fixed-Length Contextâ€. In: _ACL (1)_ . Ed. by Anna Korhonen, David R.
Traum, and LluÃ­s MÃ rquez. Association for Computational Linguistics, 2019, pp. 2978â€“2988. isbn: 978-1-950737-48-2.

[24] Tri Dao. â€œFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioningâ€. In: _The Twelfth Inter-_
_national Conference on Learning Representations_ [. 2024. url: https://openreview.net/forum?id=mZn2Xyh9Ec.](https://openreview.net/forum?id=mZn2Xyh9Ec)

[25] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. â€œFlashAttention: Fast and Memory-Efficient
Exact Attention with IO-Awarenessâ€. In: _Advances in Neural Information Processing Systems_ . Ed. by S. Koyejo, S.
Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh. Vol. 35. Curran Associates, Inc., 2022, pp. 16344â€“16359. url:
[https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-](https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf)
[Paper-Conference.pdf.](https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf)

[26] Tri Dao and Albert Gu. â€œTransformers are SSMs: Generalized models and efficient algorithms through structured
state space dualityâ€. In: _arXiv preprint arXiv:2405.21060_ (2024).

[27] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan K Mathur, Rajat Sen, and Rose Yu. â€œLong-term Forecasting
with TiDE: Time-series Dense Encoderâ€. In: _Transactions on Machine Learning Research_ (2023). issn: 2835-8856. url:
[https://openreview.net/forum?id=pCbC3aQB5W.](https://openreview.net/forum?id=pCbC3aQB5W)

[28] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba
Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. â€œGriffin: Mixing gated linear recurrences with
local attention for efficient language modelsâ€. In: _arXiv preprint arXiv:2402.19427_ (2024).

[29] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. â€œFlex Attention: A Programming Model
for Generating Optimized Attention Kernelsâ€. In: _arXiv preprint arXiv:2412.05496_ (2024).

[30] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu,
Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, et al. â€œHymba: A Hybrid-head Architecture for Small
Language Modelsâ€. In: _arXiv preprint arXiv:2411.13676_ (2024).

[31] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. â€œSigmoid-weighted linear units for neural network function approximation in reinforcement learningâ€. In: _Neural networks_ 107 (2018), pp. 3â€“11.

[32] Yukun Feng, Feng Li, Ziang Song, Boyuan Zheng, and Philipp Koehn. â€œLearn to remember: Transformer with
recurrent memory for document-level machine translationâ€. In: _arXiv preprint arXiv:2205.01546_ (2022).

[33] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. â€œHungry Hungry
Hippos: Towards Language Modeling with State Space Modelsâ€. In: _The Eleventh International Conference on Learning_
_Representations_ [. 2023. url: https://openreview.net/forum?id=COZDy0WYGg.](https://openreview.net/forum?id=COZDy0WYGg)

[34] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. â€œTest-time training with masked autoencodersâ€. In:
_Advances in Neural Information Processing Systems_ 35 (2022), pp. 29374â€“29385.

[35] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,
Anish Thite, Noa Nabeshima, et al. â€œThe pile: An 800gb dataset of diverse text for language modelingâ€. In: _arXiv_
_preprint arXiv:2101.00027_ (2020).

[36] Felix A Gers, JÃ¼rgen Schmidhuber, and Fred Cummins. â€œLearning to forget: Continual prediction with LSTMâ€. In:
_Neural computation_ 12.10 (2000), pp. 2451â€“2471.

[37] Alex Graves, Greg Wayne, and Ivo Danihelka. _Neural Turing Machines_ [. 2014. arXiv: 1410.5401 [cs.NE]. url:](https://arxiv.org/abs/1410.5401)
[https://arxiv.org/abs/1410.5401.](https://arxiv.org/abs/1410.5401)

[38] Klaus Greff, Rupesh K Srivastava, Jan KoutnÃ­k, Bas R Steunebrink, and JÃ¼rgen Schmidhuber. â€œLSTM: A search space
odysseyâ€. In: _IEEE transactions on neural networks and learning systems_ 28.10 (2016), pp. 2222â€“2232.

[39] KatarÃ­na GreÅ¡ovÃ¡, Vlastimil Martinek, David ÄŒechÃ¡k, Petr Å imeÄek, and Panagiotis Alexiou. â€œGenomic benchmarks:
a collection of datasets for genomic sequence classificationâ€. In: _BMC Genomic Data_ 24.1 (2023), p. 25.

[40] Albert Gu and Tri Dao. â€œMamba: Linear-Time Sequence Modeling with Selective State Spacesâ€. In: _First Conference_
_on Language Modeling_ [. 2024. url: https://openreview.net/forum?id=tEYskw1VY2.](https://openreview.net/forum?id=tEYskw1VY2)

[41] Albert Gu, Karan Goel, and Christopher Re. â€œEfficiently Modeling Long Sequences with Structured State Spacesâ€.
In: _International Conference on Learning Representations_ [. 2022. url: https://openreview.net/forum?id=](https://openreview.net/forum?id=uYLFoz1vlAC)
[uYLFoz1vlAC.](https://openreview.net/forum?id=uYLFoz1vlAC)


19


[42] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. â€œLM-Infinite: Zero-Shot
Extreme Length Generalization for Large Language Modelsâ€. In: _Proceedings of the 2024 Conference of the North_
_American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long_
_Papers)_ . Ed. by Kevin Duh, Helena Gomez, and Steven Bethard. Mexico City, Mexico: Association for Computational
[Linguistics, June 2024, pp. 3991â€“4008. doi: 10.18653/v1/2024.naacl-long.222. url: https://aclanthology.](https://doi.org/10.18653/v1/2024.naacl-long.222)
[org/2024.naacl-long.222.](https://aclanthology.org/2024.naacl-long.222)

[43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. â€œLiquid
Structural State-Space Modelsâ€. In: _The Eleventh International Conference on Learning Representations_ . 2023. url:
[https://openreview.net/forum?id=g4OTKRKfS7R.](https://openreview.net/forum?id=g4OTKRKfS7R)

[44] Zexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry Krotov, and Rogerio Feris. â€œCAMELoT:
Towards Large Language Models with Training-Free Consolidated Associative Memoryâ€. In: _arXiv preprint_
_arXiv:2402.13449_ (2024).

[45] Donald Olding Hebb. _The organization of behavior: A neuropsychological theory_ . Psychology press, 2005.

[46] John J Hopfield. â€œNeural networks and physical systems with emergent collective computational abilities.â€ In:
_Proceedings of the national academy of sciences_ 79.8 (1982), pp. 2554â€“2558.

[47] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. â€œMultilayer feedforward networks are universal approximatorsâ€. In: _Neural networks_ 2.5 (1989), pp. 359â€“366.

[48] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg.
â€œRULER: Whatâ€™s the Real Context Size of Your Long-Context Language Models?â€ In: _First Conference on Language_
_Modeling_ [. 2024. url: https://openreview.net/forum?id=kIoBbc76Sy.](https://openreview.net/forum?id=kIoBbc76Sy)

[49] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. â€œBlock-recurrent transformersâ€.
In: _Advances in neural information processing systems_ 35 (2022), pp. 33248â€“33261.

[50] Kazuki Irie, RÃ³bert CsordÃ¡s, and JÃ¼rgen Schmidhuber. â€œThe dual form of neural networks revisited: Connecting test
time predictions to training patterns via spotlights of attentionâ€. In: _International Conference on Machine Learning_ .
PMLR. 2022, pp. 9639â€“9659.

[51] Kazuki Irie, Imanol Schlag, RÃ³bert CsordÃ¡s, and JÃ¼rgen Schmidhuber. â€œGoing beyond linear transformers with
recurrent fast weight programmersâ€. In: _Advances in neural information processing systems_ 34 (2021), pp. 7703â€“7717.

[52] Vidit Jain and Erik Learned-Miller. â€œOnline domain adaptation of a pre-trained cascade of classifiersâ€. In: _CVPR_
_2011_ . IEEE. 2011, pp. 577â€“584.

[53] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las
Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. â€œMistral 7Bâ€. In: _arXiv preprint_
_arXiv:2310.06825_ (2023).

[54] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. â€œPolySketchFormer: Fast Transformers via Sketching Polynomial Kernelsâ€. In: _Forty-first International Conference on Machine Learning_ [. 2024. url: https://openreview.net/](https://openreview.net/forum?id=ghYrfdJfjK)
[forum?id=ghYrfdJfjK.](https://openreview.net/forum?id=ghYrfdJfjK)

[55] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
Radford, Jeffrey Wu, and Dario Amodei. â€œScaling laws for neural language modelsâ€. In: _arXiv preprint arXiv:2001.08361_
(2020).

[56] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. â€œTransformers are rnns: Fast autoregressive transformers with linear attentionâ€. In: _International conference on machine learning_ . PMLR. 2020,
pp. 5156â€“5165.

[57] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. â€œGeneralization through
Memorization: Nearest Neighbor Language Modelsâ€. In: _International Conference on Learning Representations_ . 2020.
[url: https://openreview.net/forum?id=HklBjCEKvH.](https://openreview.net/forum?id=HklBjCEKvH)

[58] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Igorevich Sorokin, Artyom Sorokin, and Mikhail
Burtsev. â€œBABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystackâ€. In: _The Thirty-_
_eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track_ [. 2024. url: https:](https://openreview.net/forum?id=u7m2CG84BQ)
[//openreview.net/forum?id=u7m2CG84BQ.](https://openreview.net/forum?id=u7m2CG84BQ)

[59] Hung Le, Truyen Tran, and Svetha Venkatesh. â€œSelf-attentive associative memoryâ€. In: _International conference on_
_machine learning_ . PMLR. 2020, pp. 5682â€“5691.

[60] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler,
Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, et al. â€œRetrieval-augmented generation for knowledge-intensive nlp
tasksâ€. In: _Advances in Neural Information Processing Systems_ 33 (2020), pp. 9459â€“9474.


20


[61] Danny Leybzon and Corentin Kervadec. â€œLearning, Forgetting, Remembering: Insights From Tracking LLM Memorization During Trainingâ€. In: _Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural_
_Networks for NLP_ . 2024, pp. 43â€“57.

[62] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. â€œRevisiting long-term time series forecasting: An investigation on linear
mappingâ€. In: _arXiv preprint arXiv:2305.10721_ (2023).

[63] Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, and Qiang Liu. â€œLonghorn: State space models are amortized
online learnersâ€. In: _arXiv preprint arXiv:2407.14207_ (2024).

[64] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
â€œLost in the middle: How language models use long contextsâ€. In: _Transactions of the Association for Computational_
_Linguistics_ 12 (2024), pp. 157â€“173.

[65] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. â€œitransformer:
Inverted transformers are effective for time series forecastingâ€. In: _arXiv preprint arXiv:2310.06625_ (2023).

[66] George Mandler. â€œThe structure of value: Accounting for tasteâ€. In: _Affect and cognition_ . Psychology Press, 2014,
pp. 3â€“36.

[67] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. â€œLong Range Language Modeling via
Gated State Spacesâ€. In: _The Eleventh International Conference on Learning Representations_ [. 2023. url: https:](https://openreview.net/forum?id=5MkYIYCbva)
[//openreview.net/forum?id=5MkYIYCbva.](https://openreview.net/forum?id=5MkYIYCbva)

[68] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. â€œPointer Sentinel Mixture Modelsâ€. In:
_International Conference on Learning Representations_ [. 2017. url: https://openreview.net/forum?id=Byj72udxe.](https://openreview.net/forum?id=Byj72udxe)

[69] William Merrill, Jackson Petty, and Ashish Sabharwal. â€œThe Illusion of State in State-Space Modelsâ€. In: _Forty-first_
_International Conference on Machine Learning_ [. 2024. url: https://openreview.net/forum?id=QZgo9JZpLq.](https://openreview.net/forum?id=QZgo9JZpLq)

[70] Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ramanan, and Kayvon Fatahalian. â€œOnline model distillation
for efficient video inferenceâ€. In: _Proceedings of the IEEE/CVF International conference on computer vision_ . 2019,
pp. 3573â€“3582.

[71] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. â€œLeave no context behind: Efficient infinite context
transformers with infini-attentionâ€. In: _arXiv preprint arXiv:2404.07143_ (2024).

[72] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. â€œMetalearned neural memoryâ€. In:
_Advances in Neural Information Processing Systems_ 32 (2019).

[73] Tsendsuren Munkhdalai and Hong Yu. â€œNeural semantic encodersâ€. In: _Proceedings of the conference. Association for_
_Computational Linguistics. Meeting_ . Vol. 1. NIH Public Access. 2017, p. 397.

[74] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, Stefano Massaroli,
Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. â€œHyenadna: Long-range genomic sequence modeling at single
nucleotide resolutionâ€. In: _Advances in neural information processing systems_ 36 (2024).

[75] A Nichol. â€œOn first-order meta-learning algorithmsâ€. In: _arXiv preprint arXiv:1803.02999_ (2018).

[76] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. â€œA time series is worth 64 words:
Long-term forecasting with transformersâ€. In: _arXiv preprint arXiv:2211.14730_ (2022).

[77] Hideyuki Okano, Tomoo Hirano, and Evan Balaban. â€œLearning and memoryâ€. In: _Proceedings of the National Academy_
_of Sciences_ 97.23 (2000), pp. 12403â€“12404.

[78] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De.
â€œResurrecting recurrent neural networks for long sequencesâ€. In: _International Conference on Machine Learning_ .
PMLR. 2023, pp. 26670â€“26698.

[79] Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle,
Marco Baroni, Gemma Boleda, and Raquel FernÃ¡ndez. â€œThe LAMBADA dataset: Word prediction requiring a broad
discourse contextâ€. In: _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume_
_1: Long Papers)_ . Ed. by Katrin Erk and Noah A. Smith. Berlin, Germany: Association for Computational Linguistics,
[Aug. 2016, pp. 1525â€“1534. doi: 10.18653/v1/P16-1144. url: https://aclanthology.org/P16-1144/.](https://doi.org/10.18653/v1/P16-1144)

[80] Badri N. Patro and Vijay S. Agneeswaran. _SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate_
_Time series_ [. 2024. arXiv: 2403.15360 [cs.CV].](https://arxiv.org/abs/2403.15360)

[81] Guilherme Penedo, Hynek KydlÃ­Äek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro
Von Werra, and Thomas Wolf. â€œThe FineWeb Datasets: Decanting the Web for the Finest Text Data at Scaleâ€. In:
_The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track_ . 2024. url:
[https://openreview.net/forum?id=n6SCkn2QaG.](https://openreview.net/forum?id=n6SCkn2QaG)

[82] Bo Peng. _RWKV-LM_ [. Version 1.0.0. Aug. 2021. doi: 10.5281/zenodo.5196577. url: https://github.com/](https://doi.org/10.5281/zenodo.5196577)
[BlinkDL/RWKV-LM.](https://github.com/BlinkDL/RWKV-LM)


21


[83] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao,
Xin Cheng, Michael Nguyen Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He,
Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, BartÅ‚omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna
Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, StanisÅ‚aw WoÅºniak,
Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. â€œRWKV: Reinventing RNNs for the Transformer Eraâ€.
In: _The 2023 Conference on Empirical Methods in Natural Language Processing_ [. 2023. url: https://openreview.](https://openreview.net/forum?id=7SaXczaBpG)
[net/forum?id=7SaXczaBpG.](https://openreview.net/forum?id=7SaXczaBpG)

[84] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian
Du, Teddy Ferdinan, Haowen Hou, et al. â€œEagle and finch: Rwkv with matrix-valued states and dynamic recurrenceâ€.
In: _arXiv preprint arXiv:2404.05892_ (2024).

[85] DL Prados and SC Kak. â€œNeural network capacity using delta ruleâ€. In: _Electronics Letters_ 25.3 (1989), pp. 197â€“199.

[86] Zhen Qin, Yiran Zhong, and Hui Deng. â€œExploring Transformer Extrapolationâ€. In: _Proceedings of the AAAI_
_Conference on Artificial Intelligence_ . Vol. 38. 17. 2024, pp. 18897â€“18905.

[87] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. â€œSamba: Simple Hybrid State Space
Models for Efficient Unlimited Context Language Modelingâ€. In: _arXiv preprint arXiv:2406.07522_ (2024).

[88] Ivan Rodkin, Yuri Kuratov, Aydar Bulatov, and Mikhail Burtsev. â€œAssociative recurrent memory transformerâ€. In:
_arXiv preprint arXiv:2407.04841_ (2024).

[89] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. â€œEfficient content-based sparse attention with
routing transformersâ€. In: _Transactions of the Association for Computational Linguistics_ 9 (2021), pp. 53â€“68.

[90] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. â€œWinogrande: An adversarial winograd
schema challenge at scaleâ€. In: _Communications of the ACM_ 64.9 (2021), pp. 99â€“106.

[91] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. â€œSocial IQa: Commonsense Reasoning
about Social Interactionsâ€. In: _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing_
_and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_ . Ed. by Kentaro Inui,
Jing Jiang, Vincent Ng, and Xiaojun Wan. Hong Kong, China: Association for Computational Linguistics, Nov. 2019,
[pp. 4463â€“4473. doi: 10.18653/v1/D19-1454. url: https://aclanthology.org/D19-1454/.](https://doi.org/10.18653/v1/D19-1454)

[92] Imanol Schlag, Kazuki Irie, and JÃ¼rgen Schmidhuber. â€œLinear transformers are secretly fast weight programmersâ€.
In: _International Conference on Machine Learning_ . PMLR. 2021, pp. 9355â€“9366.

[93] JH Schmidhuber. â€œLearning to control fast-weight memories: An alternative to recurrent nets. Accepted for
publication inâ€. In: _Neural Computation_ (1992).

[94] JÃ¼rgen Schmidhuber. â€œReducing the ratio between learning complexity and number of time varying variables
in fully recurrent netsâ€. In: _ICANNâ€™93: Proceedings of the International Conference on Artificial Neural Networks_
_Amsterdam, The Netherlands 13â€“16 September 1993 3_ . Springer. 1993, pp. 460â€“463.

[95] JÃ¼rgen Schmidhuber and Sepp Hochreiter. â€œLong Short-term Memoryâ€. In: _Neural Computation MIT-Press_ (1997).

[96] Avi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C Lipton, and J Zico Kolter. â€œRethinking llm memorization
through the lens of adversarial compressionâ€. In: _arXiv preprint arXiv:2404.15146_ (2024).

[97] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. â€œSimplified State Space Layers for Sequence Modelingâ€.
In: _The Eleventh International Conference on Learning Representations_ [. 2023. url: https://openreview.net/forum?](https://openreview.net/forum?id=Ai8Hw3AXqks)
[id=Ai8Hw3AXqks.](https://openreview.net/forum?id=Ai8Hw3AXqks)

[98] Robin Staab, Mark Vero, Mislav Balunovic, and Martin Vechev. â€œBeyond Memorization: Violating Privacy via
Inference with Large Language Modelsâ€. In: _The Twelfth International Conference on Learning Representations_ . 2024.
[url: https://openreview.net/forum?id=kmn0BhQk7p.](https://openreview.net/forum?id=kmn0BhQk7p)

[99] Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. â€œAugmenting selfattention with persistent memoryâ€. In: _arXiv preprint arXiv:1907.01470_ (2019).

[100] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. â€œEnd-to-end memory networksâ€. In: _Advances in neural_
_information processing systems_ 28 (2015).

[101] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong
Wang, Sanmi Koyejo, et al. â€œLearning to (learn at test time): Rnns with expressive hidden statesâ€. In: _arXiv preprint_
_arXiv:2407.04620_ (2024).

[102] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. â€œRetentive
network: A successor to transformer for large language modelsâ€. In: _arXiv preprint arXiv:2307.08621_ (2023).

[103] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,
Morgane RiviÃ¨re, Mihir Sanjay Kale, Juliette Love, et al. â€œGemma: Open models based on gemini research and
technologyâ€. In: _arXiv preprint arXiv:2403.08295_ (2024).


22


[104] W Scott Terry. _Learning and memory: Basic principles, processes, and procedures_ . Routledge, 2017.

[105] Matteo Tiezzi, Michele Casoni, Alessandro Betti, Tommaso Guidi, Marco Gori, and Stefano Melacci. â€œOn the
resurgence of recurrent models for long sequences: Survey and research opportunities in the transformer eraâ€. In:
_arXiv preprint arXiv:2402.08132_ (2024).

[106] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste
RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. â€œLlama: Open and efficient foundation language modelsâ€.
In: _arXiv preprint arXiv:2302.13971_ (2023).

[107] Jos Van Der Westhuizen and Joan Lasenby. â€œThe unreasonable effectiveness of the forget gateâ€. In: _arXiv preprint_
_arXiv:1804.04849_ (2018).

[108] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser,
and Illia Polosukhin. â€œAttention is All you Needâ€. In: _Advances in Neural Information Processing Systems_ . Ed.
by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Cur[ran Associates, Inc., 2017. url: https : / / proceedings . neurips . cc / paper _ files / paper / 2017 / file /](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
[3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

[109] Shida Wang. â€œLongSSM: On the Length Extension of State-space Models in Language Modellingâ€. In: _arXiv preprint_
_arXiv:2406.02080_ (2024).

[110] Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin,
Jingbo Shang, and Julian McAuley. â€œMEMORYLLM: Towards Self-Updatable Large Language Modelsâ€. In: _Forty-first_
_International Conference on Machine Learning_ [. 2024. url: https://openreview.net/forum?id=p0lKWzdikQ.](https://openreview.net/forum?id=p0lKWzdikQ)

[111] Yu Wang, Chi Han, Tongtong Wu, Xiaoxin He, Wangchunshu Zhou, Nafis Sadeq, Xiusi Chen, Zexue He, Wei Wang,
Gholamreza Haffari, et al. â€œTowards LifeSpan Cognitive Systemsâ€. In: _arXiv preprint arXiv:2409.13265_ (2024).

[112] Zhiwei Wang, Yao Ma, Zitao Liu, and Jiliang Tang. â€œR-transformer: Recurrent neural network enhanced transformerâ€.
In: _arXiv preprint arXiv:1907.05572_ (2019).

[113] Jason Weston, Sumit Chopra, and Antoine Bordes. â€œMemory networksâ€. In: _arXiv preprint arXiv:1410.3916_ (2014).

[114] Bernard Widrow and Marcian E Hoff. â€œAdaptive switching circuitsâ€. In: _Neurocomputing: foundations of research_ .
1988, pp. 123â€“134.

[115] Ronald J Williams and David Zipser. â€œA learning algorithm for continually running fully recurrent neural networksâ€.
In: _Neural computation_ 1.2 (1989), pp. 270â€“280.

[116] Daniel B Willingham. â€œSystems of memory in the human brainâ€. In: _Neuron_ 18.1 (1997), pp. 5â€“8.

[117] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, and Ross Girshick. â€œLongterm feature banks for detailed video understandingâ€. In: _Proceedings of the IEEE/CVF conference on computer vision_
_and pattern recognition_ . 2019, pp. 284â€“293.

[118] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. â€œTimesNet: Temporal 2DVariation Modeling for General Time Series Analysisâ€. In: _The Eleventh International Conference on Learning_
_Representations_ [. 2023. url: https://openreview.net/forum?id=ju_Uqw384Oq.](https://openreview.net/forum?id=ju_Uqw384Oq)

[119] Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, and Zhou Yu. â€œMemformer: A memoryaugmented transformer for sequence modelingâ€. In: _arXiv preprint arXiv:2010.06891_ (2020).

[120] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. â€œEfficient Streaming Language Models
with Attention Sinksâ€. In: _The Twelfth International Conference on Learning Representations_ [. 2024. url: https:](https://openreview.net/forum?id=NG7sS51zVF)
[//openreview.net/forum?id=NG7sS51zVF.](https://openreview.net/forum?id=NG7sS51zVF)

[121] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei
Huang, Haoran Wei, et al. â€œQwen2. 5 Technical Reportâ€. In: _arXiv preprint arXiv:2412.15115_ (2024).

[122] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. â€œGated Delta Networks: Improving Mamba2 with Delta Ruleâ€. In:
_arXiv preprint arXiv:2412.06464_ (2024).

[123] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. â€œGated Linear Attention Transformers
with Hardware-Efficient Trainingâ€. In: _Forty-first International Conference on Machine Learning_ [. 2024. url: https:](https://openreview.net/forum?id=ia5XvxFUJT)
[//openreview.net/forum?id=ia5XvxFUJT.](https://openreview.net/forum?id=ia5XvxFUJT)

[124] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. â€œParallelizing Linear Transformers with the
Delta Rule over Sequence Lengthâ€. In: _The Thirty-eighth Annual Conference on Neural Information Processing Systems_ .
[2024. url: https://openreview.net/forum?id=y8Rm4VNRPH.](https://openreview.net/forum?id=y8Rm4VNRPH)

[125] Luca Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager,
Alessandro Achille, and Stefano Soatto. â€œBâ€™MOJO: Hybrid State Space Realizations of Foundation Models with
Eidetic and Fading Memoryâ€. In: _The Thirty-eighth Annual Conference on Neural Information Processing Systems_ .
[2024. url: https://openreview.net/forum?id=RnQdRY1h5v.](https://openreview.net/forum?id=RnQdRY1h5v)


23


[126] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. â€œHellaSwag: Can a Machine Really Finish
Your Sentence?â€ In: _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_ . Ed. by
Anna Korhonen, David Traum, and LluÃ­s MÃ rquez. Florence, Italy: Association for Computational Linguistics, July
[2019, pp. 4791â€“4800. doi: 10.18653/v1/P19-1472. url: https://aclanthology.org/P19-1472/.](https://doi.org/10.18653/v1/P19-1472)

[127] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. â€œAre transformers effective for time series forecasting?â€ In:
_Proceedings of the AAAI conference on artificial intelligence_ . Vol. 37. 2023, pp. 11121â€“11128.

[128] Hao Zhang, Alexander C Berg, Michael Maire, and Jitendra Malik. â€œSVM-KNN: Discriminative nearest neighbor
classification for visual category recognitionâ€. In: _2006 IEEE Computer Society Conference on Computer Vision and_
_Pattern Recognition (CVPRâ€™06)_ . Vol. 2. IEEE. 2006, pp. 2126â€“2136.

[129] Jianyu Zhang, Niklas Nolte, Ranajoy Sadhukhan, Beidi Chen, and LÃ©on Bottou. â€œMemory Mosaicsâ€. In: _arXiv preprint_
_arXiv:2405.06394_ (2024).

[130] Yunhao Zhang and Junchi Yan. â€œCrossformer: Transformer utilizing cross-dimension dependency for multivariate
time series forecastingâ€. In: _The eleventh international conference on learning representations_ . 2023.

[131] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. â€œInformer:
Beyond efficient transformer for long sequence time-series forecastingâ€. In: _Proceedings of the AAAI conference on_
_artificial intelligence_ . Vol. 35. 12. 2021, pp. 11106â€“11115.

[132] Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. â€œFast context adaptation via
meta-learningâ€. In: _International Conference on Machine Learning_ . PMLR. 2019, pp. 7693â€“7702.


24


#### **A Related Work**

There are diverse perspectives that can independently lead to the design of Titans or its components. Accordingly, to
further situate our work in a broader context, we review three categories of studies:


**A.1** **Linear Recurrent Models**


Recently, to address the computational cost of Transformers in both training and inference, linear recurrent models
have attracted much attention (Tiezzi et al. 2024), mainly due to their fast inference and training. The first generation
of modelsâ€“such as RetNet (Yutao Sun et al. 2023), LRU (Orvieto et al. 2023), RWKV (Peng, Alcaide, et al. 2023), S5 (J. T.
Smith, Warrington, and Linderman 2023), and S4 (Gu, Goel, and Re 2022)â€“uses data-independent transition matrix/decay
mechanism. The second generation of such models started to incorporate gating mechanism, a widely used techniques
in traditional RNNs (Gers, JÃ¼rgen Schmidhuber, and Cummins 2000; Greff et al. 2016; Van Der Westhuizen and Lasenby
2018), into such linear architecturesâ€“e.g., Griffin (De et al. 2024), SSMs (Behrouz, Santacatterina, and Zabih 2024; Dao
and Gu 2024; Gu and Dao 2024; Hasani et al. 2023), RWKV6 (Peng, Goldstein, et al. 2024). The third generation of linear
recurrent models are based on more complex memory updating rule based on meta-learning, online learning, and/or
delta-rule, resulting in more expressive and effective models such as: Longhorn (B. Liu et al. 2024), Gated DeltaNet (S. Yang,
Kautz, and Hatamizadeh 2024), TTT (Yu Sun et al. 2024), and DeltaNet (S. Yang, B. Wang, Yu Zhang, et al. 2024). Our
LMM model can be seen as the next generation of such models, in which we incorporate the token flow into the memory
updating mechanism, having more powerful memory updating process. See Appendix C for a detailed discussion of
different recurrent models and Titans.


**A.2** **Transformer-based Architectures**


**Transformers.** Transformers (Vaswani et al. 2017) as the de facto backbone for many deep learning models are based on
attention mechanism (Bahdanau 2014). They, however, suffer from quadratic computational cost, limiting their ability
to scale to long context window. To improve the memory consumption and throughput of softmax attention for longer
sequences, various studies focused on I/O aware implementations of attention (Dao 2024; Dao, D. Fu, et al. 2022), designing
more efficient attention mechanisms by sparsifying the attention matrix (B. Chen et al. 2021; Choromanski et al. 2021; Dai
et al. 2019; J. Dong et al. 2024; Roy et al. 2021), approximating the softmax (Arora et al. 2024), or developing kernel-based
(linear) attentions (Aksenov et al. 2024; Kacham, Mirrokni, and P. Zhong 2024; Schlag, Irie, and JÃ¼rgen Schmidhuber 2021;
S. Yang, B. Wang, Shen, et al. 2024).


**Segment-based Transformers.** Another line of research to improve the efficiency of Transformers is segment-based or
Chunk Transformers (Dai et al. 2019). The main drawback of chunk Transformers is that segments are fully separated and
so the context window is limited to the length of the chunks. To address this issue, various studies discuss the importance
of a memory so it can help the model to transfer information across chunks (Bulatov, Yuri Kuratov, et al. 2023; Bulatov,
Yury Kuratov, and Burtsev 2022; Feng et al. 2022; Hutchins et al. 2022; Rodkin et al. 2024; Z. Wang et al. 2019; Q. Wu
et al. 2020; Zancato et al. 2024). The key differences of Titans with these models are: (1) The memory in such models are
simple small size vectors, lacking expressive power to compress complex information; (2) The memory module lacks forget
mechanism, leading to a fast memory overflow; (3) only focus on momentary surprise, missing the information flow. More
specifically, recalling Recurrent Memory Transformers (RMT) (Bulatov, Yuri Kuratov, et al. 2023; Bulatov, Yury Kuratov,
and Burtsev 2022; Rodkin et al. 2024), one can treat Titans (MAC) as the generalization of RMT, where we use a neural
memory module instead of a vector-valued small size memory.


**Memory for Large Language Models.** Another interesting research direction has been to incorporate external memory
modules to LLMs after training (Z. He et al. 2024; Khandelwal et al. 2020; Y. Wang, Y. Gao, et al. 2024). Such models
are different from our approach as we incorporate the memory as a part of initial architecture and so we train it in
an end-to-end manner. Also, most of these explicit memory modules suffer from the same limitations as chunk-based
Transformers (mentioned above). For a detailed discussion of such models, we refer to the recent study of Y. Wang, Han,
et al. (2024).


25


**A.3** **Test Time Training and Fast Weight Programs**


**Memory Design and Augmentation with Memory.** In the literature, a substantial research effort have been toward
designing memory modules that are capable of either memorizing the knowledge abstraction (e.g., persistent memory) (Sukhbaatar, Grave, et al. 2019), or memorizing the data-dependent information (also known as contextual memory),
through recurrence (Bulatov, Yury Kuratov, and Burtsev 2022; Rodkin et al. 2024; Zancato et al. 2024), Transformers (Berges
et al. 2024; Cetin et al. 2024; Feng et al. 2022; Le, Tran, and Venkatesh 2020; Munkhdalai, Faruqui, and Gopal 2024; J. Zhang
et al. 2024), gradient (Irie, CsordÃ¡s, and JÃ¼rgen Schmidhuber 2022; Munkhdalai, Sordoni, et al. 2019), or other learning
paradigms (Sukhbaatar, Weston, Fergus, et al. 2015; Weston, Chopra, and Bordes 2014). These memory models, however,
either (1) are based on momentary surprise, missing the data flow and events, (2) lack forget mechanisms to remove
the memory, leading to a fast memory overflow (3) are fixed-size shallow (matrix valued) memory, resulting in poor
performance in long context, and (4) are based on fixed parameters at test time, lacking test time adaption.


**Fast Weight Programs.** The idea of seeing linear layers as the key-value (associative) memory system backs to fast
weight programs, in which dynamic fast programs are incorporated into recurrent neural networks to serve as writable
memory (Schlag, Irie, and JÃ¼rgen Schmidhuber 2021; JH Schmidhuber 1992; JÃ¼rgen Schmidhuber 1993). The two learning
rules of Hebbian (Hebb 2005) and delta (Prados and Kak 1989) are the most popular learning rules for fast weight programs,
which have been extensively explored in various studies (Irie, Schlag, et al. 2021; Munkhdalai, Sordoni, et al. 2019;
Munkhdalai and H. Yu 2017; Schlag, Irie, and JÃ¼rgen Schmidhuber 2021; JH Schmidhuber 1992; S. Yang, Kautz, and
Hatamizadeh 2024; S. Yang, B. Wang, Yu Zhang, et al. 2024). All these models, however, are based on momentary surprise,
missing the token flow in the sequences (see Section 3.1), and most of them lacks a forgetting gate, resulting in a poor
memory management.


**Test Time Training.** The key ideas of learning at test time or learning to learn (i.e., (Andrychowicz et al. 2016)) backs to
very early studies on local learning Bottou and Vapnik 1992, in which each test data sample is trained on its neighbors
before making a prediction (Gandelsman et al. 2022; H. Zhang et al. 2006). This approach further has shown promising
performance in vision tasks (Jain and Learned-Miller 2011; Mullapudi et al. 2019), mostly due to their ability to mitigate
out-of-distribution samples. The most similar studies to ours in this direction are MNM (Munkhdalai, Sordoni, et al. 2019)
and TTT-layer (Yu Sun et al. 2024), which we discussed the key differences in Appendix C.

#### **B Language Modeling and Common-sense Reasoning Datasets**


Following recent studies on linear recurrent models (Dao and Gu 2024; S. Yang, Kautz, and Hatamizadeh 2024; S. Yang,
B. Wang, Yu Zhang, et al. 2024), we use Wikitext (Merity et al. 2017), LMB (Paperno et al. 2016), PIQA (Bisk et al. 2020),
HellaSwag (Zellers et al. 2019), WinoGrande (Sakaguchi et al. 2021), ARC-easy (ARC-e) and ARC-challenge (ARC-c) (P.
Clark et al. 2018), SIQA (Sap et al. 2019), and BoolQ (C. Clark et al. 2019). Also, the baselines results for 400M models are
from the reported results by S. Yang, Kautz, and Hatamizadeh (2024).

#### **C Long-term Memory Module (LMM) as a Sequence Model**


In this section, we discuss how LMM as a sequence model is connected to modern linear recurrent models. For the sake
of simplicity, we start with a linear memory, where M _ğ‘¡_ = _ğ‘Šğ‘¡_ âˆˆ R _[ğ‘‘]_ [in] [Ã—] _[ğ‘‘]_ [in] . In this case, our objective function becomes
_â„“_ (M; _ğ‘¥ğ‘¡_ ) = [1] 2 [âˆ¥M] _[ğ‘¡]_ [k] _[ğ‘¡]_ [âˆ’] [v] _[ğ‘¡]_ [âˆ¥] 2 [2][, in which we use gradient descent with momentum and weight decay for the optimization.]

Accordingly, revisiting the recurrent formula in Equation 13:


M _ğ‘¡_ = diag (1 âˆ’ _ğ›¼ğ‘¡_ ) M _ğ‘¡_ + _ğ‘†ğ‘¡_ (32)

_ğ‘†ğ‘¡_ = diag ( _ğœ‚ğ‘¡_ ) _ğ‘†ğ‘¡_ âˆ’1 âˆ’ diag ( _ğœƒğ‘¡_ ) [ï¿½] M _ğ‘¡_ âˆ’1k _ğ‘¡_ [âŠ¤][k] _[ğ‘¡]_ [âˆ’] [v] _ğ‘¡_ [âŠ¤][k] _[ğ‘¡]_                    - _._ (33)


**LMM is Generalized Gated DeltaNet.** As discussed by S. Yang, Kautz, and Hatamizadeh (2024), DeltaNet (S. Yang, B. Wang,
Yu Zhang, et al. 2024) can alternatively be interpreted as an online learning problem that optimizes the L = [1] 2 [âˆ¥][S] _[ğ‘¡]_ [k] _[ğ‘¡]_ [âˆ’] [v] _[ğ‘¡]_ [âˆ¥] 2 [2][,]

resulting in:


S _ğ‘¡_ +1 = S _ğ‘¡_                 - _ğœƒğ‘¡_ âˆ‡L = S _ğ‘¡_ ï¿½I âˆ’ _ğœƒğ‘¡_ k _ğ‘¡_ k _ğ‘¡_ âŠ¤ï¿½ + _ğœƒğ‘¡_ v _ğ‘¡_ k _ğ‘¡_ âŠ¤ _[.]_ (34)


26


In this formulation, Gated DeltaNet is the same as above but with an additional weight decay term (S. Yang, Kautz, and
Hatamizadeh 2024). Comparing Equation 32 and Equation 34, we can see that setting _ğœ‚ğ‘¡_ = 0 results in both formulations to
be equivalent. Accordingly, we can say LMM is generalizing the very recent study of Gated DeltaNet (S. Yang, Kautz, and
Hatamizadeh 2024) from three aspects:


  - Momentum-based Rule: The Delta Rule is based on momentary surprise, meaning that the flow of tokens cannot
affect the memory update rule. LMM, however, is based on a momentum rule, which consider _both_ past and
momentary surprise.


  - Deep Memory: While Gated DeltaNet is limited to a linear (matrix-valued) memory as it requires finding the closed
recurrence form, LMM allows using deep memory module by using a gradient-based formulation, resulting in higher
expressive power.


  - Non-Linear Recurrence: While DeltaNet and Gated DeltaNet are based on linear recurrence, our LMM is using
inter-chunk non-linear recurrence and intra-chunk linear recurrence. This design allows LMM having a higher
expressive power.


Here, we discussed Gated DeltaNet as a sample of recent generation of recurrent models. Similar approaches such
as RWKV-7 (Peng 2021) are also using the same formulation and loss function, and so LMM is generalizing all such
models.


**LMM is Generalized Longhorn.** Similar to DeltaNet, Longhorn (B. Liu et al. 2024) uses the same loss function but it
derives the closed form using implicit online learning:


S _ğ‘¡_ +1 = S _ğ‘¡_ ï¿½I âˆ’ _ğ›¿ğ‘¡_ k _ğ‘¡_ k _ğ‘¡_ âŠ¤ï¿½ + _ğ›¿ğ‘¡_ v _ğ‘¡_ k _ğ‘¡_ âŠ¤ _[,]_ (35)


where _ğ›¿ğ‘¡_ = 1+ _ğœƒğœƒğ‘¡_ k _ğ‘¡_ _ğ‘¡_ k _ğ‘¡_ ~~[âŠ¤]~~ [. It, however, lacks a forgetting gate, resulting in a faster memory overflow. Therefore, in addition two]
the abovementioned aspects of (1) Momentum-based Rule, (2) Deep Memory, and (3) Non-Linear Recurrence, LMM has
the advantage of using an additional (4) Forget Gate, leading to a better memory management.


**LMM is Generalized TTT Layer.** To the best of our knowledge, TTT (Yu Sun et al. 2024), is the only modern linear
recurrent models with a gradient-based updating rule. In addition to different architectural designs and also objective
functions, our LMM has three key differences with presented TTT layers (Yu Sun et al. 2024):


1. Forgetting Mechanism: TTT layers are updating memory at each time, without having the chance to forget the
past data. Accordingly, when fixing the memory size, the model cannot manage the memory for long sequences. A
forget mechanism, such as LMMâ€™s, allows clearing the memory when very past information is not needed anymore.
We show that in a general case, this forget mechanism is equivalent to weight decay and provide a fast method to
incorporate it into the parallel training.


2. Momentum-based Update Rule: TTT layers are based on momentary surprise, meaning that the flow of tokens
cannot affect the memory update rule. LMM, however, is based on a momentum rule, which consider _both_ past and
momentary surprise. See Section 3.1 for the motivation of this design.


3. Deep Memory: While TTT-layers allows for deeper memory, the advantages/disadvantages of such deeper memory
modules have not been experimentally evaluated.


To the best of our knowledge, our neural long-term memory module is the first linear recurrent model with momentumbased update rule.


Finally, as a key difference with all the above and other recent linear recurrent studies, note that the hybrid variants of
modern linear modelsâ€“such as Griffin (De et al. 2024), DeltaNet (S. Yang, B. Wang, Yu Zhang, et al. 2024), Gated DeltaNet (S.
Yang, Kautz, and Hatamizadeh 2024), H3 (D. Y. Fu et al. 2023), Mamba2 (Dao and Gu 2024), Samba (Ren et al. 2024), etc.â€“all
are based on sequential layer-wise design. We present Titans to show how effectively one can incorporate such memory
modules into an architecture.


27


